# 통찰의 왕이 되는 분석방법들 - 다중회귀분석과 로지스틱 회귀 분석

## 통계학의 왕도 회귀분석

통계의 힘 1편에서는 아웃컴이 비교하려는 그룹 간 우연이라고 말하기 어려운 수준의 차이가 있는지 없는지 따져보았다. 더 나아가기 전에 설명변수와 아웃컴에 의한 분석방법을 아래 표로 이해할 수 있다.

|아웃컴\설명변수|  질적(범주형)|양적(연속형)| 
|--|--|--|
|질적(범주형)| 비율 차이를 Z검정 또는 카이제곱검정  | 회귀분석|
|양적(연속형)| 평균값 차이를 Z검정 또는 T검정 | 회귀분석|

정리된 그림을 보면 설명변수가 질적일 경우, Z검정이나 카이제곱검정으로 아웃컴의 비율을 비교한다.  그러나 설명변수가 양적일 경우는 바로 우리가 공부할 회귀분석을 한다. 

## 산포도와 회귀직선으로 경향을 파악한다. 

가로와 세로축을 양적 항목으로 잡고 점을 그려넣은 그래프를 전문용어로 산포도라고 한다. 산포도를 보면 두 양적 항목에 대해서 경향성을 볼 수 있다. 
우리는 그러한 경향성을 가장 잘 나타내는 타당한 직선을 알고 싶다. 그러한 직선은 현실 데이터에 기반한 경향성 직선과 이상적인 경향선 직선과 차이를 최소화하는 직선이라고 말할 수 있다. 이러한 직선을 회귀직선이라고 한다. 

회귀직선에서 한 일화가 있다. 우생학에서 주장하는 것을 반박하는 내용인데 부모의 키가 크면 자녀도 키가 크겠지만 부모의 평균 키만큼 크지는 않다. 반대로 부모의 키가 작으면 자녀의 키도 작겠지만 부모의 평균키 만큼 작지도 않다. 이러한 경향성을 평범으로 회귀라고 부르며 우생학을 반박한 일화가 있다. 회귀분석이라는 이름은 평균값으로 회귀가 일어나는 현상에 대한 분석방법이라는 뜻이다. 

## 회귀분석으로 잘 보이지 않는 관계성 분석이 가능하다. 

최소제곱법을 이용하여 얻은 2개의 양적 항몬간의 경향성을 나타내는 직선을 회귀직선이라고 부르며 이를 실제 구했던 사람은 피어슨이다. 
사실 최소제곱법 계산 자체는 100년 전인 카우스에 의해 발견되었지만, 그보다 훨씬 단순한 회귀분석을 사람들은 발명이라고 할가?

회귀분석에서 가장 큰 차이점은 잘 보이지 않는 관계성을 분석할 수 있다는 점이다. 피어슨은 최소제곱법을 어떤 변수로도 나타낼 수 있는 산포도라는 추상적인 것으로 확장하여, 그 어떤 정보도 일단 수치화하면 관련성을 명백히 할 수 있는 통계학의 만능성을 가진다.

자 그러면 실제 회귀식은 어떻게 구할까?
단순한 예로 A,B,C 3명의 영업직원에게 이달의 고객 방문회수와 성사 계약건수를 물었다. 이 데이터로 계약 건수를 아웃컴으로 경향성을 해석하고 방문 1건을 늘릴때마다 평균 몇건의 계약 성사가 기대가 되는지 알고 싶다. 

설명변수인 방문회수를 가로축, 아웃컴인 계약건수를 Y축에 두고 산포도를 그리면 경향성을 해석할 수 있다. 

$$Y = aX+ b$$

$a$는 X값이 1 늘어날때마다 Y값이 얼마나 늘어나고 줄어드는지 나타내는 기울기이다. 이 기울기를 회귀계수라고도 부른다. $b$는 X가 0일때 Y값이 뭔지 나타내는 절편이다. 

최소제곱법을 사용하여 회귀직선을 구할때는 산포도에 각 점에 대해서 실제의 아웃컴값과 예측 휘귀식에서 구해지는 예측값 차이의 제곱합을 최소화 한다. 
이때 아웃컴과 예측값의 차이를 전문용어로 잔차제곱합이라고 하며, 이 값이 가장 작을때 최량의 회귀직선으로 생각한다. 

우리는 이 최량의 회귀직선을 찾는 것이 목표이다. 최량을 만드는 a,b의 조합은 무엇인가를 생각하는것이 곧 회귀분석 결과다. 

[보충 13 단순회귀분석]

단순회귀 모델은 회귀직선 $y=ax+b$을 기본적인 모델로 가정한다. $x_i, y_i$라는 그룹의 $n$개의 데이터에 대해 회귀직선 모델 $y=ax+b$을 적용, 잔차제곱합(residual sum of squares)은 아래로 식으로 나타난다. 
$$잔차제곱합 = ∑_{i=1}^{n}(y_i-(ax_i+b))^2$$

잔차(residual)는 실제 값과 모델 추정값의 차를 말한다. $y_i-(ax_i+b)$

잔차제곱합은 a,b 값에 의해 좌우되는 함수로 생각 할 수 있으므로 이를 아래와 같이 표현할 수 있다. 우리는 이 잔차제곱합을 최소화 하는 a, b를 알고 싶다. 

$$R(a, b) = ∑_{i=1}^{n}(y_i-ax_i-b)^2$$
이제 a, b로 각각 편미분을 하자.

>b 편미분
$$-2∑_{i=1}^{n}(y_i-ax_i-b) = 0 \Leftrightarrow <=>∑_{i=1}^{n}y_i =  a∑_{i=1}^{n}x_i+nb$$

여기서 $n$으로 양변은 나누면, $\bar y = a\bar x + b$이다. 즉 회귀직선이 $x$와 $y$ 각각의 평균값을 지니는 것이다.

>a 편미분  
>$$-2∑_{i=1}^{n}(y_i-ax_i-b)x_i = 0 \Leftrightarrow <=>∑_{i=1}^{n}x_iy_i =  ∑_{i=1}^{n}x_i^2+b∑_{i=1}^{n}x_i$$

a의 편미분으로 나온 방정식에 b의 편미분으로 나온 식($\bar y = a\bar x + b$)으로 b를 치환하자. 그러면 아래와 같은 식이 구해진다. 

$$∑_{i=1}^{n}x_iy_i= a∑_{i=1}^{n}x_i^2+b∑_{i=1}^{n}x_i = ∑_{i=1}^{n}x_i^2+(\bar y-a\bar x)∑_{i=1}^{n}x_i= a∑_{i=1}^{n}x_i^2+(\bar y-a\bar x){n}\bar x $$
$$= a∑_{i=1}^{n}x_i^2+{n}\bar x\bar y- an\bar x^2 = a(∑_{i=1}^{n}x_i^2-n\bar x^2) + n\bar x \bar y$$
정리하자면, $$a = (∑_{i=1}^{n}x_iy_i-n \bar x \bar y) / (∑_{i=1}^{n}x_i^2 - n \bar x^2)$$

> 우변  
$$∑_{i=1}^{n}x_i^2 - n \bar x^2 = ∑_{i=1}^{n}(x_i - \bar x)^2 =  ∑_{i=1}^{n}(x_i^2 - 2 \bar xx_i+ \bar x^2) $$
$$=  ∑_{i=1}^{n}x_i^2 -2\bar x ∑_{i=1}^{n}x_i + n \bar x^2 =  ∑_{i=1}^{n}x_i^2 - 2\bar x n \bar x + n \bar x^2 = ∑_{i=1}^{n}x_i^2 - n \bar x^2$$

즉 우변 분모는 $∑_{i=1}^{n}x_i^2 - n \bar x^2 = ∑_{i=1}^{n}(x_i - \bar x)^2$가 성립하고 $x$의 평균값에서 벗어난 값의 제곱합을 말한다. 이 값을 n이나 n-1로 나누면 분산이다.
>우변 분자
>$$∑_{i=1}^{n}(x_i-\bar x)(y_i - \bar y) = ∑_{i=1}^{n}x_iy_i - \bar y n \bar x - \bar xn\bar y + n\bar x \bar y  = ∑_{i=1}^{n}x_i- \b$

 우변 분자는 $∑_{i=1}^{n}x_iy_i-n \bar x \bar y = ∑_{i=1}^{n}(x_i-\bar x)(y_i - \bar y)$이 성립한다.  즉 $x$의 평균값에서 벗어난 값과 $y$의 평균값에서 벗어난 값을 곱한 값의 합산을 뜻하는데, 이것을 n으로 나누는 것은 $x$와 $y$가 서로 독립적이라면 0이 된다. 이 값은  사실 관련성의 기준이 되는 공분산이라고 불리는 지표를 뜻한다. 

 우변 분모와 우변 분자를 n으로 나누어 정리하면 아래식과 같이 공분산을 분산으로 나눈다는 의미이다. $$ \frac {∑_{i=1}^{n}(x_i-\bar x)(y_i - \bar y)}{n} / \frac {∑_{i=1}^{n}(x_i - \bar x)^2}{n} $$

우리는 공분산과 분산을 구하면 기울기 a를 구할수 있다는 것을 밝혔다. 그러면 회귀계수의 표준오차는 어떻게 고려해야할까?
$$a = (∑_{i=1}^{n}x_iy_i-n \bar x \bar y) / (∑_{i=1}^{n}x_i^2 - n \bar x^2)$$

앞서 보았던 위의 식에서서 우변 분모를 x의 평균에서 벗어난 값의 제곱합(Sum of Squares)인 $S_x$라고 표현하자. 그러면 아래 식의 전개가 나온다.
$$a = \frac{1}{S_x} (∑_{i=1}^{n}x_iy_i-n \bar x \bar y) = \frac{1}{S_x} ∑_{i=1}^{n}(x_i-\bar x)y_i - \frac{\bar y}{S_x}∑_{i=1}^{n}(x_i-\bar x$$

∑_{i=1}^{n}(_i-{}^  n   $$  다.

)$$
$$= \frac{1}{S_x} ∑_{i=1}^{n}(x_i - \bar x)y_i - \frac{\bar y}{S_x}(n\bar x-n\bar x) = \frac{1}{S_x}∑_{i=1}^{n}(x_i- \bar x)y_i$$ 
지금까지 다뤄왔던 기울기와 절편은 어디까지나 데이터로 부터 산출된 값이지만 진정한 참값을 각각 α, β 또 i번째 데이터에 대응하는 잔차(residual)을를 $ε_i$라고 두고 식으로 표현해보자. 
 통계학에서는 오차나 잔차에 관한 것은 흔히 error의 머리글자인 e나 거기에 대응하는 그리스 문자 입실론 $ε_i$으로 표현한다. 이 식은 모든 i$i에 대해 성립한다. 좀 더 엄밀하게 말하면, 회귀분석에서 오차항 입실론은 몇 개인가의 가정을 하고 오차의 기댓값은 0, x나 y의 값과 오차는 독립적, 오차항은 정규분포를 따른다는 상황을 생각한다. 

$$a = \frac{1}{S_x}∑_{i=1}^{n}(x_i- \bar x)y_i = y_i = αx_i + β +ε_i$$  이것을 $$a = \frac{1}{S_x}∑_{i=1}^{n}(x_i- \bar x)(αx_i+β+ε_i)$$

$$ \Leftrightarrow<=> a = \frac{α}{S_x} ∑_{i=1}^{n}(x_i- \bar x)x_i + \frac{β}{S_x} ∑_{i=1}^{n}(x_i- \bar x) +   \frac{1}{S_x} ∑_{i=1}^{n}(x_i- \bar x) ε_i$$

>우변2∑_{i=1}^{n}(xy_i- \bar x)x_i = ∑_{i=1}^{n} x_i^2 - \bar x ∑_{i=1}^{n} x_i = ∑_{i=1}^{n}x_i^2 - n\bar x^2 = S_x$$

>우변 x_i-b)x_ix  =1}^{n}(x_i- \bar x)y_i = -n \bar x - n \bar x = 0$$

이들을 적용시키면 아래식이 성립한다. bar y $$

$$aC = \frac{α}{S_x} ∑_{i=1}^{n}(x_i- \bar x)x_i + \frac{β}{S_x} ∑_{i=1}^{n}(x_i- \bar x) +   \frac{1}{S_x} ∑_{i=1}^{n}(x_i- \bar x)ε_i$$
$$\Leftrightarrow a =\frac{α S_x}{S_x} + \frac{β}{S_x}0+\frac{1}{S_x}∑_{i=1}^{n}(x_i- \bar x)ε_i$$
위의 식에서 a의 기댓값을 구하면 $ε_i$는 $x_i-\bar x$에서 독립하여 기대값이 0이 되므로,

$$E(a) = α + \frac{1}{S_x}E(∑_{i=1}^{n}(x_i-\bar x)) \cdot E(ε_i) = α$$
즉 최소제곱법에 의해 얻어진 산출된 a의 기댓값은 진정한 회귀계수 $α$의 기댓값과 일치한다. 

이제 분산에 대해 생각하자면, 
$$V(a) = E((a-α)^2) = E((\frac{1}{S_x}∑_{i=1}^{n}(x_i- \bar x)ε_i )^2)$$ 이고
여기서 다른 $ε_i$끼리는 서로 독립적이고 또 

$$E((∑_{i=1}^{n}(x_i- \bar x))^2) = E(∑_{i=1}^{n}(x_i - \bar x)^2) = S_x$$가 성립한다. 따라서
$$V(a)  = \frac {1}{S_x^2}S_xE(ε_i^2) = \frac{E(ε_i^2)}{S_x}$$이다.

$ε_i$는 진정한 회귀식에서 벗어나 있으며 데이터의 수가 충분히 있을때 회귀식에서 벗어난 값의 제곱의 평균값이 평균제곱잔차는 $ε_i^2$의 기댓값과 일치한 ($∑_{i=1}^{n}{\frac{x_i-μ}{σ}}^2$) $C$는 자유도 $n$의 카이제곱($χ^2$)분포를 따른다. $σ^2$는 진정한 분산을 나타낸다. 

따라서 $$회귀 계수 a의 표준편차 = \sqrt {\frac{평균제곱잔차}{x의 편차평균합}} = \sqrt {\frac{잔차평균합}{x의 편차평균합*데이터의 수}}$$가 성립한다. 

회귀 계수를 이 표준오차로 나눈 값은 n이나 충분히 크지 않을 때 표준정규분포가 아닌 자유도 n-2인 t분포를 따른다. 즉 이 표준오차의 배우에는 자유도 n-2의 카이제곱 분포가 존재하는 셈이다. 잃어버린 2의 자유도는 기울기와 절편을 각각 참값이 아닌 데이터에서 산출된 값을 사용한 데서 유래한다. 
표준 정규분포는 +- 1.96이라는 범위에 해당하는 평균값을 중심으로 95%의 데이터가 존재하는 t 분포 구간은 자유도에 따라 달라서, 자유도가 크면 클수록 +- 1.96이라는 범위에 가깝고 작으면 작을수록 넓은 범위를 생각해야한다. 따라서 자유도가 1인 경우 += 12.7이라는 범위까지 확장해야 한다. 즉 오차범위가 넓어진다는 이야기대학에서 배운 편미분을 이용하여 a,b가 얼마일때 잔차제곱합이 최소가 되는지 알 수 있다. 

## 회귀분석에서는 기울기의 표준오차를 생각한다. 

데이터가 극단적으로 적을땐, 데이터가 우연히 발생한 상황일수도 있기 때문에 경향성을 못 믿겠다고 생각 할 수 있다. 
Z검정이나 T검정에서는 평균값 차이의 표준오차를 생각했지만, **회귀 분석에서는 회귀계수의 표준오차(SE)**를 생각한다. 

평균, 비율과 마찬가지로 회귀계수에서도 원시 데이터가 그 무엇이든 모두 더하면 정규분포에 가까워 진다는 중심극한 정리가 작용한다. 수백에서 수천건 이상의 데이터로 회귀분석을 수행하면 100회 정도의 데이터 수집과 분석을 통해 산출되는 회귀 계수의 95회 정도는 진정한 회귀계수 +2SE범위에 수용된다. 데이터 수가 수백건 이라하면 회귀계수에 대해서도 평균값의 차이와 마찬가지로 정규분포보다 T분포를 사용하는 편이 편하다는 점도 완전히 동일하다. 

그러면 어디에서 차이가 있을까?

Z검정에서는 평균값에서 벗어난 값을 제곱하여 계산했다. 이 벗어난 편차의 제곱합을 전문용어로 편차제곱함이라 하는데 이것을 데이터 수로 나눈것이 붓나이다. 

회귀계수의 표준오차는 예측값과 실제 값의 차이를 제곱합을 데이터의 수로 나눈 것인 잔차제곱합을 사용한다. 잔차 제곱합을 데이터의 수롤 나눈 것을 평균제곱잔차라고 한다. 제곱한 차이의 평균값이라는 의미로 평균제곱오차라고도 한다. 

## 회귀분석 오차의 계산에 추가로 필요로 한것

추가로 하나 고려해야 할 사항은 그룹 간 평균 값을 비교할때는 필요가 없었던 설명변수의 불규칙성 크기이다. 

회귀 계수의 표준오차 크리를 생각할 경우, 아웃컴의 예측값과 실제 값이 평균적으로 어느정도 벗어나 있는가라는 크기를, 설명변수가 어느정도 불규칙한지 크기에 대한 비율로 상대적인 판단을 할 필요가 있는 것이다. 

수백에서 수천건 이상의 데이터가 있을때 회귀 계수의 표준오차는 아래와 같다. [보충 13]
$$회귀계수의 표준오차 = \sqrt{\frac{잔차제곱합}{설명변수의 편차제곱합*데이터 수}}$$

현대의 분석방법에서 회귀계수의 표준오차나 신뢰구간을 산출하는데 t분포를 사용하지 않고 정규분포에 가까워지기란 사실상 불가능하다. 굳이 신경써야할 부분은 아니지만 이책에서 개념의 이해를 위해 사용한 정규분포와 현실에서 응용해야 하는 t분포 아래서의 계산결과가 한정된 수의 데이터에만 있는 상황에서는 반드시 일치하지 않는다. 이 점에 대해선 꼭 주의를 기울이자.

# 다양한 설명변수를 한번에 분석해주는 다중회귀분석

회귀분석으로 양적 설명변수가 늘어날수록 아우섬이 어느정도 늘거나 주는지 관계성을 알 수 있다. 
>단순 회귀분석 
>하나의 설명변수와 아웃컴의 연관성을 보기위한 회귀분석을 가리켜 단순회귀분석이라 한다. 단순은 설명변수가 하나뿐이라는 의미다. 

하나의 설명변수와 하나의 아웃컴간의 관계성만 분석하면 간과하고 있었던 다른 요인에 의해 결과가 왜곡 되는 경우가 종종 발생 한다. 예로는 설명변수에 방문횟수당 계약건수를 들었다. 데이터로 회귀직선을 구하면 $Y=0X+3$이 되는데 이는 방문횟수가 어떻든 계약건수는 변하지 않는다는 결론이 나온다. 하지만 남성과 여성이라는 그룹별 설명변수가 추가된다면, 그룹별 경향성이 나타난다. 

## 서브그룹 해석은 한계에 부딫치기 쉽다. 

이런 문제에 대처하는 방법 중 하나가 서브그룹 해석이다. 데이터 안에 방문횟수와 계약건수 외에 성별이나 세대, 출신자에 관한 정보가 포함되어 있다면 데이터를 성별, 세대별, 출신지별로 각각 나누어 분할된 그룹(서브그룹) 별로 방문회수와 계약 건수의 관계성을 분석한다. 
 그 결과 모든 서브구룹에서 방문횟수와 계약 건수의 관계성이 동일하게 나타났다면 적어도 서브 그룹해석에 사용한 요인이 결과를 왜곡 시키는것이었는지 아닌지 확인할 수 있다. 

서브그룹해석은 매우 단순하지만 데이터 항목이 많아지면 굉장히 수고스럽다. 또한 데이터가 적으면 서브그룹으로 나누었을시 오차면에서 큰 문제가 발생한다. 
이럴때 다중회귀분석을 쓰자

## 다중회귀분석이라면 한번에 분석이 가능하다. 

> 다중회귀분석
> 복수의 설명변수와 아웃컴의 관련성을 한번에 분석하는 방법
>  다중은 여러겹으로 풀이되는데 설명변수가 여럿 있는 회귀분석이라는 뜻이다. 

어느 요인이 동일하다면 이란 조건을 달고 그 조건을 따르는 분석 결과를 나타내는 것을, 그 요인으로 조정한다라고 표현한다 .

정리하자면, 서브그룹해석은 성별이나, 세대, 거주지역등 조건이 다르면 방문횟수가 동일하더라도 계약건수가 달라질 수 있다는 사실을 인정한다. 그래서 완전히 개별적으로 서로 다른 분석을 하여 정확한 설명변수와 아웃컴간의 관련성을 찾으려고 한다. 

한편 다중회귀분석 아래서는 방문회수가 동일하더라도 계약 건수가 다르다는 문제에 대해 구체적으로 얼마나 달리지는지 하는 값을 추정하고 그 값을 조정함으로써 정확한 관련성을 찾으려고 한다. 

## 회귀분석, z검정, t 검정의 결과가 일치하는 이유

다중회귀분석에서 실제의 회귀 계수와 절편을 계산할때도 단순회귀분석과 마찬가지로 최소제곱법을 사용한다. 지금까지는 회귀분석이란, 양적인, 즉 숫자에 의해 크기가 표현되는 설명변수와 그 아웃컴에 대해 이루어지는 분석이였다. 그런데 다중회귀분석은 그런 변수를 여럿을 동시에 분석하는 것인데 그렇다면 질적변수는 어떻게 처리해야 할까?

회귀분석에서는 설명변수가 숫자의 형태여야 하기 때문에 0과 1로 표현되는 이항변수의 형태로 질적변수도 바꿔주면 회귀분석으로 처리가 가능하다. 
이렇게 질적 설명변수를 표현하기 위한 0또는 1의 이항변수를 더미변수라고 한다.
사실상 지금까지 따로따로 설명해왔던 z검정이나 t검정, 이항변수를 설명변수로 두는 단순회귀분석은 완전히 동일한 성질을 가지고 있다.

회귀직선은 최소제곱법에 따라 회귀직선의 예측값과 실제값의 차이의 제곱합이 최소화되는 선을 그리는데, 더미 변수가 0인 그룹의 경우 데이터 차치의 제곱합이 최소화 되는 점은 데이터의 평균값을 말한다. 마찬가지로 더미 변수가 1인 경우 그 그룹의 데이터차이의 제곱합을 최소화하는 점인 평균값이다. 

그렇다면 더미변수를 사용한 설명변수를 토대로 그린 회귀직선의 절편과 회귀계수(기울기)는 무엇을 나타내는가?
절편이란 설명변수가 0일때 아웃컴이 회귀직선위에 얼마인가 하는 값이기 때문에 자연스럽게 더미변수가 0인 그룹의 평균값을 말한다. 그리고 회귀계수는 설명변수가 1이 많아질때 평균 아웃컴이 얼마나 늘고 주는지를 나타낸다. 질적변수에서 설명변수가 1이 많아 진다는 의미는 다른 그룹이 된다는 이야기고, 그에 따라 아웃컴이 어떻게 늘고 주는지는 평균값이 얼마나 늘었는가 줄었는가를 뜻한다. 

다시 말해 이 두 값의 설명변수에서의 회귀계수란 z검정이나 t검정에서 생각했던 그룹간 평균값의 차이와 완전히 같은 의미다. 
z검정이나  t검정에서 사용하는 평균값 차이의 표준오차와 두 값의 설명변수에 대해 단순회귀분석에서 사용하는 회귀계수의 표준오차는 완전히 똑같다. 

평균값 차이의 표준오차를 구하기 위해 필요한 각 그룹값의 분산은 그룹 각각의 값과 평균값의 차이에서 벗어난 값의 제곱의 평균값이다. 한편 회귀계수 표준오차를 구하기 위해 필요한 평균제곱잔차란 회귀직선에서 벗어난 값의 제곱의 평균값인데 그 회귀직선이 그룹별 평균값을 지나가는 이상, 이 역시 평균값에서 벗어난 값의 제곱의 평균값이다. 즉 양쪽을 통해 보려는 불규칙성도 본질적으로는 같다. 

계산방법은 그룹별 차이의 제곱 평균값을 구하고 데이터 수로 나누어 모두 더하는가 또는 전체에서 벗어난 값의 제곱의 평균값을 구하고 데이터 수와 설명변수의 분산으로 나누는가 하는 부분에 차이가 있지만 계산 결과는 같다.

## 카테고리가 셋이상인 경우

예를 들어, 성별이 남성인가 여성인가 처럼 두 분류로 나눈는 것이 가능한 경우 관례적으로 1로 주어진쪽의 카테고리명 으로 부른다. 지금까지는 두 분류로 카테고리를 나눌때만 고려했지만 셋 이상일 경우는 어떻게 더미 변수화를 할 수 있을까?

더미 변수화를 잡을때는 기준 카테고리를 잡고 그 카테고리와 나머지 카테고리를 비교하는 식으로 더미변수를 여럿 만들게 된다.
예를 들어, 장치로 '컴퓨터/태블릿/스마트폰/PMP'라는 4개의 카테고리가 있다고 하자. 컴퓨터가 기준 카테고리라고 한다면 나머지 3가지 장치에 대해선 0 또는 1의 더미 변수를 각각 가지게 된다. 이렇게 기준 카테고리를 제외한 나머지 카테고리 수 많큼 더미변수를 만들어 사용하면 된다. 

## 현장에서 압도적으로 사용되는 다중회귀분석

다중회귀분석에서도 회귀계수와 절편의 산출은 최소제곱법을 따르고 편미분으로 아웃컴에 대해 회귀식에서 추정되는 값과 실제 값차이의 제곱합이 초소화되는 값의 조합을 구한다. 

다중회귀분석에서도 역시 아웃컴의 예측값과 실제값의 차이의 제곱합과 설명변수의 분산에서 회귀계수의 표준오차를 구한다는 점에서 다를바는 없지만 다수의 설명변수가 얽혀있기 때문에 단순한 나눗셈만으로는 해결할수 없는 측면이 있다. 

이를 해결하기 위해 등장한 것이 **행렬적 의미의 나눗셈인 역행렬**이다. 행렬이란 다수의 숫자를 직사각형으로 들어놓고 한꺼번에 계산하는 선형대수를 말한다. 보통 $a*2 = 1$ 이라는 수식이 있으면 $a = 1 /2 = 0.5$처럼 나눗셈을 통해 a의 값을 구하지만 행렬끼리 연산에서는 기본적으로 나눗셈이 존재하지 않으므로 모두 곱셈으로 해결해야 한다. 예를 들어, A*B = C라는 행렬이 존재할때는 A값을 구하려면 양변에 $B^{-1}$를 곱해서 $A*B*B^{-1} = C*B^{-1}$를 만들어 계산하게 된다. 간단하게는 행렬 B와 그것의 역행렬 $B^{-1}$를 곱하면 수식에서 사라지는 행렬로 생각하자.

지금까지 다중회귀분석에 대한 내용을 다루었다. 이 분석방법을 사용하면 설명변수가 양적이든 질적이든 상관없다. 그리고 설명변수의 갯수에 몇개든 한꺼번에 분석하면서 상호의 설명변수가 결과를 왜곡 시킬지도 모르는 위험성 마저 배제할 수 있다.

z검정이나 t검정, 단순회귀분석과 같은 분석 방법은 기본적으로 이해가 필수지만, 업무에서 설명변수의 후보가 대량인 데이터가 있다고 한다면 우선 모든 설명변수를 다중회귀분석에 적용하여 p-값이 작고 회귀계수가 큰것을 탐색하는 방식을 압도적으로 많이 선호한다.

[보충 15] 다중회귀분석

다중회귀분석의 모델은 단순 회귀계수 분석의 모델과는 다르게 다수의 설명변수가 아웃컴에 영향을 미친다. 그래서 설명변수가 몇개가되든 상관없도록 하는 다중회귀분석의 회귀식은 설명변수의 수가 k개라고 했을 때, 아래와 같이 나타낼 수 있다.

$$y_j = β_0 + β_1x_{1j}+β_2x_{2j}+\cdots + β_ix_{ij}+\cdots+β_kx_{kj}+ε_j$$

절편은 $β_0$이고 $i$는 설명변수가 몇번째인지를 나타낸다. $j$는 몇번째 데이터에 대응하는지를 나타낸다. 임의의 $x_{ij}$는 i번째 설명변수에 대해서 j번째 데이터라는 의미다. $i$는 반드시 1이상이 되는 $β_i$가 설명변수 $x_i$에 대응하는 회귀계수. 맨 마지막 $ε_j$는 기대값이 0, 오차는 독립적, 정규분포를 따드로록 만드는 오차항이다. 

다중회귀분석에서 관례적으로 회귀계수의 참값을 $β_i$로 표현하고 데이터에서 추정한것이라는 의미에서 $\hat β_i$ (hat)으로 표현하기도 한다. 책에서는 $b_i$로 표현한다. 

앞서 설명했던 모델은 진정한 회귀식에 대한 관계성을 나타내지만 데이터로부터 산출된 절편과 회귀계수를 $b_i$, 그에 의해 나타나는 회귀식으로부터의 잔차를 $e$로 두면 아래와 같은 식이 나온다. 

$$y_j = b_0 + b_1x_{1j}+ \cdots +b_kx_{kj} + e_j$$

최소제곱법에 근거한 잔차 제곱합을 $b_0, b_1, \cdots , b_k$에 의해서 좌우되는 함수라 생각할 수 있으므로 아래로 나타낸다. 

$$R(b_0, b_1, \cdots , b_k) = ∑_{j=1}^{n}(y_j - b_0-b_1x_{1j} - \cdots -b_kx_{kj})^2$$

이것을 $b_0$ 이나 $b_1$로 미분하여 '=0'이라는 k+1개의 식을 만들고 연립방적식으로 푼다.  다만 설명변수가 많아지면 연립방정식을 풀기도 어려워지므로 대학 이후에서는 행렬이라는 도구를 사용하며 문제를 푼다. 여기선 편미분을 쓰지 않고 연립방정식을 사용하는 방법을 취한다. 

만약 y의 평균, 즉 $\bar y$를 어떻게 구하는가에 대해선 아래식에 좌변 $y_j$를 $∑$로 더하여 데이터의 수로 나누면 된다.
 $$y_j = b_0 + b_1x_{1j}+ \cdots +b_kx_{kj} + e_j$$
 $$\bar y = \frac{1}{n}∑_{j=1}^ny_j =\frac{1}{n}∑_{j=1}^n (b_0 + b_1x_{1j}+ \cdots +b_kx_{kj} + e_j)$$
여기서 우변을 좀 더 보면, 아래 식이 성립한다.

 $$\frac{1}{n}∑_{j=1}^n b_0 + \frac{1}{n}∑_{j=1}^nb_1x_{1j}+ \cdots +\frac{1}{n}∑_{j=1}^nb_kx_{kj} + \frac{1}{n}∑_{j=1}^ne_j$$
$$=b_0 + b_1 \bar x_1 + \cdots + b_k\bar x_k + \bar e$$

여기서 $\bar e$는 오차항의 가정에 의해 0이 된다고 하면, 아래식이 나타난다.
$$\bar y=b_0 + b_1 \bar x_1 + \cdots + b_k\bar x_k$$

이것만으로는 회귀계수가 모두 정해지지는 않겠지만 다음으로 $x_1y$의 평균값을 생각해본다. 그러려면 양변에 $x_i$를 곱한다.
$$y_jx_{1j} = b_0x_{1j} + b_1x_{1j}^2+ \cdots +b_kx_{kj}x_{1j} + x_{1j}e_j$$   
여기서 $∑$로 합계하여 데이터 수로 나누는 평균을 구해보자. 
$$\bar x_1 \bar y = b_0 \bar x_{1} + b_1 \bar x_{1}^2+ \cdots +b_k \bar x_{1} \bar x_{k} + \bar x_{1} \bar e$$ 

$\bar x_{1} \bar e$는 역시 회귀분석 오차항에 관한 가정으로 0이라 두면 

$$\bar x_1 \bar y = b_0 \bar x_{1} + b_1 \bar x_{1}^2+ \cdots +b_k \bar x_{1} \bar x_{k}$$ 이 성립한다. 여기서 우변의 처음 두항을 회귀계수로 미분하여 0으로 둔다는 값과 비교해 보면 사실 둘은 완전히 같다. 또 마찬가지로 $x_2y$의 평균을 생각하면

$$\bar x_2 \bar y = b_0 \bar x_{2} + b_1 \bar x_{2} \bar x_{1}+ \cdots +b_k \bar x_{2} \bar x_{k}$$
이외에도 절편전부와 회귀계수를 포함한 k+1개의 식을 만들수 있는데 모두 절편이나 회귀계수에 대해 편미분을 하고 0으로 둔다는 식과 대응한다. 

k+1개의 연립방정식에 실제 데이터를 적용하여 k개의 회귀계수와 1개의 절편 값을 계산하여 다중회귀분석을 마무리한다. 

## 14. 로지스틱 회귀분석과 그 계산을 가능케 하는 로그오즈비

양적인 아웃컴은 다중회귀분석을 사용하면 좋고 설명변수가 질적이든 양적이든 불문하고 동시에 다루는 것을 앞 단원에서 보았다. 이번 차례는 질적인 아웃컴을 동일하게 다루어 보자. 

그 대표적인 분석방법이 로지스틱 회귀분석이다. 여기서 로지스틱은 물류가 아니라 기호 논리학의 의미다. 이항 논리에 관한 아웃컴을 분석하는 회귀분석을 한다는 것이다.

그리고 로지스틱 회귀분석은 설명변수의 갯수에 상관없이 단순회귀분석인지 다중회귀분석인지 구별하지 않는다.

# 데이터의 배후를 파악한다 - 인자분석과 군집분석
# 통계 분석방법의 총정리와 사용순서



> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjAzODk1NzQyLC05MDM1NjMyMSwtMTkyMT
I5MjIxNSwyNjIwNTUxMzgsMjEzNTMyNzQ4LC00Mzk0NjA1NTEs
NzYyMTc3NTcxLC03NjcwMjEzMzQsNDYwMTY0NDk0LC03MjU1Mz
A3OTksLTExNDMzNjg1MTksLTk5MDY4NzE0MCwxODY4NjI2NDg0
LC02NTk2NDc3NywxNjY1Mjg4MCwyMDk0NTIzNjk1LDIwNTM0OT
M4MSwtMTcxNzgyNDE4NywtMTQxNDIyMzUxMSwtMTM5NDQ0MjY1
Nl19
-->