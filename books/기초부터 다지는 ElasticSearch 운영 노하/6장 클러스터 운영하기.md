# Overview

클러스터 운영하기 

ES 클러스터를 구축한 후에는 안정적으로 운영하기 위한 운영 이슈들이 생겨난다. 예를 들면, ES 버전 업그레이드, 샤드 배치 방식 변경, 인덱스의 댜앙한 설정값 변경등이 있다. 이번 장에서는 이렇게 클러스터 운영과 관련한 다양한 주제들을 살펴보자. 이번 장에서 다룰 내용은 아래와 같다. 

* ES 클러스터 버전 업그레이드
* 인덱스의 샤드 배치 방식 변경
* 운영 중 온라인으로 클러스터나 인덱스 설정 변경하기 
* 인덱스 API 활용하기
* 템플릿 활용하기

# 버전 업그레이드

ES의 새로운 버전은 빠르게 공개된다. 새로운 기능이 추가되고 이전 버전의 버그도 수정된다. 운영 중이 ES의 치명적인 버그가 포함되어 있거나, 꼭 필요한 새로운 기능이 추가되었다면 버전 업그레이드가 필요하다. 

ES에서 버전 업르게리드를 하는 방법은 크게 두 가지가 있다. 

* Full Cluster Restart - 모든 노드를 동시에 재시작하는 방식이며, 다운 타임이 있지만 빠르게 버전 업그레이드가 가능하다.
* Rolling Start - 노드를 순차적으로 한대씩 재시작한다. 다운 타임은 없지만 노드 개수에 따라서 업그레이드에 소요되는 시간이 길어질 수 있다.

Rolling Start 방식은 아래와 같은 순서로 작업 순서를 따라가야 한다. 

* 클러스터내 샤드 할당 기능 비활성화
* 프라이머리 샤드와 레플리카 샤드 데이터 동기화
* 노드 한 대 버전 업그레이드 이후 클러스터 합류 확인
* 클러스터 내 샤드 할당 기능 활성화
* 클러스터 그린 확인

레플리카 샤드가 있는 상태에서 한대에 노드에 장애가 생겨서 클러스터에 제외가 되어도 해당 데이터 노드가 가지고 있던 샤드를 다른 데이터 노드에 재분배하는 것을 알 수 있다. 하지만 ES의 버전 업그레이드 작업은 장애가 아니라 의도적으로 노드를 클러스터에서 제외하는 것이기 때문에, 해당 노드가 클러스터에서 제외되어 있는 동안을 장애 상황으로 판단하고 샤드를 재분배해서는 안된다. 

업그레이드 때문에 클러스터에서 제외된 노드의 샤드를 다른 데이터 노드로 재분배하는 것은 네트워크 비용이나 디스크 I/O 측면에서 큰 낭비다. 

클러스터내에서 샤드가 많으면 많을 수록, 또 샤드의 크기가 크면 클 수록 더 심한 낭비가 된다. 이러한 불상사를 피하기 위해서 업그레이드를 진행하는 동안 클러스터 내 샤드 할당 기능을 비활성화해야 한다. 

## 클러스터 내 샤드 할당 기능 비활성화

아래와 같은 코드를 통해 클러스터 API 설정을 수정하여 진행한다.

```
curl -X PUT "localhost:9200/_cluster/settings?pretty" -H Content-Type application/json -d
{
	"persistent": {
		cluster.routing.allocation.enable: none
	}
},
{
	"acknowledged": true,
	"persistent": {
		"cluster": {
			"routing" : {
				"allocation" : {
					enable : none
				}
			}
		}
	}
}
```

클러스터 내의 샤드 할당과 관련된 설정은 cluster.routing.allocation.enable이다. 이 값을 none으로 설정하여 샤드를 재분배 하지 않도록 한다.

위와 같이 샤드 할당 기능을 비활성화하면 노드 프로세스를 중지하여 클러스터에서 제외되더라도 해당 노드에 포함된 샤드를 다른 노드로 재분배하지 않는다. 

두번 째로 현재 배치해 놓은 프라이머리 샤드와 레플리카 샤드간의 데이터를 똑같은 형태로 맞춘다. 두 샤드가 가지고 있는 문서가 완벽히 일치해야 클러스터에서 노드가 갑작스럽게 제외되더라도 데이터의 정합성을 보장할 수 있기 때문이다. 

# 샤드 배치 방식 변경

ElasticSearch는 대부분 자동으로 샤드를 배치하지만 경우에 따라서 샤드 배치 방식을 변경해야 할때가 있다. 예를 들어 특정 노드에 장애가 발생하여 생성된 unassigned 샤드들에 대한 재할당 작업이 5회이상 실패하거나 일정기간이 지난 오래된 인덱ㅅ의 샤드를 특정 노드에 강제로 배치해야하는 경우다. 

이번 절에서는 수동으로 샤드의 배치 방식을 변경하는 법을 알아보자. 

| 옵션|설명  |
|--|--|
|reroute  | 샤드 하나하나를 특정 노드에 배치할때 사용|
|allocation  | 클러스터 전체에 해당하는 샤드 배치 방식을 변경할때 사용|
|rebalance  | 클러스터 전체에 해당하는 샤드 재분배 방식을 변경할때 사용한다.|
|filtering | 특정 조건에 해당하는 샤드들을 특정 노드에 배치할때 사용한다.|

## reroute

먼저, reroute는 샤드 하나하나를 개별적으로 특정 노드에 배치할때 사용한다. 제어할 수 있는 동작은 샤드 이동, 샤드 이동 취소, 레플리카 샤드의 특정 노드 할당이다. 샤드 이동은 인덱스 내에 정상적으로 운영중인 샤드를 다른 노드로 이동할때 사용한다.

```
{
	"commands": [
		{
			"move": {
				"index": "user",
				"shard": 1,
				"from_node": "data-1.es.com",
				"to_node": "data-2.es.com",
			}
	]
}
```

위 명령어를 보면, 이동할 샤드가 속한 인덱스 이름이 user 인덱스 임을 알 수 있다. 인덱스 이름을 잘못 넣으면 잘못된 샤드가 넘어가니 조심해야 한다. 

shard 필드에는 이동할 샤드 번호를 넣는다. from_node에는 현재 이동할 샤드가 배치되어 있는 node를 나타내며, to_node는 최종적으로 이동될 node를 말한다. 

위에서 기입한 정보가 이상이 없다면 정상 처리가 된다.

책의 내용으로는 1번 샤드가 data-2.es.com으로 이동이 되지만 원래 data-2.es.com이 가지던 0번 샤드가 다시 자동적으로 data-1.es.com 노드로 이동한다. 이것은 디스크 사용량을 기준으로 샤드를 할당하는 기능이 기본으로 활성화되어 있기 때문이다.

Elasticsearch는 노드마다 균등하게 샤드를 배치하기 때문에 수작업으로 샤드를 이동시키면 균형을 맞추기 위해 자동으로 다른 샤드를 하나 디오시킨다. move 명령어를 사용할때 이부분을 유념하자. 


move 명령어 대신 cancel을 넣으면 재배치 중인 샤드의 이동을 취소할수도 있다. 

unassigned 샤드에 대해 allocate_replica 명령을 사용할 수도 있다. allocate_replica 명령은 레플리카 샤드를 배치하기 위한 명령이며 이 명령으로 unassgsigned 상태의 샤드들을 start로 변경할 수 있다. 

```
{
	"commands": [
		{
			"allocate_replica": {
				"index": "user",
				"shard": 1,
				"node": "data-1.es.com"
			}
	]
}
```

index는 레플리카 샤드를 배치할 대상 샤드가 속한 인덱스 이름이다. shard는 레플리카 샤드를 배치할 대상 샤드 번호이다. node는 레플리카 샤드를 배치할 노드 이름이다. 

allocate_replica 명령은 이미 배치된 레플리카 샤드에는 사용할 수 없고, 배치되지 않은 레플리카 샤드에 대해서만 사용이 가능하다. 

즉 unassgined 상태의 레플리카 샤드가 있는 경우만 사용할 수 있다. 이렇게 샤드가 배치되지 않은 상태로 unassgined 상태로 남아있다는 것은 클러스터내의 샤드를 배치할 수 없는 상황이 발생했다는 의미이다. 이런경우 샤드 배치 실패의 원인을 제거하고 위 명령을 진행하면 된다. 

그렇다면 어떤 경우에 실패하게 될까?

가장 빈번한 경우는 노드들의 디스크 사용량이 높아지는 경우다. 이런 경우 아무리 샤드 배치를 시도해도 배치할 수 있는 노드가 없기 때문에 실패한다. 이럴땐 불필요한 인덱스를 제거하거나 데이터 노드를 증성하여 용량을 확보하는것이 좋다. 

## allocation

앞서 다룬 reroute가 인덱스의 특정 샤드를 재배치한다면, allocation은 클러스터 전체에 적용된다. 

allocation은 클러스터 내의 샤드 배치 기능을 활성/비활성화 하는 기능이 가장 많이 쓰인다. 

```
{
	"persistent" : {
		"cluster,routing.allocation.enable:"none"
	}
}
```

자세한 옵션을 살펴보자. 

* nono
	* 모든 샤드 배치 작업을 비활성화한다.
* all
	* 모든 샤드의 배치를 허용하며 노드간 샤드 배치가 진행된다. 클러스터에 새로운 노드가 증설되면 기존 노드가 가지고 있는 샤드들을 프라이머리와 레플리카 샤드 구분없이 나눠준다. 또 노드 한대가 클러스터에서 제외가 되면, 제외된 노드가 가지던 샤드를 모두 다른 노드에 배치한다. 기본값이다.
* primaries
	* all과 유사하지만 배치 대상 샤드를 프라이머리로만 한정한다. 즉 레플리카 샤드는 한번 배치된 이후로 노드를 증설해도 재배치 되지 않는다.
* new_primaries
	* 새롭게 추가되는 인덱스의 프라이머리 샤드만 재배치한다. 새롭게 투입된 노드들에 기존 인득세들의 샤드가 배치되지 않으며, 투입 이후 새로운 추가되는 인덱스에 대해서만 프라이머리 샤드가 배치된다.
* null
	* 옵션을 초기값으로 재설정할 때 사용한다. 

## rebalance

allocation은 노드가 증설되거나 클러스터에서 노드가 이탈했을때 동작과 관련된 설정이다. rebalance는 클러스터 내의 샤드가 배치된 후에 특정 노드에 샤드가 많다거나 배치가 고르지 않을때 이동과 관련된 설정이다. 

reroute와 rebalance는 정상적으로 배치가 된 상태에서 노드간 샤드를 주고받는는다는 점에서는 유사한 동작이다. 

```
{
	"persistent": {
		"cluster.routing.rebalance.enable": "replicas"
	}
}
```

사용할 수 있는 옵션은 아래와 같다.

* all
	* 프라이머리 샤드와 레플리카 샤드 전부 재배치 허용
* primaries
	* 프라이머리 샤드만 재배치 허용
* replicas
	* 레플리캬 샤드만 재배치 허용
* none
	* 모든 샤드의 재배치 비활성
* null
	* 설정을 초기화하여 Default인 all로 설정

아무때나 샤드 rebalance가 생성되는 것이 아니라 cluster.routing.allocation.disk.threshold_enabled 설정에 의해 노드 중 한 대 이상이 디스크 사용량 임계치에 도달했을때 동작하게 된다. 

아래는 가능한 임계치 설정 옵션이다. 

* cluster.routing.allocation.disk.watermark.low
	* 특정 노드의 임계치가 넘어가면 더 이상 할당하지 않음. 새로운 인덱스에 대해서는 적용되지 않음
	* 85%
* cluster.routing.allocation.disk.watermark.high
	* 임계치를 넘어선 노드를 대상으로 즉시 샤드 재할당 진행. 새롭게 생성된 인덱스에도 적용
	* 90%
* cluster.routing.allocation.disk.watermark.flood_stage
	* 전체 노드가 임계치를 넘어서면 인덱스를 read only 모드로 설정
	* 95%
* cluster.info.update.interval
	* 임계치 설정을 체크할 주기
	* 30s

기본적으로 디스크 사용률을 기반으로 배치하기 때문에 디스크 사용량이 높은 노드는 새롭게 생성되는 인덱스에 샤드를 배치하지 않을 수도 있고, 해당 노드는 샤드가 배치되지 않은 인덱스에 대해선 검색이 불가하게 된다. 

하나의 노드에서 디스크의 사용률이 커져가는 상황을 가정해보자. watermark.low에 설정한 값보다 높아지면 low룰이 적용된다. 새롭게 생성된 인덱스의 샤드들은 배치 하지만 현재 생성되어 있던 샤드들을 더이상 배치하지 않는다. 

그러다가 사용률이 더 높아져서 watermark.high에 도달하면 해당 노드의 샤드를 다른 노드로 옮기기 시작한다. 마지막으로 flood_stage에 도달하면 더이상 해당 노드 속한 샤드들에 대해선 쓰기 작업이 불가는해지고 읽기 전용모드로 변경된다. 

이렇게 되면 상태는 green 상태인데, 더 이상 문서를 색인할 수 없게 된다. 그렇기 때문에 설정 항목들은 매우 중요한 모니터링 지표이다. 

```
{
	"persistent" : {
		"cluster.routing.allocation.disk.watermark.low": "75%",
		"cluster.routing.allocation.disk.watermark.high": "85%",

"cluster.routing.allocation.disk.watermark.flood_stage": "90%",

	}
}
```

디스크 사용량이 90%를 넘어 읽기 전용 모드가 되는 경우가 운영중에 제법 빈번히 발생한다. 이 경우 불필요한 인덱스 삭제 또는 데이터 노드 증설을 통해 디스크 공간을 확보해야 한다. 

하지만 읽기 전용 모드에서 쓰기 가능 모드로 변경은 자동적으로 되지 않는다. 아래와 같이 API 호출을 통해 수동으로 변경해주어야 한다. 

```
curl -X PUT "localhost:9200/_all/_"
{
	"index.blocks.read_only_allow_delete" : null
}
```

위 항목을 null로 주어 기본값으로 변경한다. 이를 통해 인덱스에 대한 쓰기 작업이 다시 가능해진다. 

인덱스 단위로 읽기 전용 모드를 해제할 수 있지만, flood_stage로 인한 읽기 전용 모드는 모든 인덱스에 설정되므로 카능한 코드는 _all 파라미터로 모두에 적용하는 것이 좋다.

## filtering

filtering은 특정 조건에 맞는 샤드를 특정 노드에 배치할 수 있는 작업이다. 

* cluster.routing.allocation.include.{attribute}
	* 설정이 정의된 하나 이상의 노드에 샤드를 할당
* cluster.routing.allocation.require.{attribute}
	* 설정이 정의된 노드에만 샤드를 할당
* cluster.routing.allocation.exclude.{attribute}
	* 설정이 정의된 노드로부터 샤드를 제외

include와 require은 특정 노드에만 샤드를 배치하는 설정이다. exclude는 특정 노드에서 샤드를 제외하는 설정이다. 
exclude 설정은 Rolling Start를 할때 안정성을 위해서 작업 대상의 노드의 샤드를 강제로 다른 노드로 옮기는 용도로 사용한다. 

> 복제본을 충분히 운영한다면 exclude를 사용하지 않고 작업해도 되지만 로그의 유실이나 클러스터 상태에 민감한 서비스라면 exclude를 통해 안전하게 작업하는 것도 좋다.

```
curl -X PUT "localhost:9200/_cluster/settting"
{
	"persistent" : {
		"cluster.routing.allocation.exclude._name": "data-3.es.com"
	}
}
```

exlucde뒤에 _name이라는 속성이 붙었다. 이는 노드의 이름을 의미하며, data-3.es.com이라는 이름의 노드를 샤드 배치에서 제외한다는 의미다. exclude 처리가 완료되면 해당 data-3.es.com에 있는 모든 샤드가 다른 노드로 재배치 된다.

_name 말고 속성으로 설정할 수 있는 옵션은 _ip, _host 가 있다. 노드는 comma를 기준으로 여러 노드에 걸쳐서 설정할 수 있다. 

```
curl -X PUT "localhost:9200/_cluster/settting"
{
	"persistent" : {
		"cluster.routing.allocation.exclude._name": "data-2.es.com", "data-3.es.com"
	}
}
```

이렇게 다수의 노드를 샤드 배치에서 제외시킬고 해도, 클러스터의 안정성을 위한 최소한의 룰에 위배된다면 제외되지 않는다. 따라서 exclude 명령이 무시된다.

ES 클러스터는 샤드 배치 방식을 다양하게 바꿀 수 있다. 클러스터 상황에 맞게 적절한 설정을 진행하여 최적화된 샤드 배치 전략을 수립하고 적용할 수 있다. 또한 이번 절에서 다룬 설정들은 서비스 중에 다운 타임 없이 변경이 가능하기 때문에 필요한 경우 다양하게 활용하여 클러스터 안정성을 유지할 수 있다. 

# 클러스터와 인덱스의 설정 변경

먼저 클러스터의 설정을 변경하는 법을 살펴보자.

클러스터의 설정 변경은 cluster/settings라는 API를 통해서 변경할 수 있다. settings앞에 cluster가 있는데 이런 형태의 API를 클러스터 API라고 하며, 클러스터 전체를 대상으로 설정을 변경할 때 클러스터 APIf를 쓴다. 

클러스터 설정 확인 방법
```
/_cluster/settings?pretty
{
	"persistent": {},
	"transient": {}
}
```

persistent는 영구히 적용되는 설정이다. 아무런 값이 없으면 각 항목의 기본값이 자동으로 적용된다. 여기서 설정한 값은 클러스터를 재시작해도 유지된다. 

transient는 운영중인 동안에만 적용되는 설정들이다. persistent와 마찬가지로 아무런 값이 없다면 각 항목의 기본값이 적용된다. 여기서는 설정한 값은 클러스터를 재시작하면 초기화 된다.

기본적인 설정값은 elasticsearch.yml 파일을 통해서도 설정할 수 있는데 만약 persistent, transient, elasticsearch.yml 이 있다면 실제로 적용되는 우선순위는 transient, persistent, elasticsearch.yml 순위로 정해져 있다. 

그렇다고 모든 항목을 클러스터 API로 설정할 수 있는 것은 아니고 일부 항목은 elasticsearch.yml에서만 설정할 수 있는 것도 있다. 

클러스터 공통에 사용되는 항목은 elasticsearch.yml파일에 설정하고, 자주 변경되지 않지만 간헐적인 변경은 persistent에, 자주 변경되는 설정은 transient에 하는 것이 좋다. 

예를 들어, 데이터 노드의 디스크 사용률은 대부분 비슷한 수준으로 유지 될것이기 때문에 persistent에 설정하고 자주 변경되는 샤드 할당/비활성화 설정은 transient 모드에 정의하는 게 좋다.

# 인덱스 API

ElasticSearch에서는 인덱스의 설정을 동적으로 변경하는 방법 외에 인덱스를 대상으로 한 다양한 API를 지원한다. 이 중 자주 사용되는 인덱스 API에 대해 살펴보자. 

* open / close
	* 인덱스 open/close하는 API
* aliases
	* 인덱스에 별칭을 부여하는 API
* rollover
	* 인덱스를 새로운 인덱스로 분기시키는 API
* refresh
	* 문서를 세그먼트로 내리는 주기는 설정하는 API
* forcemerge
	* 샤드 내의 세그먼트를 병합 시키는 API
* reindex
	* 인덱스를 복제하는 API

위 API들은 특정 상황에서의 인덱스 설정을 변경하기 위해서도 사용되고, 장기간 데이터를 보관해야하는 경우에도 사용한다.

## open / close API

인덱스를 사용가능한 상태/불가능한 상태로 만드는 API이다. 신규로 인덱스를 생성하면 인덱스는 색인과 검색이 가능한 상태로 생성된다. 

이 상태를 open 상태라고 한다. 인덱스를 구성하는 샤드들은 클러스터를 구성하는 전체 샤드 개수에 포함된다. 이 상태에서 close API를 사용하면 인덱스는 더 이상 색인 할 수도 검색할 수도 없는 상태가 된다. 이 상태를 close 상태라 한다. 

인덱스를 close 상태로 변경하면 해당 인덱스에는 접근할 수 없지만 open API로 상태를 변경하면 언제든지 사용 가능하다. 

```
localhost:9200/user/_close? ... 
localhost:9200/user/_open? ... 
```

## aliases API

인덱스에 별칭을 부여할 수 있는 API다. 별칭을 통해 클라이언트는 인덱스의 이름뿐만 아니라 별칭으로도 인덱스에 접근할 수 있게 된다. 

```
localhost:9200/aliases? ... 
{
	"actions" : [
		{"add" : { "index" : "test1", "alias": "alias1"}}
	]
}
```

add는 alias를 생성하고, remove는 alias를 삭제한다. 또한 alias를 설정할 인덱스와 alias의 이름을 입력한다. 

위 처럼 설정하면 test1인덱스에 접근하기 위해서 alias1도 가능해진다. alias는 여러 인덱스에 걸쳐서 설정할 수도 있고, 패턴 매칭으로도 설정할 수 있다.

alias 주의할 점이 있는데 단일 인덱스에 alias별칭을 통해선 색인과 검색이 모두 가능하지만, 다수의 인덱스에 걸쳐 설정된 alias의 경우는 별칭을 통해선 색인이 되지 않고 검색만 가능하다. 주의하자.

여러 인덱스에 걸쳐 생성된 alias가 있을 때 close API로 인덱스가 하나라도 close된 상태라면 검색 요청이 불가능해진다. 

## rollover API 

rollover API는 인덱스에 특정 조건을 설정하여 해당 조건을 만족하면 인덱스를 새로 생성하고 새롭게 생성된 인덱스로 요청을 받는 API이다. 

**rollover API는 aliases API를 통한 별칭 설정이 반드시 필요한 API이다.**

roll over를 가기 전에 기존 인덱스 이름으로 색인과 검색 요청을 한다. 그 이후에 기존 인덱스에 너무 많은 문서가 색인되거나 요청이 커져서 인덱스를 하나 더 생성해야 한다면 rollover API를 이용해서 새로운 인덱스를 생성하고 기존에 생성된 별칭을 새로운 인덱스로 변경한다. 

사용자 입장에서는 아무런 작업을 하지 않고도 기존 인덱스를 유지한채 계속해서 색인하고 검색이 가능하다. 

```
localhost:9200/index?
{
	"aliases" : {"logs_write": {}},
	{
		"acknowledged" : true,
		"shared_acknowledged" : true,
		"index" : "logs-000001"
	}
}
```
logs_write 라는 별칭을 설정한다. 

```
localhost:9200/logs_write/_rollover/new_index?
{
	"conditions": {
		"max_age": "7d",
	}
	"aliases" : {"logs_write": {}},
	{
		"acknowledged" : true,
		"shared_acknowledged" : true,
		"index" : "logs-000001"
	},
	{
		"ackowledged": true,
		"shards_acknowledged": true,
		"oid_index": "logs-000001",
		"new_index": "new_index",
		"rolled_over": "true",
		"dry_run": false,
		"conditions": {
			"[max_size: 5gb]" : false,
			"[max_docs: 2]" : true,
			"[max_age: 7d]" : false
		}
	}
}
```

roll over가 진행되면서 새롭게 인덱스가 생성되기 때문에 의도치 않게 롤오버가 진행되었을 때에 되돌리기 어려울 수 있다. 이러한 문제를 예방하기 위해서 rollover API는 모의 실행을 할수 있는 dry_run 모드를 지원한다. 

dry_run 모드는 실제 변경이 적용되지 않고 변경이 적용되면 어떻게 되는지 보여주기 때문에 실제 롤 어보하기 전에 어떻게 변경되는지 확이니 가능하다.

## Refresh API

앞서 인덱스의 동적 설정 변경을 통해서 refresh_interval 설정을 바꾸는 방법을 살펴보았다. refresh API는 refresh_interval과 상관없이 메모리 버퍼 캐시에 있는 문서들을 바로 세그먼트로 저장해주는 API다.

```
"localhost:9200/user/_refresh?pretty"
{
	"_shards" : {
		"total" : 10,
		"successful" : 10,
		"failed" : 0
	}
}
```

refresh API를 호출한 user 인덱스는 refresh_interval 주기를 기다리지 않고 버퍼 캐시에 머물러 있던 사용자의 문서를 바로 디스크 세그먼트에 저장하여 해당 문서를 검색 가능한 상태로 만들어 준다.

## forcemerge API

forcemerge API는 인덱스의 샤드를 구성하는 수많은 세그먼트를 강제로 병합하는 API이다.

```
"localhost:9200/user/_forcemerge?max_num_segments=10&pretty...
{
	"_shards" : {
		"total" : 10,
		"successful" : 10,
		"failed" : 0
	}
}
```

max_num_segment 옵션으로 샤드 내 세그먼트들을 몇개의 세그먼트로 병합할 것인지 설정한다. 여기서는 10으로 설정했다.

forcemerge는 성능에 영향을 줄 수 있는 API이기 때문에 좀더 자세히 알아보자. 

몇개의  큰 세그먼트를 하나로 합치느냐에 대한 값을 max_num_segment로 설정하며 forcemerge API는 해당 인덱스에 속한 모든 샤드를 대상으로 진행하기 때문에 병합의 대상이 되는 세그먼트들은 샤드의 개수에 비례해서 증가하고 때에 따라서는 많은 양의 디스크 I/O 작업을 일으킨다. 따라서 너무 많은 세그먼트를 대상으로 forcemerge API 작업을 수행하면 성능저하가 발생할 수 있다. 

또한 계속 문서의 색인이 일어나고 있는 인덱스라면 세그먼트에 대한 변경 작업이 계속 되기 때문에 forcemerge API 작업은 하지 않는 것이 좋다. 

그래서 forcemerge 



> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzExMDQxMTEwLDE1ODEzMjM2ODgsMTU5MT
QzNTQxNiwtNzc2MjQ4MjU0LC0zMDkwODk0ODgsLTQ4MTkyODA1
MCwtMTc5MzQzNTA3MywxMTY1NjYwMzYxLDE1NTY4NDE5MDgsLT
ExODU4ODg4OTYsLTEzNTI4NTc3MjYsNzk2ODY4ODEsLTE4NTA5
NTg4OTcsLTEwNTMyNTMzNSwtMzk2ODgzOTAyLC02MDEyMTU3MT
AsMTQwNzQ1NDcxOCwyMTMxMjUzOTgsMTMyNDMwNTA2NiwxNjM5
MzgwODBdfQ==
-->