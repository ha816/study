# Overview

이번 장에서는 클러스터를 구축하는 방법을 알아보자. 먼저 ES에서 가장 중요한 환경 설정 파일인 elasticsearch.yml 파일을 통해서 어떤 항목들을 설정할 수 있는지, 그리고 성능에 중요한 영향을 미치는 jvm.options 파일을 통해서는 어떤 항목들을 설정할 수 있는지 살펴보자. 

* elasticsearch.yml 파일을 통해 설정할 수 있는 항목들
* jvm.options 파일을 통해 설정할 수 있는 항목들
* 클러스터 구축을 위해 노드들의 역할을 설정하는 방법
* 고가용성을 고려하여 설계된 클러스터를 사용하고 검증하는 방법

# elasticsearch.yml 설정 파일

elasticsearch.yml 파일은 ES를 구성하기 위해 기본이 되는 환경 설정 파일이다. 아래 코드는 ES를 처음 설치했을 때의 기본설정이다. 

```
# ---- Cluster ----
#
# cluster.name: my-application
#
# ---- Gateway ----
#
# gateway.recover_after_nodes: 3
#
# ---- Various ----
#
# action.destructive_requires_name : true
#
```

대부분 설정이 주석으로 처리되어 있으며, 해당 설정에 대한 간략한 설명이 주석으로 제공된다. 기본으로 제공되는 설정과 더불어 필수적으로 알아야 하는 여러 설정을 자세히 알아보자. 


## Cluster 영역

클러스터  영역의 설정은 클러스터 전체에 적용되는 설정이다. cluster.name: my-application의 경우 이름을 설정하는 항목으로 클러스터를 구성하는 모든 노드들은 동일한 클러스터 이름을  사용해야한다. 클러스터 이름을 변경하려면 클러스터내의 모든 노드를 재시작해야 하기 때문에 처음부터 신중해야 한다. 

## Node영역

Node 영역은 해당 노드메낭 적용되는 설정이다. 

node.name: node-1
node.attr.rack: r1

노드의 이름은 클러스내에서 유일해야 한다. ES에는 ${HOSTNAME}이라는 노드의 호스트명을 인식할 수 있는 변수값을 미리 정의해 놓았기 때문에 다음과 같이 설정하면 자동으로 노드의 이름이 호스트명과 같아져서 다른 노드들과 겹치지 않게 설정할 수 있다. 

```
node.name: ${HOSTNAME}$
```

노드 이름 또한 운영 중에는 변경이 불가능하며, 변경하려면 노드를 재시작해야 한다. 클러스터에서는 노드 하나가 서비스에서 제외된다고 큰 문제는 없지만 이름을 변경하기 위해선 불필요하게 재시작이 필요하기에 신중히 설정하는 것이 좋다. 

## Path 영역

데이터와 로그의 저장 위치와 관련된 설정이다. 

```
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
```

path.data는 노드가 가지고 있을 문서를 저장할 경로를 설정하는 항목이다.  4장에서 본것처럼 색인이 완료된 문서들은 세그먼트에 파일로 저장되는데 이 파일이 위치하게 될 경로다. 

path.logs 영역은 ES에서 발생하는 로그를 저장할 경로로 설정하는 항목이다.

Paths 영역은 elasticsearch.yml의 기본값들 중에서 유일하게 주석처리가 없는 영역이다. 사용자가 색인한 문서와 ES에서 발생하는 애플리케이션 로그를 저장하기 위해 반드시 설정되어야 하는 값들이기 때문이다. 참고로 이 항목들이 설정되어 있지 않으면 ES가 실행되지 않는다.

path.data는 참고로 복수의 경로에 나누어서 생성이 가능하다. 
```
path.data: /var/lib/elasticsearch/data1, /var/lib/elasticsearch/data2 
```

즉 어떤 문서는 data1, 다른 문서는 data2에 저장된다. 여러개의 디스크를 가지고 있는 노드에서 이 방법으로 여러 디스크에 분산해서 자장하면 성능상 이점이 있다. 하지만 관리가 복잡해지고, 두개의 경로 중 어느 하나가 문제가 발생했을때, 어떤 문서가 영향을 받았는지 확인하기 어렵다는 문제가 있다. 

## Memory 영역

Memory 영역에는 ES 프로세스에 할당되는 메모리 영역을 어떻게 관리할 것인지 간략하게 설정할 수 있다. 

bootstrap.memory_lock: true 는 시스템의 스왑 메모리 영역을 사용하지 않도록 하는 설정이다. ES는 최대한 스왑 메모리 영역을 사용하지 않도록 권고하고 있다. 

이 설정으로 스왑 영역을 사용하지 않으면 성능을  보장할 수 있지만, 시스템의 메모리가 부족한 경우에는 OutOfMemory 에러를 일으켜 노드의 장애로 이어질 수 있다. 

대부분의 경우는 문제 없지만, JVM 힙 메모리의 용량이 시스템 메모리 용량의 절반 이상이 된다면 OOM 에러가 발생할 수 있기 때문에 조심해야 한다. 

### limits.conf

이 옵션을 사용하려면 elasticsearch.yml 뿐만아니라 OS의 /etc/security/limits.conf 파일도 수정해야 한다. 

```
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
```

elasticsearch soft memlock unlimited과 같이 memlock 항목은 항상 unlimited로 되어야 한다. 맨 앞에 elasticsearch는 ES 프로세스를 실행시키는 계정의 이름이다. 

1장에서 봤던 yum이나 rpm 같은 패키지 매니저를 이용해서 설치하면 ES 유저가 자동으로 생성된다. ES 프로세스를 elasticsearch 계정으로 실행할 수 있도록 설정한다. 

만약 다른 방식으로 설치해서 ES 프로세스를 다른 계정으로 실행한다면 elasticsearch가 아닌 계정 이름으로 변경해주면 된다. 또한 systemd로 프로세스를 시작하면 아래와 같은 설정이 추가로 필요하다. 

```
sudo vi /etc/systemd/system/elasticsearch.service.d/override.conf
[Service]
LimitMEMLOCK = infinity
sudo systemctl deamon-reload
```
위와 같이 추가설정을 하지 않으면 bootstrap.memory_lock: true로 설정했을때 ES 프로세스가 시작되지 않으니 주의하자. 

## Network 영역

Network 영역은 ES 애플리케이션이 외부와 통신할때 사용하게 될 IP주소를 설정하는 항목이다. 외부와 통신뿐만 아니라 노드간의 통신에도 Network 영역의 설정값이 기반이 되기 때문에 주의하도록 하자. 

network.host : 192.168.0.1은 ES에서 사용하게될 IP 주소를 설정한다. 외부에 노출하지 않고 서버 내부에서만 사용할 수 있는 127.0.0.1과 같은 로컬 IP를 사용할 수도 있고, 외부 통신을 가능하게 하기 위해서 서버에서 사용하는 IP를 사용할 수도 있다. 만약 두 가지 모두를 사용하고 싶다면 0.0.0.0의 IP주소를 사용할 수도 있다. 

http.port:9200 은 ES 애플리케이션이 사용하게 될 포트이다. 

### Network.host

network.host를 설정하면 해당 서버 노드의 ES가 본 hostname 또는 ip 주소로 할당되고 클러스터 내의 다른 노드들에게도 이 주소를 사용하게 된다. 

network.host는 서버의 내/외부 주소를 모두 동시에 지정하는데 만약 ES 클러스터 노드간에 사용하는 내부 주소와 클러스터 외부에서 접근하는 주소를 다르게 설정하고자 하면 network.bind_host와 network.publish_host을 설정하면 된다. 

bind_host는 외부에서 들어오는 요청을 듣기 위해 할당되는 ip주소이다. 반면에 publish_host는 클러스터 내부에서 ES 노드간 통신으로 사용되는 ip주소이다. 따라서 publish_host는 서버 노드마다 가지는 ip주소이다. 

다시 돌아와서 network.host의 기본 설정은 _local_로 Elasticsearch 노드가 개발 모드로 실행을 뜻한다. 만약에 실제 사용할 IP 주소로 변경하게 되면 그 때부터는 운영 모드로 실행이 되며 노드를 시작할 때 부트스트랩 체크를 하게 된다.

클러스터를 실제로 구축할때는 대부분 network.host 항목을 설정하기 보다는 network.bind_host와 network.publish_host 두 항목을 따로 설정하는 경우가 많다. 

만약 한 인스턴스 노드의 network.host를 10.10.10.10으로 설정했다고 가정하자. 그러면 내부적으로 bind_host, publish_host 모두 10.10.10.10으로 설정된 것이다. 사실 기능적으로는 통신에는 문제가 없다. 외부에서는 해당 서버의 bind_host로 접근이 가능하고 ES 클러스터 노드 간에도 10.10.10.10으로 접근하게 된다. 

그런데 문제는 해당 서버 내부에서 localhost(127.0.0.1) 내부 도메인을 이용해 ES를 사용할 수 없다는 것이다. 

```
curl http://localhost:9200
curl : Faild to connect to localhost port 9200: Connection refused
```

이 이슈는 localhost로 접근시 자동으로 변환되는 127.0.0.1 ip주소로는 ES에 연결할 수 없기 때문이다. 즉 bind_host에 해당하는 10.10.10.10으로만 접근이 가능한 것이다. 사실 내부에서 localhost 대신에 10.10.10.10이라는 IP를 사용해서 호출해도 되지만, 기본 ES 설정이 localhost를 이용하는 경우가 많아 localhost를 호출할 수 있게 하는것이 편하다. 

이럴땐 해결방법으로 network.host를 0.0.0.0으로 설정하면 된다. 0.0.0.0으로 설정하면 ES가 bind_host로  10.10.10.10과 127.0.0.1 두 IP를 모두 사용할 수 있도록 해준다. 사실 0.0.0.0은 어떤 ip주소로 접근해도 포트만 일치하면 접근이 가능하다는 의미다. ( `0.0.0.0` is an acceptable IP address and will bind to all network interfaces)

덕분에 localhost로 API를 호출할 수 있게 된다.  참고로 network.host를 0.0.0.0를 설정하면 discover.seed_hosts, cluster.initial_master_nodes와 같은 설정을 반드시 해주어야 한다. 안해주면 서비스가 비동작 한다.

그렇다면 network.host를 0.0.0.0으로 하면 모든 이슈가 해결될까? 그건 아니다. 클러스터 노드들이 사용하는 publish_host도 0.0.0.0으로 설정이 되는것을 주의하자. 모든 서버의 network.host가 0.0.0.0이 되면 자동적으로 모든 publish.host가 0.0.0.0이 되고 ES 노드간 통신이 불가하게 된다.

자 이제 지금까지 내용을 정리해보자. 우리의 요구사항은 클라이언트의 외부 요청은 서버 자신의 IP 뿐만 아니라 localhost 내부 도메인인 127.0.0.1로도 받을 수 있어야 한다. 그리고 publish_host도 서버 자신의 IP도 자연스럽게 제공해야 한다. 

결론적으로 network.host는 주석처리하고 클라이언트의 요청을 처리하기 위한 IP로 network.bind_host는 0.0.0.0으로, 클러스터 내부의 노드 간의 통신에 사용하기 위한 network.publish_host는 노드 자신의 IP로 설정해야 한다.

## Discovery 영역

Discovery 영역은 노드간의 클러스터링을 위해 필요한 설정이다. 

discovery.zen.ping.unicast.hosts: ["host1", "host2"]의 경우는 클러스터링을 위한 다른 노드들의 이름을 나열한다. 배열 형식으로 두 대이상을 나열해도 된다. 

discovery.zen.minimum_master_nodes:2 는 클러스터링을 구축하기 위해 필요한 최소한의 마스터 노드 대수를 설정한다. 

ES 애플리케이션이 최초 구동될때 Discovery영역 설정을 읽어 어떤 노드들과 클러스터링을 구축하게되는지 확인한다. 아래 상황을 상정해보자. 

```
discovery.zen.ping.unicast.hosts: ["10.10.10.10"]
discovery.zen.minimum_master_nodes:2
```

ES가 실행되면 "10.10.10.10"인 서버에 ES가 동작 중인지 확인한다. 그리고 서버로 부터 현재 구축된 클러스터 정보를 가져온다. 만약 10.10.10.10 서버가 현재 이미 다른 클러스터에 합류한 상태라면 해당 클러스터에 대한 모든 정보를 받아온다. 그리고 이 정보들 중 마스터 노드의 개수가 2개 이상인지 확인하여 2개 이상이라면 성공적으로 클러스터에 합류하게 된다. 

discovery.zen.minimum_master_nodes는 굉장히 중요한 값인데, 이 클러스터를 유지하기 위한 최소한의 마스터 대수이다. 그리고 split brain이라는 현상을 막기 위해 반드시 필요한 설정이다. 

### Split Brain

3대의 마스터 노드와 3대의 데이터 노드가 구성되어 있다고 하자. 이 클러스터는 minimum_master_nodes를 따로 설정하지 않고 기본값인 1로 사용하고 있다. 

이 상황에서 네트워크 장애가 발생하여 하나의 마스터 노드와 나머지 두 대의 마스터 노드간의 통신 연결이 발생했다고 하자. 그러면 서로 다른 마스터 노드를 가진 두 개의 클러스터가 생겨날 수도 있다. 두 클러스터 모드 클러스터를 유지하기 위한 최소한의 마스터 노드가 1이기 때문에 이러한 현상이 발생한 것이다. 이렇게 클러스터를 관할하는 brain인 마스터 노드가 나뉘는 현상을 split brain 현상이라고 한다. 

하지만 minimum_master_nodes를 과반수인 2로 설정하면 네트워크 단절이 발생해도 하나의 마스터노드로는 클러스터를 만들 수 없어 두개의 클러스터가 생겨나는 상황은 피할 수 있다. 이러한 이유로 ES에서는 discovery.zen.minimum_master_nodes값을 과반수로 설정하도록 권고 하고 있다. 

```
total number of master-eligible nodes / 2 + 1
```

## Gateway 영역

Gateway 설정은 클러스터 복구와 연관된 내용이 포함된다. 

gateway.recover_after_nodes는 최소 몇 개의 노드가 정상적인 상태일때 인덱스 데이터의 복구를 시작할것인지 설정한다. ES 버전 업그레이드를 진행하거나 전체 노드 장애로 인해 클러스터 내의 모든 노드를 다시 시작해야할때가 있다. 이런 작업을 Full Cluster Restart라고 부르며, 재시작한 노드들은 순차적으로 다시 클러스터링을 진행한다. 클러스터링을 시작하면 클러스터 내의 인덱스 데이터들을 복구하기 시작하는데, 이때 사용자가 지정한 노드의 수만큼 노드들이 복귀하였을때 부터 인덱스 데이터의 복구를 시작하도록 하는 설정이다. 

사실 recover_after_master_nodes와 recover_after_data_nodes로 나위어 있어서 master, data의 롤에 따라 복귀 수를 별도로 지정할 수 있다. 

## Various 영역

action.destructive_requires_name: true

클러스터에 저장되는 인덱스를 _all이나 wildcard 표현식으로 삭제할 수 없도록 막는 설정이다. 인덱스를 삭제할때 사용자의 실수로 전체 인덱스나 많은 인덱스가 한번에 삭제되지 못하게 하는 대표적인 방법이다. 

# 노드의 역할 정의

elasticsearch.yml 파일의 각 항목을 어떻게 설정하느냐에 따라 클러스터의 롤이 바뀐다. 이제 노드 역할을 설정하는 방법을 알아보자. 

```
node.master // 마스터노드 설정여부 
node.data: false // 데이터노드 설정여부
node.ingest: false // 인제스트노드 설정여부
// 코디네이트 노드 설정 없음
```

master, data, ingest 역할의 기본값은 true이다. 이 말은 아무런 설정하지 않은 노드는 마스터, 데이터, 인제스트 노드의 역할을 모두 한다는 이야기다. 역할을 사용하지 않으려면 false로 설정한다. 


## 마스터 노드 설정

마스터 노드로 설정하려면 master를 제외한 나머지를 false로 세팅한다. 
```
node.master:true 
node.data: false 
node.ingest: false
```

위와 같이 설정된 노드는 마스터가 될 자격이 있는 상태로 클러스터에 합류한다. ES 클러스터에 존재하는 마스터 노드들은 모두 마스터가 될 자격을 가지고 있는 노드들이다. 실질적으로는 클러스터 관리를 담당하는 마스터 노드는 한대이지만 말이다. 

마스터 노드에 장애가 발생하여 클러스터로 부터 분리가 되면, 나머지 가격을 부여받은 마스터 노드들 중 하나가 새로운 마스터로 승격된다. 하지만 남아있는 마스터 노드의 수가 minimum_mastser_nodes를 만족하지 못하면 클러스터 응답 불능 상태가 되니 주의하자. 

## 데이터 노드 설정

```
node.master: false
node.data: true 
node.ingest: false
```

데이터 노드로 사용할 때에는 위와 같이 설정하면 된다. 이제 실제 사용자가 색인 요청한 문서를 저장하고 검색 요청에 결과를 응답하는  데이터 노드가 된다. 

## 인제스트 노드 설정

```
node.master: false
node.data: false
node.ingest: true
```

인제스트 노드 설정은 위와 같다. 이제 해당 노드는 사용자의 색인 요청 문서에 대해 사전 처리만 진행하고 결과를 데이터 노드에 넘긴다. 


## 코디네이터 노드 설정

코디네이터 역할 설정은 따로 없고 모든 롤 옵션을 false로 설정하면 된다. 코데네이터 노드는 사용자의 요청을 받아 이를 처리해야할 데이터 노드에 전달하고 노드로 부터 받은 검색 결과를 하나로 취합해서 돌려준다. 

```
node.master: false
node.data: false
node.ingest: false
```

그렇다면 코디네이터는 왜 필요할까? 코디네이터를 별도로 분리하는 가장 큰 이유는 데이터 노드가 코디네이터 노드 역할과 데이터 노드의 역할을 동시에 하여 부하가 커지는 것을 방지하기 위함이다. 

하나의 노드가 데이터, 코디네이터 역할을 모두 할 수 있는 노드라고 하자. 이 노드에 요청이 들어왔고 스스로가 데이터 노드이기 때문에 자신이 가진 데이터 중에 검색 결과가 있는지 확인해본다. 그리고 이 데이터 노드의 검색결과를 데이터 큐에 저장하게 된다. 

해당 노드의 데이터만으로 올바른 검색결과를 낼수 있으면 좋겠지만, 해당 노드가 아닌 다른 데이터 노드로 부터 검색 결과를 모두 받아 취합된 검색 결과를 반환해야 하는 경우도 있다.  이때 검색 결과 취합을 위해 코디네이터 노드의 메모리 공간은 코디네이트 큐이다. 

정리하자면, 두 역할을 하는 노드는 데이터 큐와 코디네이트 큐가 동시에 필요하다. 이는 힙 메모리 사용의 증가를 뜻하는데, 데이터 노드는 색인 작업만으로도 충분히 많은 힙 메모리가 필요하다. 여기서 검색 결과 취합을 위한 힙 메모리를 추가한다면 OOM 현상이 쉽게 발생한다. 특히 aggregate API를 통한 통계 작업 요청이 빈번히 발생하는 클러스터는 사용량을 고려하여 코디네이트 노드 역할을 분리하는 것이 좋다. 


> 클라이언트 노드
> 코디네이트 역할을 하는 노드를 클라이언트 노드라고도 한다. 클라이언트 노드를 분리할때에는 데이터 노드들로부터 취합된 데이터를 모두 담을 정도로 충분한 리소스를 확보해야 한다. 한번의 요청에 한대의 클라이언트 노드만 동작하기 때문에 다수의 클라이언트 노드를 사용하더라도 각 클라이언트 노드는 모든 데이터 노드에서 취합된 데이터 결과를 담을 수 있는 수준이어야 한다. 


# jvm.options 설정 파일

ES는 자바로 만들어진 애플리케이션이기 때문에 힙 메모리, GC 방식등과 같은 JVM 설정이 필요하다. 이는 ES의 성능에 결정적 역할을 하기 때문에 잘 공부해두자. 

-Xms1g, -Xmx1g는 JVM에서 사용하게 될 힙 메모리의 크기를 설정하는 항목이다. 최소값은 Xms, 최대값은 Xmx이다. 동적으로 메모리 크기를 추가 할당 가능하지만 그 할당 과정에서 성능이 느려지기 때문에 두 값을 같은 값으로 두는게 좋다. 

대부분의 설정은 힙 메모리 크기만 설정해주면 된다. GC 설정의 경우, 성능에 큰 영향을 주기 때문에 정확히 이해하고 수정해야 한다. 

힙 메모리 크기는 ES 공식 문서에 따르면 가능한 32GB를 넘지 말것,ㅍOS 전체 메모리의 절반 정도까지를 힙메모리로 설정할것을 권고하고 있다. 

##  Compressed Ordinary Object Pointer

JVM은 연산을 위한 데이터를 저장하기 위한 공간으로 힙 메모리를 사용한다. 이때 저장 데이터들을 오브젝트라 부르고 이 오브젝트를 가리키는 메모리 상의 주소를 OOP(Ordinary Object Pointer)라는 구조체에 저장한다 OOP는 시스템 아키텍처에 따라 32bit 혹은 64bit까지 주소 공간을 가리킬 수 있는데, 32bit라면 최대 4GB까지 메모리 주소공간을 가리킨다. 

64bit의 경우 이론상 훨씬 넓은 메모리 주소공간을 사용할 수 있지만 더 많은 연산과 더 많은 메모리 공간을 필요하기 때문에 성능 측면에서는 32bit보다 떨어질 수 밖에 없다. 그래서 JVM은 시스템 아키텍처가 64bit라고 해도 확보해야할 메모리 영역이 4GB보다 작다면 32비트 기반의 OOP를 사용하여 성능을 확보한다. 

문제는 4GB 보다 클 경우에 발생한다. 32bit로는 4GB가 넘는 메모리 공간을 인식할 수 없기 때문이다. 그리하여 JVM은  Compressed OOP를 통해 32bit 기반의 OOP를 사용하되 4GB이상의 영역을 표현할 수 있도록 했다. 

Native OOP에서는 주소 공간 사이의 GAP을 1bit단위로 표현했지만, Compressed OOP에서는 주소 공간 사이 최소단위를 8bit단위로 표현한다. 즉 Native OOP보다 8배 더 많은 주소 공간을 표시하게 되고, 32GB까지 힙 메모리 영역을 인식하게 된다. 

## 페이지 캐시

다음으로 전체 메모리의 절반 정도를 힙 메모리로 할당하도록 권고하는 이유를 알아보자. ES는 색인된 데이터를 세그먼트라는 물리적 파일로 저장한다. 파일이기 때문에 어쩔 수 없이 I/O작업이 많이 발생한다. 

I/O 작업은 굉장히 느린작업이기 때문에 OS에서는 이러한 성능 저하를 피하기 위해 파일의 내용을 최대한 메모리에 저장해놓는 패이지 캐시 기법을 사용한다. 

페이지 캐시는 애플리케이션들이 사용하지 않는 미사용 메모리를 활용해서 동작하기 때문에 페이지 캐시를 최대한 활용하기 위해서는 애플리케이션이 사용하는 메모리를 줄이는 것이 좋다. 특히 ES와 같이 I/O 작업이 빈번한 경우 가급적 많은 메모리를 페이지 캐시로 사용하여 




# 클러스터 사용하기






 




 




> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE3MTY0NTkxOTksLTEwNTU5MzA4NjcsNT
kzNDg2MTM2LDEzNjc1MzQwNTQsMTQ3MTczNjQwLC0xNjY1OTUz
MTU5LDMwNzQ3MzE1LDMwNTQzMjcxNCwtMjg0MDkyNzYwLC0xMj
g4MzE4MDIwLC0xNjkzNDA1NDM5LC0xNzY4MTExNDE5LDEwMjk1
ODQzLDc0NDcxMjAzNywxODkxODgyMjY2LC0zMjc4MzI1NDEsLT
E4NDg4Njk1MjEsLTkxNTQyMTEzNSwtOTE1MTc4NTUyLDEzMjcx
NDM2OF19
-->