# Overview

이번 장에서는 클러스터를 구축하는 방법을 알아보자. 먼저 ES에서 가장 중요한 환경 설정 파일인 elasticsearch.yml 파일을 통해서 어떤 항목들을 설정할 수 있는지, 그리고 성능에 중요한 영향을 미치는 jvm.options 파일을 통해서는 어떤 항목들을 설정할 수 있는지 살펴보자. 

* elasticsearch.yml 파일을 통해 설정할 수 있는 항목들
* jvm.options 파일을 통해 설정할 수 있는 항목들
* 클러스터 구축을 위해 노드들의 역할을 설정하는 방법
* 고가용성을 고려하여 설계된 클러스터를 사용하고 검증하는 방법

# elasticsearch.yml 설정 파일

elasticsearch.yml 파일은 ES를 구성하기 위해 기본이 되는 환경 설정 파일이다. 아래 코드는 ES를 처음 설치했을 때의 기본설정이다. 

```
# ---- Cluster ----
#
# cluster.name: my-application
#
# ---- Gateway ----
#
# gateway.recover_after_nodes: 3
#
# ---- Various ----
#
# action.destructive_requires_name : true
#
```

대부분 설정이 주석으로 처리되어 있으며, 해당 설정에 대한 간략한 설명이 주석으로 제공된다. 기본으로 제공되는 설정과 더불어 필수적으로 알아야 하는 여러 설정을 자세히 알아보자. 


## Cluster 영역

클러스터  영역의 설정은 클러스터 전체에 적용되는 설정이다. cluster.name: my-application의 경우 이름을 설정하는 항목으로 클러스터를 구성하는 모든 노드들은 동일한 클러스터 이름을  사용해야한다. 클러스터 이름을 변경하려면 클러스터내의 모든 노드를 재시작해야 하기 때문에 처음부터 신중해야 한다. 

## Node영역

Node 영역은 해당 노드메낭 적용되는 설정이다. 

node.name: node-1
node.attr.rack: r1

노드의 이름은 클러스내에서 유일해야 한다. ES에는 ${HOSTNAME}이라는 노드의 호스트명을 인식할 수 있는 변수값을 미리 정의해 놓았기 때문에 다음과 같이 설정하면 자동으로 노드의 이름이 호스트명과 같아져서 다른 노드들과 겹치지 않게 설정할 수 있다. 

```
node.name: ${HOSTNAME}$
```

노드 이름 또한 운영 중에는 변경이 불가능하며, 변경하려면 노드를 재시작해야 한다. 클러스터에서는 노드 하나가 서비스에서 제외된다고 큰 문제는 없지만 이름을 변경하기 위해선 불필요하게 재시작이 필요하기에 신중히 설정하는 것이 좋다. 

## Path 영역

데이터와 로그의 저장 위치와 관련된 설정이다. 

```
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
```

path.data는 노드가 가지고 있을 문서를 저장할 경로를 설정하는 항목이다.  4장에서 본것처럼 색인이 완료된 문서들은 세그먼트에 파일로 저장되는데 이 파일이 위치하게 될 경로다. 

path.logs 영역은 ES에서 발생하는 로그를 저장할 경로로 설정하는 항목이다.

Paths 영역은 elasticsearch.yml의 기본값들 중에서 유일하게 주석처리가 없는 영역이다. 사용자가 색인한 문서와 ES에서 발생하는 애플리케이션 로그를 저장하기 위해 반드시 설정되어야 하는 값들이기 때문이다. 참고로 이 항목들이 설정되어 있지 않으면 ES가 실행되지 않는다.

path.data는 참고로 복수의 경로에 나누어서 생성이 가능하다. 
```
path.data: /var/lib/elasticsearch/data1, /var/lib/elasticsearch/data2 
```

즉 어떤 문서는 data1, 다른 문서는 data2에 저장된다. 여러개의 디스크를 가지고 있는 노드에서 이 방법으로 여러 디스크에 분산해서 자장하면 성능상 이점이 있다. 하지만 관리가 복잡해지고, 두개의 경로 중 어느 하나가 문제가 발생했을때, 어떤 문서가 영향을 받았는지 확인하기 어렵다는 문제가 있다. 

## Memory 영역

Memory 영역에는 ES 프로세스에 할당되는 메모리 영역을 어떻게 관리할 것인지 간략하게 설정할 수 있다. 

bootstrap.memory_lock: true 는 시스템의 스왑 메모리 영역을 사용하지 않도록 하는 설정이다. ES는 최대한 스왑 메모리 영역을 사용하지 않도록 권고하고 있다. 

이 설정으로 스왑 영역을 사용하지 않으면 성능을  보장할 수 있지만, 시스템의 메모리가 부족한 경우에는 OutOfMemory 에러를 일으켜 노드의 장애로 이어질 수 있다. 

대부분의 경우는 문제 없지만, JVM 힙 메모리의 용량이 시스템 메모리 용량의 절반 이상이 된다면 OOM 에러가 발생할 수 있기 때문에 조심해야 한다. 

### limits.conf

이 옵션을 사용하려면 elasticsearch.yml 뿐만아니라 OS의 /etc/security/limits.conf 파일도 수정해야 한다. 

```
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
```

elasticsearch soft memlock unlimited과 같이 memlock 항목은 항상 unlimited로 되어야 한다. 맨 앞에 elasticsearch는 ES 프로세스를 실행시키는 계정의 이름이다. 

1장에서 봤던 yum이나 rpm 같은 패키지 매니저를 이용해서 설치하면 ES 유저가 자동으로 생성된다. ES 프로세스를 elasticsearch 계정으로 실행할 수 있도록 설정한다. 

만약 다른 방식으로 설치해서 ES 프로세스를 다른 계정으로 실행한다면 elasticsearch가 아닌 계정 이름으로 변경해주면 된다. 또한 systemd로 프로세스를 시작하면 아래와 같은 설정이 추가로 필요하다. 

```
sudo vi /etc/systemd/system/elasticsearch.service.d/override.conf
[Service]
LimitMEMLOCK = infinity
sudo systemctl deamon-reload
```
위와 같이 추가설정을 하지 않으면 bootstrap.memory_lock: true로 설정했을때 ES 프로세스가 시작되지 않으니 주의하자. 

## Network 영역

Network 영역은 ES 애플리케이션이 외부와 통신할때 사용하게 될 IP주소를 설정하는 항목이다. 외부와 통신뿐만 아니라 노드간의 통신에도 Network 영역의 설정값이 기반이 되기 때문에 주의하도록 하자. 

network.host : 192.168.0.1은 ES에서 사용하게될 IP 주소를 설정한다. 외부에 노출하지 않고 서버 내부에서만 사용할 수 있는 127.0.0.1과 같은 로컬 IP를 사용할 수도 있고, 외부 통신을 가능하게 하기 위해서 서버에서 사용하는 IP를 사용할 수도 있다. 만약 두 가지 모두를 사용하고 싶다면 0.0.0.0의 IP주소를 사용할 수도 있다. 

http.port:9200 은 ES 애플리케이션이 사용하게 될 포트이다. 

### Network.host

network.host를 설정하면 해당 서버 노드의 ES가 본 hostname 또는 ip 주소로 할당되고 클러스터 내의 다른 노드들에게도 이 주소를 사용하게 된다. 

network.host는 서버의 내/외부 주소를 모두 동시에 지정하는데 만약 ES 클러스터 노드간에 사용하는 내부 주소와 클러스터 외부에서 접근하는 주소를 다르게 설정하고자 하면 network.bind_host와 network.publish_host을 설정하면 된다. 

bind_host는 외부에서 들어오는 요청을 듣기 위해 할당되는 ip주소이다. 반면에 publish_host는 클러스터 내부에서 ES 노드간 통신으로 사용되는 ip주소이다. 따라서 publish_host는 서버 노드마다 가지는 ip주소이다. 

다시 돌아와서 network.host의 기본 설정은 _local_로 Elasticsearch 노드가 개발 모드로 실행을 뜻한다. 만약에 실제 사용할 IP 주소로 변경하게 되면 그 때부터는 운영 모드로 실행이 되며 노드를 시작할 때 부트스트랩 체크를 하게 된다.

클러스터를 실제로 구축할때는 대부분 network.host 항목을 설정하기 보다는 network.bind_host와 network.publish_host 두 항목을 따로 설정하는 경우가 많다. 

만약 한 인스턴스 노드의 network.host를 10.10.10.10으로 설정했다고 가정하자. 그러면 내부적으로 bind_host, publish_host 모두 10.10.10.10으로 설정된 것이다. 사실 기능적으로는 통신에는 문제가 없다. 외부에서는 해당 서버의 bind_host로 접근이 가능하고 ES 클러스터 노드 간에도 10.10.10.10으로 접근하게 된다. 

그런데 문제는 해당 서버 내부에서 localhost(127.0.0.1) 내부 도메인을 이용해 ES를 사용할 수 없다는 것이다. 

```
curl http://localhost:9200
curl : Faild to connect to localhost port 9200: Connection refused
```

이 이슈는 localhost로 접근시 자동으로 변환되는 127.0.0.1 ip주소로는 ES에 연결할 수 없기 때문이다. 즉 bind_host에 해당하는 10.10.10.10으로만 접근이 가능한 것이다. 사실 내부에서 localhost 대신에 10.10.10.10이라는 IP를 사용해서 호출해도 되지만, 기본 ES 설정이 localhost를 이용하는 경우가 많아 localhost를 호출할 수 있게 하는것이 편하다. 

이럴땐 해결방법으로 network.host를 0.0.0.0으로 설정하면 된다. 0.0.0.0으로 설정하면 ES가 bind_host로  10.10.10.10과 127.0.0.1 두 IP를 모두 사용할 수 있도록 해준다. 사실 0.0.0.0은 어떤 ip주소로 접근해도 포트만 일치하면 접근이 가능하다는 의미다. ( `0.0.0.0` is an acceptable IP address and will bind to all network interfaces)

덕분에 localhost로 API를 호출할 수 있게 된다.  참고로 network.host를 0.0.0.0를 설정하면 discover.seed_hosts, cluster.initial_master_nodes와 같은 설정을 반드시 해주어야 한다. 안해주면 서비스가 비동작 한다.

그렇다면 network.host를 0.0.0.0으로 하면 모든 이슈가 해결될까? 그건 아니다. 클러스터 노드들이 사용하는 publish_host도 0.0.0.0으로 설정이 되는것을 주의하자. 모든 서버의 network.host가 0.0.0.0이 되면 자동적으로 모든 publish.host가 0.0.0.0이 되고 ES 노드간 통신이 불가하게 된다.

자 이제 지금까지 내용을 정리해보자. 우리의 요구사항은 클라이언트의 외부 요청은 서버 자신의 IP 뿐만 아니라 localhost 내부 도메인인 127.0.0.1로도 받을 수 있어야 한다. 그리고 publish_host도 서버 자신의 IP도 자연스럽게 제공해야 한다. 

결론적으로 network.host는 주석처리하고 클라이언트의 요청을 처리하기 위한 IP로 network.bind_host는 0.0.0.0으로, 클러스터 내부의 노드 간의 통신에 사용하기 위한 network.publish_host는 노드 자신의 IP로 설정해야 한다.

## Discovery 영역

Discovery 영역은 노드간의 클러스터링을 위해 필요한 설정이다. 

discovery.zen.ping.unicast.hosts: ["host1", "host2"]의 경우는 클러스터링을 위한 다른 노드들의 이름을 나열한다. 배열 형식으로 두 대이상을 나열해도 된다. 

discovery.zen.minimum_master_nodes:2 는 클러스터링을 구축하기 위해 필요한 최소한의 마스터 노드 대수를 설정한다. 

ES 애플리케이션이 최초 구동될때 Discovery영역 설정을 읽어 어떤 노드들과 클러스터링을 구축하게되는지 확인한다. 아래 상황을 상정해보자. 

```
discovery.zen.ping.unicast.hosts: ["10.10.10.10"]
discovery.zen.minimum_master_nodes:2
```

ES가 실행되면 "10.10.10.10"인 서버에 ES가 동작 중인지 확인한다. 그리고 서버로 부터 현재 구축된 클러스터 정보를 가져온다. 만약 10.10.10.10 서버가 현재 이미 다른 클러스터에 합류한 상태라면 해당 클러스터에 대한 모든 정보를 받아온다. 그리고 이 정보들 중 마스터 노드의 개수가 2개 이상인지 확인하여 2개 이상이라면 성공적으로 클러스터에 합류하게 된다. 

discovery.zen.minimum_master_nodes는 굉장히 중요한 값인데, 이 클러스터를 유지하기 위한 최소한의 마스터 대수이다. 그리고 split brain이라는 현상을 막기 위해 반드시 필요한 설정이다. 

### Split Brain

3대의 마스터 노드와 3대의 데이터 노드가 구성되어 있다고 하자. 이 클러스터는 minimum_master_nodes를 따로 설정하지 않고 기본값인 1로 사용하고 있다. 

이 상황에서 네트워크 장애가 발생하여 하나의 마스터 노드와 나머지 두 대의 마스터 노드간의 통신 연결이 발생했다고 하자. 그러면 서로 다른 마스터 노드를 가진 두 개의 클러스터가 생겨날 수도 있다. 두 클러스터 모드 클러스터를 유지하기 위한 최소한의 마스터 노드가 1이기 때문에 이러한 현상이 발생한 것이다. 이렇게 클러스터를 관할하는 brain인 마스터 노드가 나뉘는 현상을 split brain 현상이라고 한다. 

하지만 minimum_master_nodes를 과반수인 2로 설정하면 네트워크 단절이 발생해도 하나의 마스터노드로는 클러스터를 만들 수 없어 두개의 클러스터가 생겨나는 상황은 피할 수 있다. 이러한 이유로 ES에서는 discovery.zen.minimum_master_nodes값을 과반수로 설정하도록 권고 하고 있다. 

```
total number of master-eligible nodes / 2 + 1
```

## Gateway 영역

Gateway 설정은 클러스터 복구와 연관된 내용이 포함된다. 

gateway.recover_after_nodes는 최소 몇 개의 노드가 정상적인 상태일때 인덱스 데이터의 복구를 시작할것인지 설정한다. ES 버전 업그레이드를 진행하거나 전체 노드 장애로 인해 클러스터 내의 모든 노드를 다시 시작해야할때가 있다. 이런 작업을 Full Cluster Restart라고 부르며, 재시작한 노드들은 순차적으로 다시 클러스터링을 진행한다. 클러스터링을 시작하면 클러스터 내의 인덱스 데이터들을 복구하기 시작하는데, 이때 사용자가 지정한 노드의 수만큼 노드들이 복귀하였을때 부터 인덱스 데이터의 복구를 시작하도록 하는 설정이다. 

사실 recover_after_master_nodes와 recover_after_data_nodes로 나위어 있어서 master, data의 롤에 따라 복귀 수를 별도로 지정할 수 있다. 

## Various 영역

action.destructive_requires_name: true

클러스터에 저장되는 인덱스를 _all이나 wildcard 표현식으로 삭제할 수 없도록 막는 설정이다. 인덱스를 삭제할때 사용자의 실수로 전체 인덱스나 많은 인덱스가 한번에 삭제되지 못하게 하는 대표적인 방법이다. 

# 노드의 역할 정의

elasticsearch.yml 파일의 각 항목을 어떻게 설정하느냐에 따라 클러스터의 롤이 바뀐다. 이제 노드 역할을 설정하는 방법을 알아보자. 

```
node.master // 마스터노드 설정여부 
node.data: false // 데이터노드 설정여부
node.ingest: false // 인제스트노드 설정여부
// 코디네이트 노드 설정 없음
```

master, data, ingest 역할의 기본값은 true이다. 이 말은 아무런 설정하지 않은 노드는 마스터, 데이터, 인제스트 노드의 역할을 모두 한다는 이야기다. 역할을 사용하지 않으려면 false로 설정한다. 


## 마스터 노드 설정

마스터 노드로 설정하려면 master를 제외한 나머지를 false로 세팅한다. 
```
node.master:true 
node.data: false 
node.ingest: false
```

위와 같이 설정된 노드는 마스터가 될 자격이 있는 상태로 클러스터에 합류한다. ES 클러스터에 존재하는 마스터 노드들은 모두 마스터가 될 자격을 가지고 있는 노드들이다. 실질적으로는 클러스터 관리를 담당하는 마스터 노드는 한대이지만 말이다. 

마스터 노드에 장애가



 




 




> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4MzU3NTQ5NDksMzA1NDMyNzE0LC0yOD
QwOTI3NjAsLTEyODgzMTgwMjAsLTE2OTM0MDU0MzksLTE3Njgx
MTE0MTksMTAyOTU4NDMsNzQ0NzEyMDM3LDE4OTE4ODIyNjYsLT
MyNzgzMjU0MSwtMTg0ODg2OTUyMSwtOTE1NDIxMTM1LC05MTUx
Nzg1NTIsMTMyNzE0MzY4LDE2ODA5MjMwMDIsMTA2ODYxNjQ1Ni
wxNjk4MDg5MTIwLDY5NTMxNzQyMiwtMTI4MjkwOTQyLC0xMDgz
ODM3OTkzXX0=
-->