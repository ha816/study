# 스파크란?

스파크(Apache Spark)는 고속 범용 분산 컴퓨팅 플랫폼이다. 
* 메모리 기반으로 하둡 맵 리듀스보다 10배 ~100배 빠르다.
* 컬렉션 기반의 API로 사용자는 클러스터를 다루는 사실을 인지할 필요없다.
* 파이썬, 자바, R, 스칼라도 사용가능하다.

스파크는 또한 맵리듀스와 유사한 일괄 처리기능, 실시간 데이터 처리, SQL과 유사한 정형 데이터 처리, 그래프 알고리즘, 머신 러닝 알고리즘을 모두 단일 프레임워크로 통합했다.

하지만 일부 애플리케이션은 스파크를 사용하기 부적합하다. 분산 아키텍처이기 때문에 약간의 오버헤드가 필연적으로 발생한다. 대량의 데이터를 다룰때는 괜찮지만 단일 머신에서도 처리할 수 있는 작은 데이터를 다룰때는 작은 데이터셋의 연산에 최적화된 다른 프레임 워크를 사용하는 것이 더 효율적이다. 
또 스파크는 온라인 트랜잭션 처리 애플리케이션을 염두에 두고 설계되지 않았다. 즉 원장성을 보장해야 하는 트랜잭션을 빠르게 처리해야 하는 경우 스파크는 부적합하다. 일괄 처리 작업이나 데이터 마이닝 같은 온라인 분석 처리 작업에는 적합하다. 

## 하둡의 HDFS과 맵리듀스의 한계

HDFS(Hadoop File System)과 맵리듀스 처리 엔진으로 구성된 하둡 프레임 워크는 분산 컴퓨터팅을 최초로 대중화 하는데 성공했다. 하둡은 데이터 분산 처리에서 반드시 고민해야 하는 다음 세 가지 문제를 해결했다.

* 병렬처리(parallelization): 전체 연산을 잘게 나누어 동시에 처리하는법
* 데이터 분산(distribution): 데이터를 여러 노드로 분산하는 방법
* 장애 내성(fault tolerance): 분산 컴포넌트의 장애에 대응하는 방법

하둡은 활발히 사용되고 있지만 여러 한계를 가지고 있고 이는 맵리듀스 컴포넌트와 관련이 있다. 맵 리듀스 잡의 결과를 다른 잡에서 사용하려면 먼저 이 결과를 HDFS에 저장해야 한다. 이전 잡의 결과가 다음 작업의 입력이 되는 반복 알고리즘에는 비효율적이다. 또 많은 유형의 문제가 맵 리듀스 2단계 패러다임에 쉽게 들어맞지 않으며, 모든 문제를 일련의 맵과 리듀스 연산으로 분해할 수 없다. 

## 스파크가 가져다준 선물

스파크는 데이터를 메모리에 캐시로 저장하는 인메모리 모델이기 때문에 속도 측면으로 하둡 맵리듀스처럼 잡에 필요한 데이터를 디스크에서 매번 가져오는 것보다 훨씬 빠르다. 더욱이 동일한 반복 알고리즘과 같은 재사용하는 모듀 유형의 작업에 더욱 좋다. 

스파크 API는 맵리듀스 API 보다 훨씬 사용하기 쉽다.  전통적인 하둡에서는 맵리듀스 구현을 위해서 세개의 클래스를 정의해야 한다. 바로 Main, Mapper, Reducer 클래스다. 하지만 스파크에서는 스칼라코드 몇줄로도 코드를 완성할 수 있다. 

스파크는 대화형 콘솔인 스파크 셀을 제공한다. 이를 이용해 간단한 실험을 아거나 아이디어 데스트를 할 수 있다. 

마지막으로, 사용자는 스파크 자체 클러스터(Standalone), 하둡의 YARN 클러스터, 아파치 메소스 클러스터등 다양한 유형의 클러스터 매니저를 사용해 스파크를 이용할 수 있다. 

스파크는 일괄 분석을 염두에 두고 설계했기 때문에 공유된 데이터를 비동기적으로 갱신하는 연산에는 적합하지 않다. 또 스파크는 잡과 태스크를 시작하는데 상당한 시간을 소모하기 때문에 대량의 데이터를 처리하는 작업이 아니라면 굳이 스파크를 사용할 필요가 없다. 

# 스파크 구성 컴포넌트

스파크는 아래와 같은 다양한 컴포넌트를 통해 여러 기능이 집약된 통합 플랫폼으로 발전했다. 

## 코어

스파크 코어는 스파크 잡과 다른 스파크 컴포넌트에 필요한 기본 기능을 제공한다. 가장 중요한 개념은 스파크 API에서 핵심요소인 RDD(Resilient Distributed Dataset)이다. RDD는 분산 데이터 컬렉션(데이터 셋)을 추상화한 객체로 데이터셋에 적용할 수 있는 연산 및 변환 메서드를 함께 제공한다. RDD는 노드에 장애가 발생해도 데이터 셋을 재구성할 수 있는 복원성 기능이 있다.

스파크 코어는 HDFS, 아마존 S3등 다양한 파일 시스템에 접근 할수 있다. 또 공유변수(broeadcst variable)과 누적변수(accumulator)를 사용해 컴퓨팅 노드간 정보를 공유 할 수 있다. 이외에도 네트워킹, 보안, 스케쥴링, 및 데이터 셔플링등 기본 기능이 구현되어 있다. 

## SQL

스파크 SQL은 스파크와 하이브 SQL이 지원하는 SQL을 사용해 대규모 분산 정형 데이터를 다룰 수 있는 기능을 제공한다. JSON 파일, 관계형 데이터 베이스 테이블등 다양한 정형 데이터를 읽고 쓰는데 스파크 SQL을 사용할 수 있다. 

스파크 SQL은 DataFrame과 Dateset에 적용된 연산을 일정 시점에 RDD 연산으로 변환해 일반 스파크 잡으로 실행한다. SQL은 카탈리스트라는 쿼리 최적화 프레임 워크를 제공하며, 사용자가 직접 최적화 규칙을 적용해 프레임워크를 확장할 수 도 있다.  

## Streaming

스파크 스트리밍은 다양한 데이터 소스에서 유입되는 실시간 스트리밍 데이터를 처리하는 프레임 워크이다. 지원하는 스트리밍 소스에는 HDFS, 아파크 카프카, 아파치 플럼, 트위터 등이 있다. 

스트리밍은 장애가 발생하면 연산 결과를 자동으로 복구한다(스트리밍 데이터를 처리할때는 장애 복원성이 매우 중요하다). 스트리밍은 또한 이산 스트림(Discretized Stream, DStream) 방식으로 데이터를 표현하는데, 가장 마지막 타임 윈도안에 유입된 데이터를 RDD로 구성해 주기적으로 생성한다. 

## GraphX

그래프는 정점과 두 정점을 있는 간선으로 구성된 데이터 구조다. 그래프X는 그래프 RDD(EdgeRdd 및 VertexRDD) 형태의 그래프 구조를 만들 수 있는 다양한 기능을 제공한다.  GraphX에는 패이지 랭크, 최단경로, SVD등 그래프 이론에서 가장 중요한 알고리즘이 구현되어 있다. 

## MLlib

스파크 MLlib는 UC 버클리의 MLbase 프로젝트에서 개발한 머신 러닝 알고리즘 라이브러리이다. 
로지스틱 회귀, 나이브 베이즈 분류, 서포트 벡터 머신, 의사 결정 트리, 랜덤 포레스트, 선형 회귀, k-평균 군집화등 다양한 머신 러닝 알고리즘을 지원한다. 

스파크 MLlib를 사용해 RDD 또는 DataFrame의 데이터 셋을 변환하는 머신러닝 모델을 구현할 수 있다.

# 스파크 프로그램의 실행 과정

300MB 크기의 로그 파일이 노드 세 개로 구성된 HDFS 클러스트에 분산 저장되어 있다고 하자. HDFS는 이 파일을 자동으로 128MB의 청크로 분할하고 


# 스파크 생태계

# 가상머신 설정


> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbOTk4OTQxNDAsNTU1MDI1NTU4LDE0NDQ0Mj
g1NzksODEwMTE2MDM4LC0xOTkxMDMyOTM0LDM1NjE3MzQ2NSwy
MDkyMzY2NTMwLC0xODc5MjU3NzU3XX0=
-->