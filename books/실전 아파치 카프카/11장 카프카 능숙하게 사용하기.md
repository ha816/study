# Overview

지금까지 여러 카프카 사례와 예제를 소개했다. 하지만 지금까지 이야기 말고도 중요한 내용이 몇가지 있다. 

* 컨슈머 그룹(Consumer Group)
* 오프셋 커밋(Offset Commit)
* 파티션 재비치(Partition Reassignment)
* 파티션 수 결정에 참고사항
* 복제본 수(Replication-Factor) 결정 참고사항


# 컨슈머 그룹

## 컨슈머 그룹이란?

카프카에서는 컨슈머가 카프카 클러스터에서 메세지를 얻어 처리한다. 이때 컨슈머는 컨슈머 그룹이라 불리는 **하나 이상의 컨슈머로 이루어진 그룹을 형성하여 메세지**를 얻는다. 

컨슈머 그룹은 Group ID라는 ID로 구분된다. 이 **Group ID는 KafkaConsumer를 생성할때 지정하는 옵션으로 group.id라는 파라미터로 지정**한다. 

**컨슈머 그룹은 동일한 Group ID를 가지는 컨슈머들로 형셩**된다. 참고로 특정 컨슈머는 여러 컨슈머 그룹에 속하지 않고 항상 하나의 컨슈머 그룹에 속한다. 

카프카 클러스터에서 수신할 메시지는 컨슈머 그룹안에서 어느 하나의 컨슈머가 수신한다. 바꿔말하면, 카프카 클러스터에서 수신할 메시지를 동일 컨슈머 그룹에 속한 컨슈머 사이에서 분산하여 수신한다. 

컨슈머 그룹은 하나의 데이터 처리를 여러 컨슈머에서 분산처리하기 위해 사용된다. 

## 컨슈머에서 파티션 할당

어떤 메세지를 컨슈머 그룹의 어느 컨슈머에서 수신하는가에 대한 할당은 수신할 토픽에 존재하는 파티션과 그룹 내 컨슈머를 매핑함으로써 가능하다. 

카프카 클러스터에서 다루는 메시지는 특정 토픽 중에 특정 파티션에 반드시 포함된다. 메시지는 컨슈머 그룹에서 각 파티션에 매핑되는 컨슈머가 수신하게 된다. 

컨슈머와 파티션의 매핑에서 각 파티션에 반드시 하나의 컨슈머가 매핑되어 있다. 반대로 파티션 수에 따라 하나의 컨슈머에는 동시에 여러 파티션이 할당될 수 도 있다. 

특정 파티션에 기록된 메시지는 반드시 매핑된 특정 컨슈머가 처리하도록 고정되어 있다. 따라서 메시지를 수신하는 토픽의 파티션 보다 컨슈머 쪽이 많은 경우에는 파티션이 매핑이 되지 않은 컨슈머가 발생할 수도 있다. 

각 파티션을 컨슈머 그룹 내에 있는 어떤 컨슈머에 매핑할 것인가에 대해서는 컨슈머 그룹에 새로운 컨슈머가 가입되는 경우등 필요에 따라 변경된다. 이 결정 정책은 컨슈머가 설정되어 있는 Assignor 로직을 따른다. 

|Assignor| Class  | 할당법|
|--|--|--|
|RoundRobin| RoundRobinAssignor  | 매핑할 파티션을 컨슈머에 하나씩 차례대로 매핑|
|Range| RangeAssignor  | 매핑할 파티션을 나열하고 컨슈머 수로 영역을 분할하여 할당|
|Sticky| StickyAssignor  | 최대한 균형있게 할당하고 재할당 시에는 원래의 매핑에서 변경되지 않도록 할당한다. |

# Offset Commit

## Offset Commit이란?

카프카를 사용하는 시스템에서 컨슈머가 카프카 클러스터에서 메시지를 얻어 처리한다. 이때 컨슈머는 어느 메시지 까지 처리를 완료했는지 카프카 클러스터에 기록을 남길 수 있다. 정확히는 **다음 수신 및 처리해야 할 메시지의 오프셋을 기록한다. 이러한 기록을 남기는 처리를 오프셋 커밋**이라고 한다. 

이 오프셋은 커밋은 각 컨슈머가 카프카 클러스터에 기록을 요청함으로써 실행한다. 오프셋 커밋의 기록은 컨슈머 그룹 단위로 이루어진다. 컨슈머 그룹마다 각 토픽의 파티션에서 어느 오프셋까지 처리했는지 정보를 기록한다. 

오프셋 커밋은 처리완료 여부를 메시지 마다 기록하는 것이 아니라 처리를 완료한 메시지 중에서 최대의 오프셋을 기록하는 형태다. 이것은 카프카가 임의로 메시지를 처리하는 것이 아니라 파티션 안의 메시지를 연속적으로 처리하기 때문에 가능하다.

오프셋 커밋 정보에 의해서 컨슈머는 카프카에서의 메시지 수신 처리를 재개할때 어떤 메시지부터 재개해야하는 지 알 수 있다. 여기에는 유지보수 등 계획된 정지 뿐만아니라 장애에 의한 비정상적인 정지의 재개도 포함된다. 

재개후에 새로운 메시지만이 처리되어 불필요한 메시지를 재처리하는 것  같은 그 영향을 줄일 수 있다. 

커밋된 오프셋 정보는 __consumer_offsets라는 전용 토픽에 기록된다. 이 토픽은 일반 토픽처럼 파티션과 복제본 구조를 하고 있다. 카프카 클러스터는 오프셋 커밋 처리를 분산할 수 있으며, 여러 대의 브로커가 정지해도 데이터 손실 없이 처리 가능하다. 

참고로 오프셋 커밋 방버에는 Auto Offset Commit과 Manual Offset Commit이 있다. 각각 장단점이 있어 어느것을 사용할지는 요구사항에 따라 달라진다.

### Auto Offset Commit

자동 오프셋 커밋은 일정 간격으로 자동 오프셋 커밋을 하는 방식이다. 컨슈머의 옵션은 enable.auto.commit을 true로 하면 된다. 일정 간격은 KafkaConsumer 옵션의 auto.commit.interval.ms로 지정할 수 있으며, 기본 값은 5초다. 

자동 오프셋 커밋에는 설정된 타이밍에 카프카 클러스터에서 완료된 메시지에 대해 오프셋 커밋을 실행한다. 

자동 오프셋 커밋의 장점은 컨슈머 어플리케이션에서 오프셋 커밋을 명시적으로 하지 않아도 된다는 점이다. 즉 컨슈머 애플리케이션이 간결해진다. 

반면에 컨슈머에 장애가 발생했을때 메시지가 손실되거나 여러 메시지의 재처리 (메시지 중복)가 발생할 수 있다는 단점이 있다. 

이 방식에서는 오프셋 커밋이 일정한 간격이 이루어지기 때문에 장애가 발생한 타이밍에 따라서는 오프셋 커밋된 메시지 처리가 완료되지 않거나 여러 메시지 처리가 완료되었지만 오프셋 커밋이 이루어지지 않은 경우가 발생할 수 있다.

장애가 발생했을때 처리 중이던 메시지가 오프셋 커밋으로는 전송 되었기 때문에 처리를 재개 시켰을때 장애가 발생한 데이터가 처리가 완료된 메시지로 취급되어 장애로 재처리가 되지 않게 된다.

장애 발생시 처리가 완료된 여러 메세지에 대해 오프셋 커밋이 이루어지지 않았으므로 처리를 재개했을때 동일 메세지를 여러번 처리하게 된다. 

### Manual Offset Commit

수동 오프셋 커밋은 컨슈머 애플리케이션 안에서 Kafka Consumer의 commitSync 또는 commitAsync라는 메서드를 통해 오프셋 커밋을 실행한다.

스트림 처리 프레임워크등에서는 이 메서드를 직접 사용하지 않을 수도 있다. 

수동 오프셋 커밋의 장점은 구조를 이해하고 적절히 사용하여 메시지 손실을 발생하지 않도록 할 수 있다는 점이다. 이 방식으로 애플리케이션 안에서 언제라도 오프셋 커밋을 할 수 있다. 따라서 클러스터에서 메시지 취득 후 메시지 처리가 완료된 시점에 커밋을 할 수 있다. 

또한 컨슈머 장애 발생시 메시지 중복을 최소화할 수 있다. 수동 오프셋 커밋에서는 장애가 발생했을때 처리 중인 메세지에 대해서는 중복 가능성이 남아 있지만, 이미 처리 완료된 메시지를 포함한 메시지 중복은 피할 수 있다. 

단점으로 메시지 양에 따라 다르지만 수동 오프셋 커밋은 자주 커밋 처리를 실시함으로 카프카 클러스터의 부하가 높아진다점에 주의해야 한다. 

### 자동 오프셋 리셋

컨슈머는 앞서 언급한 오프셋 커밋 정보를 참조하여 메시지 처리를 시작할 오프셋을 결정한다. 그러나 시작할때 오프셋 커밋 기록이 없거나 기록되어 있는 오프셋이 유효하지 않는 경우 지정된 정책에 따라 초기화를 실시하여 메시지 처리를 시작할 오프셋을 결정하게 된다. 이런 오프셋 초기화 처리가 자동 오프셋 리셋이다. 

해당 옵션은 auto.offset.reset이라는 옵션으로 지정한다. 정책은 아래와 같다.

* latest
	* 해당 파티션의 가장 새로운 오프셋으로 초기화 된다. 따라서 이미 존재하는 메시지는 처리되지 않는다.
* earliest 
	* 해당 파티션에 존재하는 가장 오래된 오프셋으로 초기화된다. 클러스터에 이미 존재하는 메시지 모두에 대해 처리를 실시한다. 
* none
	* 유효한 오프셋 커밋 정보가 없는 경우에 예외를 반환한다.

자동 오프셋 리셋이 nono인 경우에 유효한 오프셋 커밋 기록이 존재하지 않을때는 예외가 반환되므로 KafkaConsumer의 seek 메서드 등으로 명시적으로 오프셋을 지정해야 한다. 

# 메시지 재배치

## 파티션 재배치란?

카프카에서의 파티션은 하나 이상의 복제본을 가지며, 카프카 클러스터 중 어느 하나의 브로커에 보관되어 있다. 일반적으로 이 복제본은 작성된 때 배치된 브로커에서 계속 보관되지만 특정 이유로 배치를 변경하고 싶은 경우가 있다. 

복제본의 배치를 변경하는 주된 이유는 카프카 클러스터 브로커를 증감 시키는 경우다. 카프카에서는 계획적인 정지나 장애로 인한 정지를 불문하고 브로커가 보유하고 있던 복제본이 다른 브로커로 자동으로 이동하지 않는다. 따라서 브로커수를 항시적으로 줄이는 경우에는 감소 시킬 브로커가 보유하고 있는 복제본을 미리 다른 브로커로 이동시켜 필요한 복제본의 수를 확보해야 한다. 

또한 브로커를 추가하여 클러스터를 확장시킬 경우에도 새로 추가한 브로커에 복제본을 배치하여 메시지의 송수신 부하를 균등하게 배분해야 한다. 이렇게 파티션의 각 복제본을 임의의 브로커에 재배치하는 작업을 파티션 재배치(Partition Reassignment)라고 한다. 

파티션 재배치로 복제본을 다른 브로커에 배치시키는 경우에 먼저 새로운 복제본을 배치할 브로커에 복제본을 생성해 동기화하고, 동기화를 완료한후 (새로 만든 복제본을 ISR에 가입) 불필요해진 복제본을 제거한다. 따라서 재배치 중에는 일반적인 메시지 송수신에 필요한 처리와 더불어 재배치로 새로 생성되는 복제본의 동기화가 필요하다. 

파티션 재배치를 할때는 평소보다도 카프카 클러스터 부하가 늘어나는 것에 주의가 필요하다. 참고로 파티션 재배치에 따른 동기화 처리에서 사용되는 네트워크 대역폭에는 제한을 걸 수 있다. 

## 파티션 재배치 방법

파티션 재배치를 실행하려면 카프카에 부속된 스크립트인 kafka-reassgin-partitions를 이용한다. 이 명령을 카프카 클라이언트에서 실행함으로써 파티션 재배치를 실행 할 수 있다. 이후로는 파티션 재배치를 실시하는 예를 소개한다.

test1이라는 토픽안에 Partition 0의 복제본이 Broker 1,2,3에 보관되어 있는 상태에서 Broker 3,4,5에 보관되는 상태로 바꾸고, Partition 1의 복제본이 Broker 3,4,5에 보관되어 있는 상태에서 Broker 1,2,3에 보관되는 상태로 변경하는 경우로 설명해보겠다. 

파티션 재배치에는 먼저 복제본을 이동시킬 파티션의 복제본 배치 정보를 기재한 JSON 파일을 작성한다. 이 JSON 파일에는 파티션 재배치 후에 각 파티션의 복제본에 대한 배치를 기재한다. 이 절에서 진행할 파티션 재배치 복제본 정보를 기재한 JSON 파일은 다음과 같다. 

```
{"version": 1, 
"partitions": [
{"topic": "test1", "partition":0, "replicas":[3,4,5]},
{"topic": "test1", "partition":1, "replicas":[1,2,3]}
]}
```

이 파일을 카프카 클라이언트에 사용자 홈 디렉터리 아래에 reassignment.json이라는 파일명으로 저장된다. 파일에는 복제본의 이동을 수반하는 파티션에 대해서만 기재되어 있으면 되고 배치를 변경하지 않는 파티션에 대해선 기재할 필요 없다. 

* version 
	* 이 파티션 재배치의 배치 정보 표기법의 버전을 나타낸다. 
* partitions
	* 복제본을 재배치하는 파티션의 정보를 말한다. 배열로 필요한 만큼 정보를 기재할 수 있다. 
* {topic: , partition, replicas}
	* 복제본의 재배치 정보이다. 첫번째 예제에서는 test1의 토픽의 partition 0의 복제본을 브로커 3,4,5에 배치한다. 기재할때 현제 복제본의 배치가 어떻게 되어있는지는 기재할 필요가 없다.

JSON 파일이 준비 되었다면 파티션 재배치를 진행한다. 
```
kafka-reassign-partitions
--zookeeper kafa-...
--reassignment-json-file ~/reassignment.json -execute
```

# 파티션 수 결정에 있어 참고 사항

카프카의 파티션은 분산처리에 있어 필요한 구조라고 설명했다. 카프카의 성능을 높이려면 파티션 수를 제대로 설정하는 것이 중요하다. 그러나 적절한 파티션 수는 구성 및 요구 사항에 따라 다르기 때문에 조심히 해야한다. 

파티션 수를 결정하는데 참고할 사항은 아래와 같다.

* 카프카 클러스터의 메시지 송수신
* 컨슈머 그룹의 할당
* 브로커가 사용하는 디스크

## 카프카 클러스터의 메시지 송수신

프로듀서와 컨슈머는 각 파티션의 복제본 중에서 하나만 존재하는 리더 복제본(Leader Repica)만이 메시지 송수신할 수 있다. 따라서 브로커 수에 비해 파티션 수가 적으면 특정 브로커에만 리더 복제본이 존재하게 된다. 

리더 복제본은 팔로워 복제본에 비해 부하가 올라가기 쉬우며, **리더 복제본이 특정 브로커에만 존재하고 있으면 카프카의 특징인 확장성을 잃게 된다.** 

브로커 6대에 파티션 수가 2개일때의 모습을 보여준다. 리더 복제본은 파티션마다 하나만 존재하기 때문에 특정 브로커에만 리더 복제본이 존재하게 된다. 

카프카의 확장성을 이용하여 브로커를 늘릴때는 새롭게 증가하는 브로커에도 리더 복제본을 배치하여 각 브로커를 균등하게 처리하기 위해 더 많은 파티션이 필요하다. 파티션 수는 카프카 클러스터를 사용하기 시작한 직후의 브로커 수뿐만 아니라 향후 확장도 고려하여 검토해야 한다. 

## 컨슈머 그룹의 할당

컨슈머 그룹도 파티션 구조에 의존하고 있으며 각 파티션을 컨슈머 그룹의 하나의 컨슈머에 할당함으로써 부하 분산을 실현하고 있다. 

따라서 컨슈머 그룹에서 기대한 대로 분산하여 메시지를 수신하기 위해서는 파티션 수가 적어도 각 컨슈머 그룹에 속하는 컨슈머보다 많아야 한다. 또한 데이터 양의 증가에 따라 컨슈머 그룹 내의 컨슈머 증가가 예상되는 경우에는 그 부분도 함께 고려하여 파티션 수를 결정해야 한다. 

## 브로커가 이용하는 디스크

브로커는 수신한 메시지를 최종적으로 디스크에 저장하여 영속화한다. 카프카는 효율적으로 디스크를 사용할 수 있도록 데이터를 최대한 순차적으로 기재하려고 한다. 따라서 브로커가 사용하는 디스크는 RAID나 JBOD등의 메커니즘을 통하지 않고 직접 사용하는 것이 좋다. 

브로커는 여러 디스크를 구분하여 사용한다. 브로커에 여러대의 디스크를 사용할때 복제본 단위로 이용할 디스크를 분리하는 구조로 되어 있다. 따라서 브로커가 보유하고 있는 본제본이 적으면 사용하지 않는 디스크가 존재하게 된다. 

1대의 브로커에 동일 파티션의 복제본을 여럿 보유하지 않기 때문에 브로커가 여러 디스크를 이용하기 위해선 그 숫자 이상의 파티션이 필요하다. 

전체 브로커에서 모든 디스크를 효율적으로 사용하기 위해선 디스크 수 이상의 파티션 수가 카프카 전체 클러스터에 필요하다.

# 복제본 수 결정에 참고사항

복제본은 내장애성을 위한 구조라고 소개했다. 복제본 수(Replica-Factor)는 카프카 클러스터가 어느정도 장애(다중 장애)에 견딜 수 있는 가에 영향을 주는 중요한 설정이다. 

복제본 수를 결정할땐 아래 사항과 장애 허용 대수와의 관계를 고려해야 한다. 

* min.insync.replicas
* 토픽을 만들때 Live Broker의 대수

## min.insync.replicas의 설정

min.insync.replicas는 프로듀서가 메시지를 보낼때 송신처 파티션의 복제본 중 ISR에 속하는 복제본이 최소 몇개나 필요한지를 설정하는 브로커와 토픽의 구성이다. 

브로커는 메시지를 수신한 직후 반드시 디스크에 Flush한다는 보장을 하지 않기 때문에, 복제본은 수신 직후에 특정 브로커에 장애가 발생했다고 해도 메시지를 분실하지 않기 위한 안정장치의 역할을 한다. 

장애 허용 대수 만큼의 브로커에 장애가 발생했을때 카프카 클러스터에 영향 없이 서비스를 계속하기 위해선 모든 파티션에서 ISR에 속하는 복제본의 수가 min.insync.replicas 수 이상이어야 한다. 즉 재배치 수는 아래와 같다.

```
(재배치 수) >= (min.insync_replicas) + 브로커 장애 허용대수
```

## 토픽을 작성할 때 Live Broker 대수

토픽 작성시에는 재배치 수를 지정한다. 그런데 이때 제대


	
	
> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTAxNTUzOTA4NCw3NzU2MDUxOTYsNTc2OT
M3MjI5LDIxMjIyNDMzNjIsOTQ1MDQ2ODcyLDY5MDYwNDI5NCwx
MDg2NTQ4MTc0LDEzMjc1MjUyMzMsLTIwNzkyMTUzMTAsMTg3MT
g5MDYzOSwtOTE1NjQzNzYwLDExMDk1NDY1NzcsLTIxMjg4MDQ3
NzcsMTAxNjQyMTE3MSwxOTM2MDIxMzI0LDE1NzA5MTE4MzYsMT
I2NDA2ODExNCw5OTExMDYyMCwtMjEwOTQzNjAzLDIwNTkzOTk4
MzNdfQ==
-->