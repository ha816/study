# 1장 아파치 카프카 개요

1장에서는 아파치 카프카로 무엇을 할 수 있는지 살펴보자. 더불어 아파치 카프카가 탄생한 배경과 현재 산업계에 어떤 영향을 주고 있는지도 알아보자.

## 아파치 카프카

아파치 카프카는 여러 대의 분산 서버에서 대량의 데이터를 처리하는 분산 메세징 시스템이다. 메세지를 받고, 받은 메세지를 다른 시스템이나 장치에 보내기 위해 사용한다. 카프카는 여러 시스템과 장치를 연결하는 중요한 역할을 한다. 

카프카는 높은 처리량(high-throughput)과 실시간(real-time)성이 특징이며 아래와 같은 4가지 특성을 가진다.

확장성 - 여러 서버로 확장(scale out)할 수 있기 때문에 데이터 양이 많아져도 대처할 수 있다. 

영속성 - 수신한 메시지 데이터를 디스크에 유지 하기 때문에 데이터의 영속성이 유지된다.

유연성 - 지원하는 제품들이 많기 때문에 데이터 허브의 역할을 충실하게 할 수 있다. 

신뢰성 - 메세지 전달을 보증하기 때문에 데이터 분실을 걱정하지 않아도 된다. 

카프카는 원래 높은 처리량으로 데이터를 실시간 처리하는 처리 성능에 초점을 두었지만, 기능과 신뢰성을 향상 시켜 현재는 **종합 스트림 처리**를 위한 플랫폼이 되었다.  

카프카는 오픈소스로 공개되어 있으며 여러 기업의 엔지니어들이 참여하고 있는 커뮤니티에서 개발되고 있다. 오픈소스 제품에 흔히 존재하는 기업용 엔터프라이즈 버전이 없고 핵심인 커뮤니티 버전만 존재한다는 것도 매력적이다. 

## 탄생 배경

카프카는 2011년 미국 링크드인에서 출발했다. 카프카는 링크드인 웹사이트에서 생성되는 로그를 처리하여 웹사이트 활동을 추적하는 것을 목적으로 개발되었다. 웹사이트에서의 활동은 사용자가 페이지 뷰와 검색 시 키워드 광고의 이용상황도 포함된다. 웹에서 생성되는 대량의 로그를 분석하여 사용자가 웹에서 하는 활동을 모니터링하고 서비스 개선에 활용하는 것이다.

빅데이터를 어떻게 활용한 것인지 큰 화제였던 당시 많은 웹 기업에서는 웹사이트에서 생성되는 로그를 활용하기 시작했다. 

링크드인의 카프카가 실현하려는 목표는 위에서 언급했던 특징과 같으며 순서대로 자세히 알아보자. 

### 높은 처리량으로 실시간 처리

데이터가 많아지면 그것에 대응하여 처리량이 우수해야 한다. 또한 사용자의 활동을 신속하게 파악하거나 사용자의 활동에 따라 즉시 피드백하기 위해서는 사용자 활동 단위로 실시간 처리가 가능해야 한다. 여기서 말하는 실시간 처리는 수집부터 시작해 수백 밀리초안에 데이터가 처리되는 방식을 가정한다.

### 임의의 타이밍에 데이터를 읽는다. 

실시간 처리에 대한 요구가 있는 반면, 링크드인은 기존 시스템에서 수집한 엑세스 로그를 일정 시간마다 배치로 처리하고 싶다는 요구도 있었따. 데이터를 사용하는 타이밍이 반드시 실시간이 아니라 이용목적에 따라 다를 수 있기 때문에 방대한 데이터를 전달할때 버퍼 역할도 가능하기를 원했다. 

### 다양한 제품과 쉽게 연동

링크드인에서는 데이터의 발생원과 관련된 시스템이 하나가 아니어서 여러 시스템을 통해 데이터를 받아들어야 했다. 또한 이용 목적에 따라 데이터 베이스, 데이터 웨어하우스, 하둡등 여러 시스템에서 데이터를 읽어와야 했다. 

### 메세지를 잃지 않음

취급하는 메세지가 방대하더라도 메세지를 잃어서는 안됐다. 다만, 링크드인의 초기 목적은 웹에서 사용자 활동을 추적하는 것이였기 때문에 한건 한건의 활동을 엄격하게 관리하기 보다는 약간의 중복이 있더라도 잃지 않는것이 중요했다. 

건마다 엄격하게 관리하면 처리 오버헤드가 커지는 것은 알고 있어, 높은 처리량으로 실시간 처리라는 요건과 균형을 가미하여 현실적으로 버릴 부분을 찾아야 했다. 

## 카프카 이전 제품

당시에 카프카와 비슷한 제품들의 상황은 어땟을까? 요구를 부분적으로 충족하는 제품이 있었을지도 모르겠지만, 모든 것을 제공하는 제품은 없었다. 몇가지 제품을 알아보고 링크드인의 요구를 충족하지 못한 부분은 무엇인지 알아보자. 

### 메세지 큐

한건의 레코드 단위로 실시간 처리를 할때 가장 먼저 떠오르는 것은 메시지 큐다. 메세지 큐 제품으로는 IBM의 WebSphere MQ와 JMS 사양을 따르는 아파치 ActiveMQ, 그외 RabbitMQ가 있다. 

메세지 큐에서 제공하는 기능은 차이가 있긴하지만 대략 아래와 같다. 모두 링크드인의 요구와 맞지 않았다. 

#### 전달 보증(message transaction)

IBM WebSphere MQ는 메세지 단위로 트랜잭션을 지원하는 기능이 있다. 하나의 메세지가 정확히 한번만 전송되는 것을 보장할 수 있다. JMS에서도 사양으로 규정되어 있으며, 애플리케이션에서 commit, rollback을 기술하여 트랜잭션을 사용할 수 있다. 

그러나 링크드인에서 다루는 로그의 성질을 생각하면 엄격한 트랜잭션 관리는 다소 오버 스펙이며, 높은 처리량이 우선순위가 높았다. 

####  스케일 아웃(scale out)

대량의 메세지를 한 서버로 처리하는것은 한계가 있다. 그러므로 처음부터 서버가 늘어날 것을 가정하고 아키텍처를 설계해야 한다. 물론 메세지 큐 제품도 클러스터 구성을 생각 했지만 실제로는 처리 성능을 높이는 목적으로 필요시 노드를 추가할 수 있는 스케일 아웃 기능을 전제로 하는 제품이 없었다 

#### 메세지가 대량으로 쌓이는 것을 예상 못했다. 

카프카 등장 이전의 메세지 큐에서는 메시지를 쌓아 둘 수 잇었는데, 큐에 쌓인 메세지가 즉시 사용되는 것으로 예상했지 장시간에 걸쳐 대량으로 축적되는 것은 예상하지 않았다. 링크드 인에서는 실시간 처리 뿐만 아니라 메세지를 배치로 처리하는 것도 가정했기 때문에 일정량의 데이터를 묵음으로 받아 데이터 웨어하우스에서 처리하기 위해서는 축적 시간이 길어져야 했기 때문에 기존 메세지 큐로는 감당 할 수 없었다. 

### 로그 수집 시스템

이어서 데이터를 수집한다는 관점에서 생각 할 수 있는 다른 제품은 로그 수집을 위한 메들웨어다. 주로 웹 서버 드으이 프론트 엔드 서버의 로그를 수집하기 위한 것이다. 당시 이 카테고리에 해당하는 제품은 페이스북이 개발한 Scribe와 미국 클라우데라가 개발한 Flume이다. 

각 프론트엔드 서버가 로그를 중계용 서버에 전송하고 거기서 로그를 수집하여 데이터 베이스와 분산 파일 시스템에 축적한다. 원래 대량의 로그를 처리하는 것을 가정했기 때문에 분산환경의 다중 서버 구성으로 이루어져 있었다. 그러나 이러한 제품은 아래와 같은 문제가 있었다. 

#### HDFS 데이터 축적과 배치 처리만 고려했다. 

이들 제품은 대량의 로그를 HDFS에 축적하고 맵리듀스로 일괄 처리하는 것이 주목적이다. 링크드인에서도 하둡을 사용하고 있지만, 동시에 데이터 웨어하우스를 이용한 데이터 분석도 많이 하고 있어 모두 하둡에서 동작하도록 구현하는 것은 비현실적이다. 

데이터 배치처리와 HDFS은 기본적으로 실시간처리에는 처리량이 낮아 처리하기 힘든 구조이다. 

#### 알기 쉬운 API의 부재

카프카 이전의 제품은 미들웨어 내에서의 구현 사양을 모르면 사용하기 힘들다는 지적이 있었다. 데이터 송신 수신을 다양한 제품과 연계하여 사용하기 위해선 쉬운 API가 필요했다. 

#### 수신하는 쪽이 임의로 메세지를 수신하기 어렵다.

로그 수집 기반 서버에서 그 다음 수신 시스템으로 메세지 전달시, 기존 제품에서는 push 명령으로 수신자에게 메세지가 전달되는 구조였다. 

수신자가 처리할 수 있는 처리량을 고려하지 않고 전달을 하는 구조였다. 필요에 따라 수신자의 상태에 따라 메세지를 수신해야 할때도 있었는데, 로그 수집 서버가 그 뒤에 있는 수신자의 상태를 일일이 모니터링하면서 push를 수행하는 것 보다 수신 시스템이 주기적으로 pull 방식으로 데이터를 가져가는것이 더 사용하기 쉽다고 생각했다. 

### ETL 도구

마지막 제푸으로는 ETL도구가 있다. 데이터 발생원에서 데이터를 추출하고 필요에 따라 변환해 데이터베이스와 웨어하우스에 로드하는 기능을 갖추고 있는 기능을 한다. 

대표적인 ETL 도구로써는 DataStage, Interstage, Cosminexus, Informatica PowerCenter, Talend 같은 제품이 있다. ETL 도구는 아래와 같은 요구사항을 지키지 못했다. 

#### 데이터를 파일 단위로 다룬다.  

카프카 등장 이전에 대량의 데이터를 높은 처리량으로 전달하려면 데이터를 파일 단위로 묶어 배치처리로 전송하는 것이 일반적이었다. 이를 위해 ETL 도구를 사용했다. 
그러나 링크드인의 한건의 레코드 단위로 실시간 처리를 하고 싶다는 요구사항을 ETL 도구는 만족할 수 없었다. 

#### 수신하는 쪽이 임의로 메세지를 수신하기 어렵다.

로그 수집과 동일한 논의가 ETL 도구에서도 성립한다. ELT 도구는 데이터를 추출, 변환하여 다른 데이터 저장소에 전달하는 것을 주축으로 하고 있지 임의의 타이밍에 데이터를 읽을 수 있는 중계역할을 하지는 않는다. 

## 카프카로 링크드인 요구사항 실현

앞서 기존 제품들로 링크드인의 요구사항을 실현하지 못하는 단점이 있었다. 그리하여 요구사항을 만족하는 카프카를 개발하게 되었다. 

카프카의 실현 수단은 아래와 같다. 

* 메세징 모델과 스케일 아웃형 아키텍처
* 디스크로의 데이터 영속화
* 이해하기 쉬운 API 제공
* 전달 보증

핵심은 메세징 모델과 스케일 아웃형 아키텍처이며, 아울러 디스크로의 데이터 영속화도 중요사항이다.

### 메세징 모델과 스케일 아웃

일반적으로 메세징 모델은 아래 세 가지 요소로 구성된다. 

Producer - 메세지 생산자
Broker - 메시지 수집/전달자
Consumer - 메세지 소비자

카프카의 메세징 모델을 설명하는데 있어 기존의 메세징 모델인 큐잉 모델과 Publish/Subscribe(Pub/Sub; 펍/섭) 모델과 비교하면서 알아보자. 카프카는 큐잉 모델과 펍섭 모델의 특징을 모두 겸비하는 형태이다. 


#### 큐잉 모델

브로커안에 큐를 준비해, 프로듀서에서의 메세지가 큐에 담기고, 컨슈머가 큐에서 메세지를 추출한다. 하나의 큐에 대해 여러 컨슈머가 존재하는 구조다. 

이 모델은 컨슈머를 여러개 준비하여 컨슈머에 의한 처리를 확장시킬 수 있으며, 컨슈머가 메시지를 받으면 다른 컨슈머는 메세지를 받을 수 없다. 

즉 다수의 컨슈머로 메세지 처리를 병렬적으로 할 수 있다. 큐에서 추출된 메세지는 컨슈머에 도착하면 사라지기 때문에, 하나의 메세지는 여러 컨슈머 중 딱 하나의 컨슈머에서 처리가 된다. 

#### 펍/섭 



> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIwMDk1ODQ1OTgsMTAzNzMzNDcwOCw0Nz
AzNDAyMjcsLTk0NTg4OTgwMywxNzk5MDIyMDY1LC0xNzQwMzQ0
NjE3LC04MDM4Mjk4MzEsLTEwNDI4MTM5MzYsLTIwNjE4Nzg0MD
ksLTE2ODI2OTQzMjMsMTg4OTc0ODUyNyw0MzIxODQ0NDldfQ==

-->