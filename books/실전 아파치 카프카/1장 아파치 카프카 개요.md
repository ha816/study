# 1장 아파치 카프카 개요

1장에서는 아파치 카프카로 무엇을 할 수 있는지 살펴보자. 더불어 아파치 카프카가 탄생한 배경과 현재 산업계에 어떤 영향을 주고 있는지도 알아보자.

## 아파치 카프카

아파치 카프카는 여러 대의 분산 서버에서 대량의 데이터를 처리하는 분산 메세징 시스템이다. 메세지를 받고, 받은 메세지를 다른 시스템이나 장치에 보내기 위해 사용한다. 카프카는 여러 시스템과 장치를 연결하는 중요한 역할을 한다. 

카프카는 높은 처리량(high-throughput)과 실시간(real-time)성이 특징이며 아래와 같은 4가지 특성을 가진다.

확장성 - 여러 서버로 확장(scale out)할 수 있기 때문에 데이터 양이 많아져도 대처할 수 있다. 

영속성 - 수신한 메시지 데이터를 디스크에 유지 하기 때문에 데이터의 영속성이 유지된다.

유연성 - 지원하는 제품들이 많기 때문에 데이터 허브의 역할을 충실하게 할 수 있다. 

신뢰성 - 메세지 전달을 보증하기 때문에 데이터 분실을 걱정하지 않아도 된다. 

카프카는 원래 높은 처리량으로 데이터를 실시간 처리하는 처리 성능에 초점을 두었지만, 기능과 신뢰성을 향상 시켜 현재는 **종합 스트림 처리**를 위한 플랫폼이 되었다.  

카프카는 오픈소스로 공개되어 있으며 여러 기업의 엔지니어들이 참여하고 있는 커뮤니티에서 개발되고 있다. 오픈소스 제품에 흔히 존재하는 기업용 엔터프라이즈 버전이 없고 핵심인 커뮤니티 버전만 존재한다는 것도 매력적이다. 

## 탄생 배경

카프카는 2011년 미국 링크드인에서 출발했다. 카프카는 링크드인 웹사이트에서 생성되는 로그를 처리하여 웹사이트 활동을 추적하는 것을 목적으로 개발되었다. 웹사이트에서의 활동은 사용자가 페이지 뷰와 검색 시 키워드 광고의 이용상황도 포함된다. 웹에서 생성되는 대량의 로그를 분석하여 사용자가 웹에서 하는 활동을 모니터링하고 서비스 개선에 활용하는 것이다.

빅데이터를 어떻게 활용한 것인지 큰 화제였던 당시 많은 웹 기업에서는 웹사이트에서 생성되는 로그를 활용하기 시작했다. 

링크드인의 카프카가 실현하려는 목표는 위에서 언급했던 특징과 같으며 순서대로 자세히 알아보자. 

### 높은 처리량으로 실시간 처리

데이터가 많아지면 그것에 대응하여 처리량이 우수해야 한다. 또한 사용자의 활동을 신속하게 파악하거나 사용자의 활동에 따라 즉시 피드백하기 위해서는 사용자 활동 단위로 실시간 처리가 가능해야 한다. 여기서 말하는 실시간 처리는 수집부터 시작해 수백 밀리초안에 데이터가 처리되는 방식을 가정한다.

### 임의의 타이밍에 데이터를 읽는다. 

실시간 처리에 대한 요구가 있는 반면, 링크드인은 기존 시스템에서 수집한 엑세스 로그를 일정 시간마다 배치로 처리하고 싶다는 요구도 있었따. 데이터를 사용하는 타이밍이 반드시 실시간이 아니라 이용목적에 따라 다를 수 있기 때문에 방대한 데이터를 전달할때 버퍼 역할도 가능하기를 원했다. 

### 다양한 제품과 쉽게 연동

링크드인에서는 데이터의 발생원과 관련된 시스템이 하나가 아니어서 여러 시스템을 통해 데이터를 받아들어야 했다. 또한 이용 목적에 따라 데이터 베이스, 데이터 웨어하우스, 하둡등 여러 시스템에서 데이터를 읽어와야 했다. 

### 메세지를 잃지 않음

취급하는 메세지가 방대하더라도 메세지를 잃어서는 안됐다. 다만, 링크드인의 초기 목적은 웹에서 사용자 활동을 추적하는 것이였기 때문에 한건 한건의 활동을 엄격하게 관리하기 보다는 약간의 중복이 있더라도 잃지 않는것이 중요했다. 

건마다 엄격하게 관리하면 처리 오버헤드가 커지는 것은 알고 있어, 높은 처리량으로 실시간 처리라는 요건과 균형을 가미하여 현실적으로 버릴 부분을 찾아야 했다. 

## 카프카 이전 제품

당시에 카프카와 비슷한 제품들의 상황은 어땟을까? 요구를 부분적으로 충족하는 제품이 있었을지도 모르겠지만, 모든 것을 제공하는 제품은 없었다. 몇가지 제품을 알아보고 링크드인의 요구를 충족하지 못한 부분은 무엇인지 알아보자. 

### 메세지 큐

한건의 레코드 단위로 실시간 처리를 할때 가장 먼저 떠오르는 것은 메시지 큐다. 메세지 큐 제품으로는 IBM의 WebSphere MQ와 JMS 사양을 따르는 아파치 ActiveMQ, 그외 RabbitMQ가 있다. 

메세지 큐에서 제공하는 기능은 차이가 있긴하지만 대략 아래와 같다. 모두 링크드인의 요구와 맞지 않았다. 

#### 전달 보증(message transaction)

IBM WebSphere MQ는 메세지 단위로 트랜잭션을 지원하는 기능이 있다. 하나의 메세지가 정확히 한번만 전송되는 것을 보장할 수 있다. JMS에서도 사양으로 규정되어 있으며, 애플리케이션에서 commit, rollback을 기술하여 트랜잭션을 사용할 수 있다. 

그러나 링크드인에서 다루는 로그의 성질을 생각하면 엄격한 트랜잭션 관리는 다소 오버 스펙이며, 높은 처리량이 우선순위가 높았다. 

####  스케일 아웃(scale out)

대량의 메세지를 한 서버로 처리하는것은 한계가 있다. 그러므로 처음부터 서버가 늘어날 것을 가정하고 아키텍처를 설계해야 한다. 물론 메세지 큐 제품도 클러스터 구성을 생각 했지만 실제로는 처리 성능을 높이는 목적으로 필요시 노드를 추가할 수 있는 스케일 아웃 기능을 전제로 하는 제품이 없었다 

#### 메세지가 대량으로 쌓이는 것을 예상 못했다. 

카프카 등장 이전의 메세지 큐에서는 메시지를 쌓아 둘 수 잇었는데, 큐에 쌓인 메세지가 즉시 사용되는 것으로 예상했지 장시간에 걸쳐 대량으로 축적되는 것은 예상하지 않았다. 링크드 인에서는 실시간 처리 뿐만 아니라 메세지를 배치로 처리하는 것도 가정했기 때문에 일정량의 데이터를 묵음으로 받아 데이터 웨어하우스에서 처리하기 위해서는 축적 시간이 길어져야 했기 때문에 기존 메세지 큐로는 감당 할 수 없었다. 

### 로그 수집 시스템

이어서 데이터를 수집한다는 관점에서 생각 할 수 있는 다른 제품은 로그 수집을 위한 메들웨어다. 주로 웹 서버 드으이 프론트 엔드 서버의 로그를 수집하기 위한 것이다. 당시 이 카테고리에 해당하는 제품은 페이스북이 개발한 Scribe와 미국 클라우데라가 개발한 Flume이다. 

각 프론트엔드 서버가 로그를 중계용 서버에 전송하고 거기서 로그를 수집하여 데이터 베이스와 분산 파일 시스템에 축적한다. 원래 대량의 로그를 처리하는 것을 가정했기 때문에 분산환경의 다중 서버 구성으로 이루어져 있었다. 그러나 이러한 제품은 아래와 같은 문제가 있었다. 

#### HDFS 데이터 축적과 배치 처리만 고려했다. 

이들 제품은 대량의 로그를 HDFS에 축적하고 맵리듀스로 일괄 처리하는 것이 주목적이다. 링크드인에서도 하둡을 사용하고 있지만, 동시에 데이터 웨어하우스를 이용한 데이터 분석도 많이 하고 있어 모두 하둡에서 동작하도록 구현하는 것은 비현실적이다. 

데이터 배치처리와 HDFS은 기본적으로 실시간처리에는 처리량이 낮아 처리하기 힘든 구조이다. 

#### 알기 쉬운 API의 부재

카프카 이전의 제품은 미들웨어 내에서의 구현 사양을 모르면 사용하기 힘들다는 지적이 있었다. 데이터 송신 수신을 다양한 제품과 연계하여 사용하기 위해선 쉬운 API가 필요했다. 

#### 수신하는 쪽이 임의로 메세지를 수신하기 어렵다.

로그 수집 기반 서버에서 그 다음 수신 시스템으로 메세지 전달시, 기존 제품에서는 push 명령으로 수신자에게 메세지가 전달되는 구조였다. 

수신자가 처리할 수 있는 처리량을 고려하지 않고 전달을 하는 구조였다. 필요에 따라 수신자의 상태에 따라 메세지를 수신해야 할때도 있었는데, 로그 수집 서버가 그 뒤에 있는 수신자의 상태를 일일이 모니터링하면서 push를 수행하는 것 보다 수신 시스템이 주기적으로 pull 방식으로 데이터를 가져가는것이 더 사용하기 쉽다고 생각했다. 

### ETL 도구

마지막 제푸으로는 ETL도구가 있다. 데이터 발생원에서 데이터를 추출하고 필요에 따라 변환해 데이터베이스와 웨어하우스에 로드하는 기능을 갖추고 있는 기능을 한다. 

대표적인 ETL 도구로써는 DataStage, Interstage, Cosminexus, Informatica PowerCenter, Talend 같은 제품이 있다. ETL 도구는 아래와 같은 요구사항을 지키지 못했다. 

#### 데이터를 파일 단위로 다룬다.  

카프카 등장 이전에 대량의 데이터를 높은 처리량으로 전달하려면 데이터를 파일 단위로 묶어 배치처리로 전송하는 것이 일반적이었다. 이를 위해 ETL 도구를 사용했다. 
그러나 링크드인의 한건의 레코드 단위로 실시간 처리를 하고 싶다는 요구사항을 ETL 도구는 만족할 수 없었다. 

#### 수신하는 쪽이 임의로 메세지를 수신하기 어렵다.

로그 수집과 동일한 논의가 ETL 도구에서도 성립한다. ELT 도구는 데이터를 추출, 변환하여 다른 데이터 저장소에 전달하는 것을 주축으로 하고 있지 임의의 타이밍에 데이터를 읽을 수 있는 중계역할을 하지는 않는다. 

## 카프카로 링크드인 요구사항 실현

앞서 기존 제품들로 링크드인의 요구사항을 실현하지 못하는 단점이 있었다. 그리하여 요구사항을 만족하는 카프카를 개발하게 되었다. 

카프카의 실현 수단은 아래와 같다. 

* 메세징 모델과 스케일 아웃형 아키텍처
* 디스크로의 데이터 영속화
* 이해하기 쉬운 API 제공
* 전달 보증

핵심은 메세징 모델과 스케일 아웃형 아키텍처이며, 아울러 디스크로의 데이터 영속화도 중요사항이다.

### 메세징 모델과 스케일 아웃

일반적으로 메세징 모델은 아래 세 가지 요소로 구성된다. 

Producer - 메세지 생산자
Broker - 메시지 수집/전달자
Consumer - 메세지 소비자

카프카의 메세징 모델을 설명하는데 있어 기존의 메세징 모델인 큐잉 모델과 Publish/Subscribe(Pub/Sub; 펍/섭) 모델과 비교하면서 알아보자. 카프카는 큐잉 모델과 펍섭 모델의 특징을 모두 겸비하는 형태이다. 

#### 큐잉 모델

브로커안에 큐를 준비해, 프로듀서에서의 메세지가 큐에 담기고, 컨슈머가 큐에서 메세지를 추출한다. 하나의 큐에 대해 여러 컨슈머가 존재하는 구조다. 

이 모델은 컨슈머를 여러개 준비하여 컨슈머에 의한 처리를 확장시킬 수 있으며, 컨슈머가 메시지를 받으면 다른 컨슈머는 메세지를 받을 수 없다. 

즉 다수의 컨슈머로 메세지 처리를 병렬적으로 할 수 있다. 큐에서 추출된 메세지는 컨슈머에 도착하면 사라지기 때문에, 하나의 메세지는 여러 컨슈머 중 딱 하나의 컨슈머에서 처리가 된다. 

#### 펍/섭 메시징 모델

펍섬 모델에서 프로듀서는 퍼블리셔, 컨슈머는 서브스크라이버라고 한다. 

퍼블리셔가 서브스크라이버에게 메세지를 직접 보내는 것이 아니라 브로커를 통해 전달한다. 

퍼블리셔는 어떤 서브스크라이버가 메세지를 수신하는지 알수 없고 브로커에 있는 토픽이라고 불리는 카테고리 안에 메세지를 등록한다. 
퍼블리셔는 브로코에 메세지를 보내기만 할뿐, 누가 메세지를 처리하는지는 신경쓰지 않는다. 퍼블리셔가 보낸 메세지는 브로커내의 토픽이라 불리는 부분에 등록된다. (브로커에는 다수의 토픽이 존재할 수 있다.)

한편 서브스크라이버는 여러개 존재하는 토픽 중에서 하나를 선택하여 메세지를 받는다. 여러 서브스크라이버가 동일한 토픽을 구독하기로 했다면, 구독한 **모든 서브스크라이버는 동일한 메세지를 받는다.**  다른 토픽을 구독한 서브스크라이버는 다른 메세지를 받는다. 

큐일 모델과는 달리 같은 토픽을 구독하는 여러 서브스크라이버는 동일한 메세지가 전달된다. 이점을 유의하자. 

#### 프로듀서와 컨슈머 사이에 브로커가 왜 있지?

큐일 모델과 펍섭 모델 모두 브로커를 가운데 끼우는 모델이다. 이런 구조를 가져가는 이유는 변경에 강한 시스템 아키텍처를 만들수 있기 때문이다. 보다 자세히는 아래와 같다.

* 프로듀서/컨슈머 모두 접속처(컨택트 포인트)가 하나다. 
	* 프로듀서, 컨슈머 모두 브로커에게만 접근하면 된다. 브로커가 없다면 프로슈서, 컨슈머 모두 접근을 위한 관리가 들어가야 한다.
* 프로듀서/컨슈머의 증가/감소에 대응이 가능하다. (네트워크 토폴리지 변경에 유연하다.)
	* 프로듀서/컨슈머 모두 서로의 존재를 몰라도 되기에 각 엔티티를 증감 하는데 문제가 없다.

#### 카프카 메세징 모델

카프카에서는 큐잉 모델에서 구현한 여러 컨슈머가 분산처리로 메세지를 소비하는 특징과 펍섭 모델에서는 같은 토픽을 구독한 여러 서브스크라이버에게 동일한 메세지를 전달하고, 토픽 기반으로 전달 내용을 변경하는 특징을 모두 가진다.  특이하게 컨슈머 그룹이라는 개념을 도입하여 컨슈머를 확장 구성할 수 있도록 설계 되어있다. 


시스템 구성을 보면 브로커의 처리량이 늘어나면 병목이 될거 같다는 느낌이 올것이다. 또한 장기간에 걸쳐 임의의 타이밍에 데이터를 제공하려면 더더욱 위험하다. 따라서 브로커는 복수 구성으로 동작하도록 되어있으며, 결과적으로는 전체적으로 확장이 가능한 구성을 보인다. 

### 디스크로의 데이터 영속화

데이터를 브로커에 적재해 두었다가 필요한 타이밍에 데이터를 가져가고 메세지를 잃지 않는 요구사항을 위해서 카프카는 브로커에 메시지를 디스크에 영속화하고 있다. 

메시지 큐에도 데이터를 영속화하는 제품이 있지만 실시간 접속에만 중점을 두고 있고 **기본적으로 장기간 보존**을 가정하지 않는다.

배치 처리의 경우에는 데이터를 일정기간동안 모아야 하기 때문에 데이터를 메모리에 유지하는 것은 어렵다. 따라서 카프카의 메시지 영속화는 디스크에서 이루어진다. 보통 디스크 영속화를 하면 처리량이 줄어들기 마련인데 디스크 영속화를 제공함에도 불구하고 **카프카는 높은 처리량을 제공한다.** 

또한 들어오는 데이터를 장기 보존 목적으로 영속화를 할 수 있기 때문에 카프카를 스토리지 시스템으로도 생각할 수 있다. 이 특징을 활용하는 예로는 처리 순서대로 로그를 계속 남기는 커밋 로그를 축적하기 위한 스토리지 시스템등을 들을 수 있다. 

영속화의 목적 - 일반적으로 데이터 영속화란 데이터를 잃지 않는, 즉 데이터 자체에 대한 내장애성(fault-tolerance) 향상을 위한 수단으로 여기는 경우가 많다. 하지만 카프카에서는 브로커의 메모리에 실리면 메모리에서 디스크로의 flush는 OS에 맡기기 때문에, 카프카에게 있어 데이터 영속화는 반드시 내장애성만을 의식한것만은 아니다. 

카프카는 단일 브로커의 고장이 발생하더라도 즉시 데이터 손실로 이어지지 않도록 복제 구조를 갖추고 있다. 따라서 브로커내에서의 최근 데이터 손실 방지는 메세지의 복제로 대응한다고 볼 수 있다. 적어도 몇개의 복제본이 브로커 내에 존재하고 있다면 그 메세지를 컨슈머에서 사용가능하다. 

### 이해하기 쉬운 API 제공

카프카는 다양한 제품과 시스템에 연동을 쉽게 하기 위한 요구사항을 만족하기 위해 Connect API를 제공한다. 이 API를 이용하여 각종 외부 시스템과 접속한다. 또한 API를 기반으로 카프카에 접속하기 위한 프레임 워크로 Kafka Connect도 제공한다. 

카프카에 존재하는 데이터에 대해 스트림 처리를 하는 Stream API를 라이브러리화한 Kafka Streams라는 클라이언트 라이브러리가 있다. 사용자는 Kafka Streams라는 라이브러리를 이용해 자바 애플리케이션을 만들고, 쉽게 작동시킬 수 있기 때문에 카프카 입출력에 사용하는 스트림 처리 애플리케이션을 비교적 쉽게 구현할 수 잇다. 

 ### 전달 보증(Transaction)

메세지 생산자 입장에서 당연한 요구인 메세지를 잃지 않고 전달하기를 보장하기 위해서 카프카는 전달 보증 기능을 제공한다. 카프카에서는 세 가지 수준으로 나누어 전달 수준을 보장한다. 

`At Most Once, At Least Once, Exactly Once`

| 종류 | 개요 | 재전송 유무| 중복삭제유무|비고|
|--|--|--|--|--|
| At Most Once | 많아도 1회는 전달을 시도해본다.  | X|X| 메세지는 중복되지 않지만 상실될수도 있다.|
| At Least Once | 적어도 1회는 전달을 시도해본다.  | O|X| 메세지가 중복될 가능성은 있지만, 상실되지 않는다.|
| Exactly Once | 1회만 전달해본다.  | O|O| 중복되거나 상실되지도 않고 확실하게 메시지가 전달되지만 성능상 이슈가 있다.|

메세지 큐에서는 Exactly Once 수준을 주목적으로 하는 경우가 많다. 따라서 트랜잭션 관리를 위한 메커니즘이 마려돼 있다. 그러나 카프카 개발 초기에는 성능을 중시하는, 즉 '높은 처리량'을 구현해야 했기 때문에 Exactly Once 수준의 보장은 미루고 최소한 메세지 상실을 피하기 위한 At Least Once 수준으로 전달 수준을 보장했다. 

At Least Once를 실현하기 위해 Ack와 오프셋 커밋이라는 개념을 도입했다. Ack는 브로커가 메세지를 수신했을때 프로듀서에게 수신완료 했다는 응답을 말한다. 프로듀서 입장에서는 Ack를 받지 못하면 재전송을 해야하는 것으로 판단한다. 

또한 컨슈머가 브로커로부터 메세지를 받을때는 컨슈머가 어디까지 메세지를 받았는지 관리를 하기 위한 오프셋이 있다. 이를 이용한 전달 범위 보증 구조를 오프셋 커밋이라고 한다. 오프셋 커밋은 메세지를 받아 정상적으로 처리를 완료한 다음 오프셋을 업데이트하여 어딘가 문제로 메세지를 브로커에서 다시 보낼때도 어디부터 재전송하면 되는지 판단할 수 있다. 

> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEzNjQ3NjA2NTAsLTExNTYxNzUyNjIsMT
A3NzU4NTM0NCwxODM0NzAxNjQ2LC0yMDQyNjQ2NDg2LDM1MTI0
NzUyOCwxMDM3MzM0NzA4LDQ3MDM0MDIyNywtOTQ1ODg5ODAzLD
E3OTkwMjIwNjUsLTE3NDAzNDQ2MTcsLTgwMzgyOTgzMSwtMTA0
MjgxMzkzNiwtMjA2MTg3ODQwOSwtMTY4MjY5NDMyMywxODg5Nz
Q4NTI3LDQzMjE4NDQ0OV19
-->