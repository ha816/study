# Overview 

이 장에서는 외부 스트림 처리의 예로 아파치 스파크의 검포넌트 중 하나인 Structured Streaming을 사용해 카프카와 조합한 스트림 처리 애플리케이션을 조립하는 예를 알아보자. 

간단히 Structured Streaming의 개요에 대해 설명하고, 작성할 예제 애플리케이션의 동작 환경에 대해 설명한다. 

스파크와 Strucutred Streaming은 스칼라, 파이썬 자바 등으로 애플리케이션 개발이 가능한데, 애플리케이션 개발에서는 가장 많이 사용하는 언어중 하나인 스칼라를 사용한다. 

# 아파치 스파크

아파치 스파크는 OSS(오픈소스 소프트웨어)의 병렬 분산 처리 프레임워크다. 하둡의 맵리듀스 프레임워크와 마찬가지로 여려 범용 서버로 구성된 클러스터를 사용해 대규머 데이터의 배치 처리를 병렬로 실행할 수 있다. 

스파크는 맵리듀스 프레임 워크에 비해 더 효율적으로 데이터를 처리할 수 있도록 설계되어 있다. 이외에도 다양한 프로그래밍 언어로 애플리케이션 개발이 가능한 점이나, 머신러닝, 쿼리 처리, 그리고 이 장의 주제인 스트림 처리 등 특정 용도에 있어서 병렬 분산 처리를 쉽게 활용할 수 있는 컴포넌트가 포함되어 있다는 점도 장점이다. 

스파크로 개발한 애플리케이션은 클러스터 관리자에 의해 계산 리소스가 관리되는 클러스터에서 동작한다. 클러스터 관리자에는 하둡의 YARN과 아파치 Mesos, 스파크에 포함된 Standalone이 있으며, 쿠버네티스도 이를 지원하고 있다. 

## 스파크의 데이터 처리 모델

스파크는 처리 대상 데이터를 RDD(Resilient Distributed Dateset; 탄력이 있게 분산된 데이터 셋)라 불리는 내장애성을 지닌 분산 컬렉션으로 추상화 한다. 즉 처리 대상의 레코드 하나하나를 RDD로 취급한다. 

```
// 아래 각 레코드는 모든 하나의 추상화된 컬렉션 구조로 취급된다.
고양이, 개, 여우, 너구리
// 각 레코드는 RDD의 한 요소로 추상화된다.
```

RDD는 데이터 처리을 위한 추상화된 인터페이스로 이해할 수 있다. 레코드를 RDD로 추상화하여 개발자는 분산 처리를 의식하지 않고 컬렉션 처리 코드를 작성하여 애플리케이션 개발이 가능하다. 스파크는 처리 대상의 파일이나 RDBMS의 데이블 등 처리 대상의 데이터에 대한 RDD 인터페이스를 부여하는 수단과 기능을 제공한다. 

```
val rdd = sc.textFile("data.txt");
val wordCountRDD = rdd.flatMap(_.split("" ).map(word => (word, 1))).reducesByKey(_+_)

wordCountRdd.savaAsText("result.txt");
```

개별 RDD요소에 정의한 map과 부여한 조건에 일치하는 요소를 거르는 filter, 특정 키별로 요소를 그룹화해서 보여주는 reduceByKey 등 미리 정의된 함수를 적용하여 데이터 처리 로직을 작성 가능하다. 

이처럼 RDD에 대한 여러 처리를 위한 함수를 변환이라고 한다. 변환 하나하나는 단순한 데이터를 처리하는 경우가 많지만, 여러 변환을 체인으로 연결하여 복잡한 데이터 처리 로직을 작성할 수 있다. 

데이터 처리 결과는 액션이라는 종류의 함수를 사용하여 그 결과를 취급한다. 액션 함수 중 하나로 saveAsText는 처리 결과를 파일로 저장할 수 있다.

개발자가 작성한 데이터 처리 로직은 스케쥴러등과 함께 Driver Program에 모이게 된다. Driver Program은 Driver라고 불리는 프로세스에 실행된다. 스파크 애플리케이션의 경우 클러스터 내의 단일 슬레이브 서버에서 Driver를 실행하는 경우와 스파크 애플리케이션을 실행하는 클라이언트가 Driver를 겸업하는 경우가 있다. 

스케줄러는 개발자가 작성한 처리 로직을 클러스터 상에서 처리하기 위한 분산 처리를 제어한다. 스케줄러는 작성된 처리 로직을 병렬 처리 가능한 단위인 Task라는 단위로 쪼개고 실행 순서를 조절한다. 

태스크를 실행하는 슬레이브 서버에서 동작하는 Executor라 불리는 프로세스다. 스파크에서는 클러스터 내부의 여러 Executor가 병렬로 태스크를 실행함으로써 전체 클러스터에서 병렬 분산 처리를 하고 있다. 


## DateFrame / Dataset

RDD는 스파크를 이용하는데 가장 기본적인 데이터 구조이자 인터페이스다. 그러나 복잡한 처리에서는 최적화가 어렵고 처리 내용을 파악하기 어려워 개발 언어에 따라서 성능의 차이가 생길 수 있다. 

클러스터 안에서 병렬 처리시 Executors는 JVM에서 동작하지만, 데이터 처리 로직을 작성한 프로그래밍 언어에 따라서는 처리 부분이 Executor가 아닌 다른 프로세스에서 동작할 수 있다. 예를 들어, 파이썬으로 데이터 로직을 작성한 경우 슬레이브 서버에서 실행된 일부가 파이썬 프로그램으로 실행된다. 파이썬 인터프리터의 성능이나 파이썬에 동작하는 프로그램과 Executor와 통신이 스칼라나 자바로 데이터 로직을 작성한 경우에 비해 성능이 저하되는 경우가 있다. 

최근 이러한 문제를 해결한 구조로 DateFrame / Dataset이 있다. DateFrame/Dataset은 스파크 컴포넌트 중 하나인 Spark SQL의 기본적인 데이터 구조다. 처리 대상의 데이터를 관계형 데이터베이스의 테이블 형태로 추상화하고 칼럼명과 데이터 형등의 스키마를 부여할 수 있다. 이후 이를 DataSet이라고 나타내겠다. 

프로그래밍 언어의 측면에서는 DateSet이 RDD와 마찬가지로 데이터 처리를 위한 인터페이스로 볼 수 있다. 그러나 RDD와는 달리 개발자는 SQL처리 처럼 Dataset에 대한 쿼리를 작성하여 데이터 처리 애플리케이션을 개발 할 수 있다. 

선언전 API가 있기 때문에 복잡한 처리를 조립하여도 RDD를 기반으로 한 처리보다 확연히 가독성이 좋다. 또한 DataSet을 기반으로 한 처리는 Spark SQL의 옵티마이저에 의해 최적화가 되어 개발 언어에 상관 없이 JVM에서 실행가능한 코드를 생성한다. 이 때문에 수작업에 의한 최적화 수고를 줄일 수 있고, 개발 언어간의 차이에 따라 성능 차이도 나지 않게 된다. 

## Strucutred Streaming

Strucutred Streaming은 스파크를 구성하는 컴포넌트 중 하나로 스트림 처리를 여러 서버에서 병렬로 실행하는데 사용된다. 

Strucutred Streaming은 Spark SQL에서 동작하는 새로운 스트림 처리를 위한 컴포넌트다. Strucutred Streaming은 장애 발생 후 일관된 복구를 위해 설계되었고, 이벤트 타임 윈도 집계 처리 지원등 Spark Streaming에서 어려웠던 기능이 구현되어 있다. Strucutred Streaming은 Dataset을 대상으로 한 컴포넌트이다. 

Strucutred Streaming은 Dateset을 기반하기 때문에 SparkSQL로 배치 처리 애플리케이션을 개발할 때와 동일한 API를 사용해 처리할 수 있다. 따라서 배치 처리와 스트림 처리를 혼용할 경우도 통일된 방법으로 처리를 할 수 있다. 또한 Spark SQL 옵티마이저로 최적화 가능한 장점도 있다. 

### Strucutred Streaming 데이터 처리 모델

대다수 스트림 처리 컴포넌트는 스트림 데이터가 도착한 시점에 처리되는 Event-Driven 방식을 따른다. 반면 Strucutred Streaming은 트리거라 불리는 데이터 처리를 위한 시간을 정의하고 있다. 

트리커는 정기적인 간격으로 반복해서 데이터를 수신하고, 수백 밀리초에서 수초 정도의 짧은 배치를 반복 실행하여 스트림을 처리한다.  이것을 마이크로 배치라고 부른다.

또한 데이터 모델의 특징도 있는데, Strucutred Streaming에서는 데이터 스트림을 RDBMS의 테이블 처럼 취급한다. 그리고 데이터 스트림 안의 데이터를 테이블에 무한히 추가되는 레코드로 취급한다. 이런 가상의 테이블을 입력 테이블(input table)이라 한다.

트리거마다 실행되는 마이크로 배치는 점진적으로 레코드가 추가되는 입력 테이블에 쿼리를 실행해 데이터를 처리한다. 입력 테이블에 쿼리를 실행해 얻는 결과를 결과 테이블이라고 부른다. 

마이크로 배치 1회의 출력은 결과 테이블에 포함된 레코드의 일부거나 전부 일 수 있다. 이 출력 레코드는 출력 모드에 의해 제어 되는데 크게 세가지 종류가 있다. 

* Complete
	* 생성된 결과 테이블에 포함된 레코드를 모두 출력한다.
* Update
	* 이전 트리커에서 생성된 결과를 갱신 또는 추가된 레코드를 출력한다.
* Append
	* 직전 트리거에서 생성된 결과 테이블에 추가된 레코드를 출력한다.

어떤 출력 모드를 설정 할 수 있는지는 쿼리로 표현된 데이터 처리 종류에 따라 다르다. 다음은 각 출력 모드가 설정 가능한 처리의 예이다.

* Complete
	* 집약 처리의 결과 노출시 사용 가능
* Update
	* 컬럼의 선택이나 필터, 집약 처리에 사용 가능
* Append
	* 컬럼의 선택이나 필터, 조인, 워터마크가 설정된 집약 처리

프로그래밍 측면에서는 Dataset이 입력 테이블이나 결과 테이블, 그리고 그 중간 테이블의 인터페이스가 된다. 입력 테이블에 해당하는 DataSet에 대한 쿼리를 작성하면 중간 테이블에 해당하는 Dataset을 얻을 수 있다. 그리고 중간 테이블에 해당하는 Dataset도 마찬가지로 쿼리를 작성해 이를 반복하여 최종으로 얻은 Dataset은 결과 테이블에 해당한다. 또한 입력 테이블에 해당하는 Dataset에서 결과 테이블에 해당하는 Dataset을 얻을 때까지 일련의 쿼리는 통상 마이크로 배치 1회당 1번 실행된다. 

# Spark 애플리케이션 동작 환경

트위터에서 수집한 트윗 데이터를 처리하는 애플리케이션을 만들고 카프카와 Strucutred Streaming을 연계하는 법을 알아보자. 

1. Twitter API를 이용하여 트윗 데이터를 수집하는 Tweet Producer가 존재.
2. Tweet Producer의 JSON 형식의 트윗 데이터를 Kafka의 Tweet 토픽에 송신
3. Apache Spark로 만든 스트림 처리 애플리케이션(Structred Streaming)에서 Tweet 토필에 쌓인 트윗 데이터를 스트림 처리 하고 다시 Kafka processed-tweat 토픽에 송신한다.
4. Kafka Console Consumer에서 processed-tweet 을 경유하여 수신한 트윗 데이터의 처리 결과를 콘솔에 표시

## Spark 설정

스파크와 Strcutured Streaming을 사용하여 개발된 애플리케이션을 동작시키는 프로덕션 환경에는 YARN 등의 클러스터 관리자에서 관리되는 클러스터를 이용하는 것이 일반적이다. 

단 




> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjAwNDQ4NzA3MSwtMTEzMzkxNjU3MiwtMT
M5NTg5MDY3OCw4MDkwNDgxODYsLTIxMTI0MTE0NDUsLTE2MDA4
MTc0NDQsLTY1MzI4NjMzNSwyMzQxMTgzNTRdfQ==
-->