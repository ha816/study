# Overview

이번 장에서는 엘라스틱서치의 시스템적 측면을 공부해보자. 그리고 이를 통해 엘라스틱서치 클러스터가 안정적으로 동작하는데 필요한 시스템 구성을 함께 고민해보자. 

# 노드 실행환경과 JVM 옵션

엘라스틱서치와 루씬은 모두 자바 언어로 개발되었다. 하지만 독립적으로 실행 가능한 엘라스틱 서치와는 달리 루씬은 독립적으로 실행될 수 없는 라이브러리 형태로 제공된다. 루씬은 자바 애플리케이션에서 사용할 수 있도록 jar형태로 배포되고, 엘라스틱서치는 이러한 jar 라이브러리를 임포트하는 방식으로 활용한다. 엘라스틱서치가 실행되어 인스턴스가 만들어지면 엘라스틱서치와 루씬 둘 다 하나의 JVM에서 함께 동작한다. 이제 엘라스틱서치 인스턴스가 실행되는 실행환경에 대해 알아보자. 

## 엘라스틱서치 릴리즈 노트

2010년 즈음 오픈 소스로 개발되기 시작해서 2014년 1.0 버전이 공식 출시된 이후로 지금도 매우 빠르게 성장하고 있다. 

실무에서는 2.x ~ 5.x 버전이 아직 많이 사용되고 있다. 하지만 새롭게 엘라스틱 서치를 도입한다면 6.x 이상을 사용할 것을 권장한다. 최신 버전을 사용함으로써 자바와 루씬의 최신 기능을 모두 사용할 수 있기 때문이다. 

엘라스틱서치와 루씬은 매우 밀접한 관계가 있다. 엘라스틱서치는 루씬을 기반으로 동작하기 때문에 엘라스틱버전에 따라 내부에서 사용하는 루씬 버전도 함께 달라진다. 루씬 라이브러리의 기능이 추가되거나 버그가 수정되면 주기적으로 버전업하게 되는데, 엘라스틱서치에서도 새롭게 버전업된 루씬을 기반으로 새로운 버전의 엘라스틱서치를 릴리스해서 제공한다. 

엘라스틱서치와 루씬 모두 오픈소스이고 엘라스틱서치의 개발자가 동시에 루씬의 개발자이기도 하다. 그래서 루씬의 기능이 새로 추가 될때마다 엘라스틱서치도 그에 따라 새로운 기능으로 릴리스가 된다. 

## 실행시 자바 8 이상을 사용해야할 이유

최근 급견한 하드웨어 기술의 발달로 CPU나 메모리 자원이 매우 저렴해졌다. 한 서버에서 다수의 CPU나 코어를 탑재하고 메모리도 비교적 넉넉하다. 

### 다수의 CPU 사용

64비트 컴퓨터로 변경되면서 메모리 사용은 사실 프로그래밍 관점에서는 크게 변화가 없었다. 하지만 다수의 CPU를 사용하는 프로그래밍에서는 변화가 컸다. 기존 멀티쓰레드 기반 프로그래밍은 하나의 CPU를 효율적으로 사용하기 위한 방식이 대부분이었기 때문에 오히려 CPU가 많은 환경에서는 비효율적인 경우가 종종 발생했다.

다수의 CPU가 탑재된 경우 동시에 모든 CPU를 점유해서 동작하여 프로그래밍을 해야 했는데 이런 방식은 멀티 쓰레드 방식 방식과 비교했을때 상대적으로 매우 어려운 일이었다. 

자바는 8 버전 부터 큰 변화가 생겼다. 그 중 대표적으로는 함수형 프로그래밍의 도입이라고 할 수 있다. 함수형 프로그래밍 언어 지원을 위해 스트림과 람다 표현식이 지원되기 시작했다. 

스트림을 사용하면 언어차원에서 손쉽게 멀티 코어로 함수를 동작시킬수 있다. 이런 특수한 함수를 람다라고 부르는데 람다를 이용해 로직을 작성하고 스트림에 입력하면 다수의 CPU에서 동시에 데이터가 처리되고 모든 CPU에 처리가 끝날때 까지 결과를 기다리게 된다. 이후 모든 결과를 모아 돌려준다. 즉 일종의 맵 리듀스로 동작하게 되는것이다. 

이런 과정은 언어 차원에서는 블랙박스로 이루어진다. 멀티 코어에 대해 깊은 지식이 없더라도 개발자는 로직에만 집중하면 되는 것이다. 람다를 쓸수 있는 것만으로도 자바 8을 써야하는 이유는 분명하다. 

### 항상 최신 버전의 엘라스틱서치를 사용하기

일반적으로 버전업이 이뤄질 경우 최대한 하위 호환을 고려해서 개발이 이뤄지지만 하위 호완성을 100%유지한다는 것은 매우 어려운 일이다. 

엘라스틱서치는 자바, 루씬, 엘라스틱서치 자체의 기능을 모두 고려해서 하위 호완성을 만들어야 하는데 이는 굉장히 어렵다. 버전이 올라갈수록 기존에 제공하는 기능이 폐기 예정이거나 사용법이 바뀔수 있기 때문에 반드시 변경사항을 꼼꼼히 확인해야 한다.

>엘라스틱서치 메이저 업데이트와 마이너 업데이트
>실무에서는 이미 엘라스틱서치에 많은 데이터가 있을 것이므로 운영중에 엘라스틱서치를 버전업하는 것은 매우 리스크할 것이다. 이러한 이유로 대부분 릴리즈 업데이트가 있어도 버전업을 못한 것이다. 이러한 경우를 고려해서 엘라스틱서치는 최신 버전에서 메이저버전이 올라가더라도 과거 버전의 버그패치와 같은 간단한 마이너 패치는 일정 기간 제공하고 있다.

### 자바 8에서 제공하는 JVM 옵션

서버상에서 자바 애플리케이션을 실행하는 경우 JVM 위에서 동작하게 된다. JVM 기반의 애플리케이션은 개발자가 직접 메모리 관리를 하지 않아도 된다는 큰 장점이 있다. 메모리 관리 책임은 JVM이 맡고, GC가 가비지 컬렉션 메커니즘으로 일정주기로 사용하지 않는 메모리를 자동으로 회수한다. 

GC동작은 자동적으로 일어나지만 실행할때 JVM에 옵션으로 일부  GC동작을 제어할 수도 있다. 그럼 엘라스틱에 적용된 JVM옵션을 무엇인지 알아보도록 하자. 

**엘라스틱서치는 분산 시스템의 특성상 스케일 인/스케일 아웃(Scale In/Scale Out)이 빈번하게 발생할 수 있다. 또한 장애를 복구한다거나 ReIndex 작업에 의해 일어나는 데이터 리밸런싱에 의해 많은 메모리를 사용하기 때문에 전체적인 성능향상을 위해 다수의 JVM옵션을 반드시 튜닝해야 한다.** 

일반적인 경우는 엘라스틱서치에서 기본적으로 제공하는 옵션 그대로 실행하면 된다. 다시 한번 강조하지만 엘라스틱서치에서 기본적으로 설정한 JVM 옵션은 수정하지 않고 그대로 사용하길 권한다.

엘라스틱서치는 다수의 노드로 구성되며, 서로 유기적으로 동작하는 매우 복잡한 애플리케이션이다. **지금까지 버전업을 해오면서 많은 문제를 해결해왔기 때문에 기본 설정으로 제공하는 JVM옵션들이 각종 문제에 대한 경험을 바탕으로 최적화한 값이다.** 

각 환경에서 설정한 대부분의 JVM 옵션들은 서비스 운영 초기에는 매우 유용한 것으로 보일수도 있다. 하지만 데이터가 커지면 결국 불안정해질 가능성이 높다. 이런 경우 JVM 옵션을 다시 기본 설정으로 변경해보는 것도 좋은 해결책 중 하나다. 

하지만 세상에는 언제나 예외가 있다. jvm.option이라는 파일에는 기본적인 JVM 힙 크기가 1GB로 설정되어 있는데, **실제 운영환경에서는 이를 반드시 더 큰 크기로 만들어야 한다.** 힙 크기가 1GB인 이유는 엘라스틱서치를 실행할 수 있는 최소한의 힙의 크기가 1GB이기 때문이다. 이는 테스트 용도이므로 실제 운영환경에서는 힙크기는 반드시 이것보다 커야 한다. 서비스 운영을 위해 힙 크기를 변경하는 것외에는 기본 설정된 JVM 옵션을 그대로 사용하는것이 좋다. 

# 힙 크기를 32GB 이하로 유지하는 이유

엘라스틱서치는 메모리를 많이 잡아 먹는 애플리케이션이다. 우리는 시스템에서 제공되는 물리 메모리를 JVM 힙에 할당해서 엘라스틱서치가 사용하도록 설정할 수 있다. 일반적으로 힙 메모리가 클수록 그에 비례해서 성능도 올라간다

그렇다면 적당한 힙 크기는 얼마일까? 너무 작으면 OOM 오류가 발생하고 너무 크면 Full GC가 발생했을때 시스템 전체가 멈추는 STW가 발생할 수도 있다. 이번 이야기는 시스템에 탑재된 물리 메모리 크기에 따라 엘라스틱서치에서 힙 크기를 얼마로 설정하는게 좋을지 알아보자.

## 엘라스틱서치와 힙 크기

기본적으로 엘라스틱서치의 힙 크기는 1GB로 설정되어 있다. 이는 테스트 용도로 제공되는 값으로 최소사양을 가정해서 설정된 값임을 명심하자. 

엘라스틱서치는 기본적으로 메모리를 많이 활용하는 애플리케이션이기 때문에 처음부터 Xms와 Xmx 크기를 같게 많이 설정하는것이 여러모로 유리하다. (Xms: 최소 힙크기, Xmx: 최대힙크기)

그럼 적절한 힙 크기는 얼마가 좋을까? 이는 매우 어려운 문제다. 일반적으로는 힙 크기가 클수록 좋다. 하지만 무작정 큰 메모리를 할당하는것은 또 다른 문제를 야기할 수 있다. 만약 수십 GB의 물리 메모리를 가지는 대형 서버가 있고 엘라스틱서치를 위해 힙크키로 물리 메모리에 대부분을 할당하면 어떻게 될까? 아마도 엘라스틱서치의 성능에 큰 문제를 발생시킬 것이다. 좀 더 설명하겠지만 여러이유로 **엘라스틱서치에서는 할당할 힙의 크기의 최대값으로 32GB이하를 설정하는 것을 권장한다.** 

### 운영체제에 50%의 메모리 공간 보장하자

엘라스틱샤드는 내부에 루씬을 가지고 있으며 루씬은 새그먼트 생성 및 관리를 위해 커널 시스템 캐시를 최대한 많이 활용하고 잇따. 실시간 검색을 지원하기 위해서는 루씬이 최대한 많은 시스템 캐시를 확보하도록 지원해야 한다. 시스템 캐시는 운영체제가 가지고 있는 메모리 공간으로 커널 내부에 존재한다. 그러므로 물리적인 메모리 공간 50%는 운영체제가 자유롭게 사용하고 나머지는 엘라스틱서치에 할당하는 것이 적절하다. 

### 자바 8기반에서는 힙 크기를 32GB이상 사용하지 말자

엘라스틱서치에서는 가급적이면 힙의 크기를 크게 잡되 최대 32GB를 넘지는 말것을 권장한다. 그렇다고 32GB를 초과하는 힙 크기를 설정해도 마냥 문제가 되는 것은 아니다 어떤 이유로 32GB로 제한을 안내하는 것일까?

이 이유는 핫스폿(Hot-Spot) JVM의 Object Pointer 정책 때문이다. 사실 모든 자바 기반 애플리케이션에는 Object Pointer 정책이 모두 동일하게 적용되기 때문에 최대 힙 크기를 32GB로 제한하는 것은 모든 자바 기반 애플리케이션에게 동일하게 해당하는 내용이다. 
 
 Object Pointer는 간단히말해 객체의 메모리 번지를 표현하는 주소값이다. 힙에 생성된 모든 객체는 이런 주소값으로 접근하게 된다. 자바 JVM은 32비트와 64비트를 모두 제공하며 이런 사실을 미루어 짐작하면 32비트 JVM은 32비트, 64비트 JVM은 64 비트 주소값을 가질 것처럼 보인다. 하지만 예쌍과 다르게 모두 32비트 Object Pointer를 사용하고 있다. 

## Ordinary Object Pointer

자바에서는 모든 객체가 힙 영역에 생성된다. 그리고 이렇게 생성된 객체는 모두 포인터를 가지고 이를 통해 객체에 접근한다. JVM은 힙 영역에 생성된 객체에 접근하기 위해 포인터의 주소를 Ordinary Object Pointer(OOP)라고 하는 특수한 자료구조를 만들어서 관리하고 있으며, 이러한 OOP들은 CPU처리 단위에 따라 동작하는 방식이 약간 달라진다.

32비트 시스템은 하나의 포인터를 표현하기 위해 32비트가 필요하다. 32비트를 이용하면 최대 4GB의 메모리 주소밖에 가리킬 수 없다. 하지만 하드웨어 기술의 발달로 64비트 컴퓨터가 보급되었다. 

64비트 컴퓨터의 경우 메모리상의 주소를 가르키는 포인터 1개를 64비트로 표현하다보니 많은 메모리 공간의 낭비가 발생한다. 인식가능한 물리적 메로리 크기가 늘어나긴 했지만 그에 따라 활용 가능한 메모리의 물리적 공간 활용성은 상대적으로 떨어지게 되었다. 이 뿐만 아니라 CPU 내부에는 빠른 연산을 위해 다양한 캐시가 있다. 캐시 적중률을 높이기 휘새 주메모리와 캐시 사이에선 지속적으로 값의 이동이 발생한다. 그런데 이때 이동하는 값들도 64비트이기 때문에 32비트에 비해 상대적으로 더 큰 대역폭을 사용하게 된다. 

64비트가 되면서 물리적 한계는 극복했지만 상대적으로 메모리 공간 낭비나 연산 속도 저하등의 단점도 나타났다. 자바의 경우도 마찬가지다. 32비트가 되면서 OOP를 위해 낭비되는 메모리 문제가 크게 대두됐다. 자바는 이러한 문제를 해결하기 위해 기존 OOP를 개선해서 Compressed OOP라는 새로운 개념의 포인터 관리 기법을 도입했다.

### Compressed OOP

자바는 64비트 가상 머신의 성능 향상과 효율적인 메모리 사용을 위해 Compressed OOP를 사용하다. 이 기능은 JDK6에 옵션으로 제공되다가 JDK7부터는 기본 설정으로 사용하고 있다. 

Compressed OOP는 포인터 공간의 낭비를 줄이고 좀 더 빠른 연산을 위해 포인터를 압축해서 표현하는 일종의 트릭이다. 이 트랙의 핵심원리는 포인터가 객체의 정확한 메모리 주소를 가리키게 하는것이 아니라 상대적인 오브젝트 오프셋(Object Offeset)을 가리키도록 살짝 변형해서 동작시키는 것이다. 만약 8비트로 이 트린을 사용하면 256바이트의 물리적인 주소공간을 표현하는게 아니라 256개의 객체를 가리킬 수 있게 된다. 한 객체의 크기가 8비트라고 하면 8비트 포인터를 이용하면 기존보다 무려 8배나 큰 주소를 사용하는 것이 가능하다. 

자바는 데이터 타입에 따라 객체를 8비트 ~ 64비트까지 8 배수 형태로 힙 메모리에 생성하기 때문에 Compressed OOP를 이용해 포인터가 객체를 가르키게 한다면 32비트만으로 최대 32GB의 힙 메모리 공간을 인식하는 것이 가능해진다. 

결과적으로 64비트 시스템에서 Compreseed OOP를 사용할 경우 포인터를 표현할때 예외적으로 32비트 포인터를 사용해 동작한다. 32비트 포인터를 이용하면서도 64비트 포인터가 가지는 메모리 낭비등의 단점을 위회해서 동작하는 것이다. 

하지만 이러한 힙은 32GB가 넘어서면 더는 사용할 수 없다. 32GB를 넘어서는 순간 64비트의 OOP로 자동 전환되기 때문에 32비트를 이용하는 이점을 모두 잃어버린다. 그래서 엘라스틱서치에서는 힙 크기를 설정할때 최대 32GB이하로만 설정하라고 안내하는 것이다.

## 엘라스틱서치에서 힙 크기 설정하기

너무 큰 힙 크기는 시스템이 자주 STW에 빠지게 만든다. 힙 크기가 커질 수록 FullGC를 수행하는 시간이 늘어나고 그에 비례해서 시스템 처리 불능 시간도 늘어난다. 

이제 상황에 따라 물리 메모리를 어떻게 사용하면 좋을지 정리하겠다.

* 적절한 성능의 서버가 있을때
	* 하나의 성능 좋은 서버 보다는 가능한한 64GB이상의 물리 메모리를 가진 다수의 서버가 낫다. 메모리의 반을 엘라스틱서치에게 할당하고 나머지는 운영체제가 가지도록하는 것이 이상적이다.
* 고성능 서버가 있을때
	* 일반적인 경우 탑제된 물리 메모리의 반을 다시 32GB씩 나누어 그 수만큼 엘라스틱서치 인스턴스를 생성한다. 예를 들어 만약 128GB의 물리 메모리를 가지고 있다면 절반을 운영체제(루씬이 시스템 캐시 사용하도록)에게 주고 나머지 64GB를 32GB씩 나누어 2개의 엘라스틱서치 인스턴스를 만드는 것이다.
* 전문 검색을 주목적으로 엘라스티서치를 사용하는 경우
	* 엘라스틱서치로 32GB를 할당하고 나머지는 모두 운영체제에게 줘서 시스템 캐시를 통해 메모리를 최대한 사용하도록 한다. 전문 검색의 경우 메모리 연산보다는 루씬의 역색인 구조를 사용하는 경우가 훨씬 많을 것이다. 루씬은 내부적으로 mmap을 통해 사용 가능한 모든 시스템 캐시를 이용해 세그먼트를 캐시할 수 있기 때문에 빠른 전문 검색이 가능해질 것이다.
* 일반적인 데이터필드에서 정렬/집계 작업을 많이 하는 경우
	* 숫자, 날짜, keyword 같은 데이터 타입은 필드가 별도의 분석과정을 거치지 않는다. 이 경우 정렬이나 집계 시 루씬의 DocValues를 사용하기 때문에 힙 공간은 거의 사용되지 않는다. 이럴땐 엘라스틱서치 힙에 32GB를 할당하고 나머지를 모두 루씬이 사용하도록 한다.
* 전문 필드에서 정렬/집계 작업을 많이 하는 경우
	* 분석된 문자열 필드에서 정렬이나 집계를 수행하는 경우 루씬의 DocValues를 사용할 수 없기 때문에 fielddata라는 힙 기반의 캐시를 써야한다. 그러므로 많은 힙 메모리가 필요하다. 따라서 32GB의 힙 크기를 가진 엘라스틱서치 인스턴스를 여러개 생성하는 방식이 좋다.

요즘은 512GB 이상의 물리 메모리를 가진 고성능 서버가 점차 보편화되고 있다. 만약 서버에 수백 GB의 물리 메모리가있다면 어떻게 해야할 까? 기본적으로는 32GB의 힙 크기를 가지도록 제한하는 것은 굉장이 중요한 기준이다. 

>하나의 물리서버에 다수의 엘라스틱서치 인스턴스 실행시 주의사항
>장애 복구를 위해 레플리카 샤드가 존재하는데 일반적으로 원본이 프라이머리 샤드와 물리적으로 서로 다른 서버에 생성된다. 물리적으로 분산되어야 서버가 다운될 경우 즉시 복구할 수 있기 때문이다.
>하지만 하나의 물리서버에서 다수의 인스턴스가 동작하면 이러한 고가용성의 문제가 생길 수 있다. 이를 방지기 위해 엘라스틱서치에서는 cluster.routing.allocation.same_shard_host 옵션을 제공한다. 인스턴스를 실행할때 이 설정으로 프라이머리 샤드와 레플리카 샤드가 같은 서버에 배치되는 것을 최대한 방지할 수 있다. 만약 하나의 물리서버에 다수의 인스턴스가 실행되는 환경이라면 반드시 이 옵션을 활성화해야 한다.

## 엘라스틱서치에서 Compressed OOP 사용하기

엘라스틱서치 클러스터 구축할때 Compressed  OOP를 사용하려면  어떻게 해야할까? 
요즘 최신 JDK에서는 Compressed OOP가 기본설정이기 때문에 힙 크기를 단순히 32GB이하로 잡으면 알아서 된다.

32GB를 의미하는 정확한 LIMIT 값은 JVM 버전 및 플랫폼에 따라 조금씩 달라진다. 최대 설정 가능한 32GB 힙 메모리를 쓴다면 몇 가지 확인해야할 사항이 있기 때문에 주의해야 한다. 

정확한 LIMIT 값을 알고 싶다면 자신의 시스템에서 일일이 확인해 보아야 한다.  힙 크기를 설정하고 JVM을 실행하고 나서 UseCompressedOops 플래그 값이 true인지 확인해 보면 되는데, 이를 통해 사용중이 시스템의 정확한 cut-off를 확인할 수 있다. 

정확히 32GB를 명시하면 보통 UseCompressedOoops = false가 나타날 것이다. 그리고 소수점 한 자리 차이로 true와 false가 갈릴 것이다. 이러한 시스템의 차이는 왜 생길까? 

**그 이유는 JVM의 힙 메모리가 사실 0번지 부터 시작하지 않기 때문이다.** 객체를 가리키는 포인터가 0번지 부터 시작되지 않았기 때문에 0번지 부터 실제 시작 번지까지 앞쪽에 위치한 일부 메모리 공간은 활용할 수 없게 되는것이다. Compressed OOp는 이론상 최대 32GB의 메모리 번지를 가리킬 수 있지만 시작 번지가 0이 아니기 때문에 실제로는 대략 31.998GB 이상의 힙 크기를 설정하는 순간 32GB를 넘어서게 된다.

모든 시스템의 메모리 번지가 0부터 시작하면 좋겠지만 시스템마다 시작 번지가 약간씩 다르다. 만약 이것 저것 고민하고 싶지 않다면 안전하게 31GB로 설정하는 것도 방법이다. 이렇게 하면 비교적 간단히 Compressed OOP를 확실히 적용할 수 있다.

## Zero-Based Compressed OOP

**자바에서는 힙 메모리에 8byte 단위로 객체를 저장하는데, 이런 특성을 이용하면 특정 객체를 찾아 가기 위해 시프트 연산을 사용하는 것이 가능해진다.** 메모리 상의 포인터를 계산하는 것은 비용이 높은 CPU 연산을 동반하는데 상대적으로 비용이 저렴한 시프트 연산만을 이용할 수 있다면 포인터 계산 성능을 대폭 끌어올릴수 있다.

Compressed OOP도 기본적으로 시프트 연산을 이용해 객체를 찾아가도록 되어 있다. 하지만 100% 시프트 연산만으로는 불가능하고 시프트 연산으로 생성된 값에 특정 번지수를 더하는 Add 연산이 병행되어야 정확한 번지수를 계산할 수 있다. 앞에서 보았듯이 JVM의 힙 메모리가 0번지 부터 시작하지 않기 때문이다. 

이러한 부분에서 성능을 끌어올리고자 Zero-Based Compressed OOP라는 개념이 도입되었다. Zero-Based Compressed OOP을 사용하면 JVM이 시작될때 힙 메모리 번지가 0번지로 시작되도록 논리적으로 강제한다. 이를 통해 Compressed OOP가 객체의 포인터를 빠른 성능의 시프트 연산만으로도 가능하게 한다. 

안타깝게도 Zero-Based Compressed OOP를 사용하려면 힙 메모리 크기를 지금보다 더 줄여야 한다. **논리적으로 힙 메모리의 시작 번지를 0으로 조절하기 때문에 강제로 0으로 조절하는 만큼의 메모리 번지를 사용할 수 없게 된다.** 

 JVM에서는 모든 조건이 만족하면 자동으로 Zero-Based Compressed OOP로 동작한다. 만약 운영 중 이를 넘어가면 Compressed OOP로 하고 넘어가면 일반적인 OOP로 동작한다.

일반적으로 Zero-Based Compressed OOP를 동작하기 위해서는 30GB정도의 힙을 사용해야 한다. 물론 2GB 공간을 JVM에 할당하지 못하는 것이 아쉽지만 다행이도 루씬의 시스템 캐시로 사용할 수 있기 때문에 너무 아쉬워하지 않아도 된다.

JVM의 성능을 위해 반드시 Compressed OOP를 사용하는 것을 권장하고 메모리를 더 줄여 Zero-based Compressed OOP를 사용하는 것도 좋다. 각자의 환경에 맞게 사용 하도록 하자.

# 엘라스틱서치와 가상 메모리

**현대 운영체제에서는 애플리케이션이 물리 메모리를 직접 할당받지 못한다.** 운영체제는 멀티 태스킹 실현을 위해 각 애플리케이션을 위한 **전용 가상 메모리를 만들고 이를 할당한다.** 가상 메모리를 할당 받은 애플리케이션은 자신이 할당받은 가상 메모리를 실제 메모리로 착각하고 사용하는 것이다. 

이번에는 자바 기반의 애플리케이션에서 이러한 가상 메모리가 어떤 식으로 적용되는지 확인하고 엘라스틱서치에서는 이 가상 메모리 관리를 어떻게 하면 좋을지 알아보자.

## Virtual Memory(가상 메모리)

현대 대부분 OS는 가상 메모리라는 메모리 관리 기법을 사용한다. 가상 메모리는 애플리케이션에서 물리적인 메모리 보다 더 많은 양의 메모리를 사용할 수 있도록 운영체제가 제공하는 대표적 메모리 관리 기술이다. 즉 애플리케이션이 실행될때 물리적 메모리 번지를 직접 할당해서 메모리 공간을 제공하는 것이 아니라 가상의 메모리 번지를 생성해 제공함으로써 애플리케이션 별로 전용 메모리 공간을 사용하도록 제공하는 메모리 관리 기법이다. 

이런 방식은 멀티태스킹이 가능한 운영체제에서 필수적이다. 이를 이용하면 모든 애플리케이션이 운영체제에서 인식하고 있는 물리 메모리 크기보다 더 큰 크기의 메모리를 할당 받아 사용할 수 있다. 

하지만 만약 특정 애플리케이션이 대용량의 가상 메모리를 할당받아 사용하면 운영체제 성능에 큰 악영향을 미칠 수 있다. 그래서 운영체제 입장에서는 애플리케이션 들이 CPU나 메모리 같은 시스템 리소스를 사용할때 
운영체제 차원에서 다양한 제약을 두고 있다. 모든 애플리케이션이 리소스를 효율적으로 써야하기 때문에 운영체제는 사용가능한 리소스를 제한하고 각 애플리케이션에는 제한된 리소스 이상을 쓰지 못하도록 강제한다.


### JVM을 위한 가상 메모리

우리는 -Xms와 Xmx 설정으로 JVM에서 사용할 힙 메모리를 정할 수 있다. 하지만 실제로  JVM을 돌려 보면 가상 메모리 크기와 실제 사용중인 메모리 공간을 확인 할 수 있는데, 대부분은 실제 사용하는 메모리가 월등히 적다. 왜 이런 현상이 나타날까?

보통은 운영체제가 -Xms와 Xmx설정에 맞게 가상 메모리를 애플리케이션에 할당한다. 그렇지만 실제 프로그램은 그보다 작은 메모리만으로도 충분히 실행되기 때문에 차이가 나타나는 것이다. 

운영체제에서는 효율적인 메모리 관리를 위해 메모리와 디스크 간에 데이터를 교환하는 **스와핑(Swapping)** 작업을 꾸준히 수행한다. **멀티 태스킹의 특성상 물리 메모리를 하나의 애플리케이션이 점유하지 못하고 여러 애플리케이션과 나눠서 사용해야 하기 때문이다.**

운영체제는 가상 메모리의 데이터를 나눠서 반드시 필요한 부분은 물리 메모리에 로드하고 나머지 데이터는 디스크에 임시로 저장한다. 프로그램이 동작하면서 메모리상에 필요한 부분은 지속적으로 변경되고 이럴때 마다 메모리와 디스크 간의 데이터 교환이 반복적으로 일어난다. 메모리를 많이 사용할 수록 스와핑 작업은 더 많이 일어나고 이는 성능에 큰 영향을 미친다. 

애플리케이션이 실행되면 운영체제로 부터 가상  메모리를 할당받는다. 그리고 이러한 가상 메모리를 생성하는 제약은 전적으로 운영체제 차원에서 관리한다. 엘라스틱서치도 실행시 운영체제에서 설정된 가상 메모리 관련 설정을 그대로 받는다. 또한 엘라스틱서치는 자바 기반의 애플리케이션이다. 그렇기 때문에 JVM 위에서 동작하게 되며, 앞서 설명한 바와 같이 실행 시 옵션으로 설정한 최대 힙 크기 이상의 가상 메모리를 사용할 수 없다.

### 엘라스틱 서치를 위한 vm.max_map_count 설정

엘라스칙서치는 검색을 위해 루씬을 내장한다. 루씬은 기본적으로 많은 리소스를 사용한다. 자바 기반 애플리케이션은 JVM위에서 돌아가도록 설계되어 있고 기본적으로는 JVM을 통해 할당받은 힙 메모리만 사용할 수 있다. 하지만 **루씬은 예외적으로 많은 리소스를 필요로 하기 때문에 특별한 방식으로 이 제약을 회피하고 있다.** 

루씬은 내부적으로 자바에서 제공하는 **NIO** 기술을 활용한다. **이는 운영체제 커널에서 제공하는 mmap 시스템 콜을 직접 호출할 수 있으며 이를 이용하면 VM을 거치지 않고 직접 커널 모드로 진입하기 때문에 높은 성능을 낼 수 있다.**  루씬은 mmap과 niofs 방식의 데이터리를 적절히 혼용해서 사용하는데 이러한 동작방식으로 mmap 시스템 콜을 직접 호출할 수 있다. 이로 인해 커널 레벨의 파일 시스템 캐시를 사용할 수 있다는 것이다. 

대부분 운영체제는 파일 시스템을 캐시하는데 커널 레벨의 메모리를 사용하는데, 루씬에서 생성한 세그먼트도 파일이기 때문에 파일 시스템 캐시의 이점을 제대로 누릴 수 있는 것이다. 이로써 자바 힙 메모리에 의존하지 않으면서도 커널 레벨에서 간접적으로 물리 메모리를 사용할 수 있게 된다. 그러한 이유로 50% 물리 메모리를 OS에 양보하라고 했던 것이다. 정리하자면 엘라스틱서치는 자바 힙 메모리도 사용할 수 있고 운영체제에 할당된 물리 메모리도 사용할 수 있는 일석이조의 결과를 누리게 된다.

엘라스틱서치에서 루씬이 원활하게 동작하기 위해선 가상 메모리 설정 중 mmap 크기 항목을 변경해야 한다. 대부분 운영체제에서 기본적으로 제공하는 설정 값이 너무 작기 때문이다. CentOS 7버전의 리눅스에서는 기본적으로 가상 메모리에서 생성 가능한 mmap 개수가 65,530으로 설정되어 있다. 

mmap시스템 콜을 내부적으로 많이 사용하는 엘라스틱서치 입장에서는 65,530은 너무 작다. 이를 미연에 방지하기 위해 vm.max_map_count 설정을 초기 로딩 과정에서 검사해 262,114,이하이면 오류 메세지를 출력하고 인스턴스를 강제로 종료한다. 따라서 엘라스틱 서치에서 실행하려는 운영체제는 vm_max_map_count 크기를 262,144이상으로 설정해야 한다. 

# 분산환경에서의 메모리 스와핑

엘라스틱서치 클러스터가 분산 시스템을 지향하고 대용량의 데이터를 많이 처리하다보니 시스템 리소스를 최대한 효율적으로 활용해야 한다. 따라서 엘라스틱 서치 클러스터를 운영하기 위해선 시스템적 지식도 많이 필요하게 된다. 

## 메모리 스와핑(Memory Swapping)

대부분 운영체제에선 효율적 메모리 관리를 위해 스와핑이라는 기술을 사용한다. 이로서 사용되지 않은 애플리케이션의 물리 메모리를 디스크로 열심히 스왑한다. 

스와핑이 일어나면 가상 메모리의 일부 내용을 디스크로 쓰기 위해 디스크의 일정 영역을 스왑영역으로 만든다. 이때 일어나는 동기화  작업으로 순산적으로 시스템 성능이 떨어지고 자칫 시스템 장애가 발생할 수 있다. 시스템 리로스가 충분하다면 가급적 스와핑이 일어나지 않도록 하는것이 안전하다. 

운영체제 입장에선 스와핑은 많은 리소스를 잡아먹는다. 그러므로 스와핑 작업을 모니터링하여 문제가 발생할 경우에 대해 철저히 대비해야 한다.  free 명령어를 쓰면 스왑 상태를 간단히 살펴 볼수 있다.

## 엘라스틱 서치에서 스와핑을 비활성화해야하는 이유

엘라스틱서치가 동작하는데 필요한 메모리도 스와핑으로 인해 언제든지 디스크로 스와핑 될 수 있다. 스와핑이 될 경우 노드의 안정성에 치명적이기 때문에 이를 최대한 피해야 한다. 메모리를 많이 사용하는 엘라스틱서치의 특성상 스와핑 작업으로 가바지 컬렉션이 비정상적으로 수분간 지속된다거나 노드 응답이 느려질 수 있다. 또한 클러스터간 연결이 불안정해서 연결과 끊어짐이 반복될 수 도 있다. 

**대부분 분산 시스템에서는 클러스터의 안정을 해치는 것보다 문제가 발생한 노드가 강제로 종료되어 클러스터 구성에서 제외되는 편이 훨씬 효율적이다. 그러므로 엘라스틱서치에서는 어떤 대가를 치루더라도 스와핑이 절대 발생하지 않도록 해야 한다.**

클러스터를 구성하는 노드에는 가능한한 엘라스틱서치를 제외한 다른 애플리케이션을 설치하지 말고 엘라스틱서치 단독으로 운영하는 것이 여러모로 유리하다.  애플리케이션으로 엘라스틱서치만 존재한다면 물리 메모리를 독점할 수 있기 때문에 스와핑을 사용할 필요가 없어진다. 

스와핑 작업이 일어나지 않도록 시스템 차원에서 여려 방법이 있다.  엘라스틱 서치 입장에선 스와핑 자체가 클러스터를 운영하는데 매우 치명적이기 때문에 여러 안전 장치를 이중 삼중으로 설정하는 것이 중요하다. 스와핑 비활성화, 그리고 memory_lock 속성도 활성화하여 추가적인 설정도 해두기를 권장한다.

### 스와핑 비활성화

시스템을 엘라스틱서치 노드 전용으로 사용하는 것이 가능하다면 스와핑을 완전히 비활성화하는 것이 가장 좋은 방법이다. 루트 권한으로 swapoff 명령어를 쓰면 일시적으로 스와핑을 비활성화할 수 있다.

### 스와핑 최소화

시스템 특성상 스와핑을 완전히 비활성화할 수 없는 상황이라면 스와핑 주기를 조절해서 발생 빈도를 최소화할 수 있다. vm.swappiness 값을 1로 설정하면 스와핑을 최대한 사용하지 않겠다는 의미이다. 

하지만 운영체제가 메모리 관리 차원에서 필연적으로 스와핑이 필요하다고 판단할 경우에는 언제든지 스와핑이 일어날 수 있으므로 주의해야 한다. 

### memory_lock 설정

시스템 레벨에서 스와핑을 사용하지 못하게 설정하려면 루트 권한이 필요하기 때문에 사용자 권한만 있다면 스와핑을 막을 수 없다. 하지만 제한적이나마 스와핑을 최소화 하는 방법이 있다. 엘라스틱서치 환경설정으로 제공하는 bootstrap.memory_lock 속성을 이용하는 방법이다. 이를 활성화 하면 mlockall()함수와 동일하게 스와핑을 최대한 방지할 수 있다. 

>mlockall() 함수
>커널 수준에서 제공하는 저수준 함수의 일종이다. 호출한 프로세스의 페이징을 금지시키고 모든 메모리가 램에 상주하는 것을 보장한다. 이는 애플리케이션 개발을 할때 사용하는 memory_lock 기술인데 이를 이용하면 애플리케이션이 최초 실행할때 할당받은 메모리를 스와핑하지 못하게 강제로 메모리에 담아둘 수 있다.

하지만 **memory_lock 기술은 어디까지나 애플리케이션 레벨에서만 의미가 있는 제한된 기술이다.** 메모리 사용량이 적을때는 의도한대로 동작하지만 운영체제 입장에서 메모리가 부족하면 이를 무시하고 스와핑이 일어날 수 있다. 따라서 루트권한을 얻어 시스템 설정을 변경하는 것이 제일 안전하다. 

# 시스템 튜닝 포인트

리눅스에서는 시스템 최적화를 위한 다양한 툴이 제공된다. 대표적인 것이 ulimit 명령어와 sysctl 명령어이다. ulimit 명령어는 유저 레벨의 최적화를 위해 사용된다. 이를 이용하면 애플리케이션이 실행하는데 필요한 각종 리소스를 설장할 수 있다. sysctl은 커널 레벨의 최적화를 위해 제공되는데 이로 커널 파라미터를 간접적으로 조정할 수 있다. 이를 이용해서 엘라스틱서치가 효율적으로 동작하기 위해 많은 부분을 최적화 할 수 있다. 

## 애플리케이션에서 튜닝 가능한 리소스

ulimit 명령어
: 애플리케이션이 실행될때 얼마만큼 운영체제로 부터 리소스를 할당받을 수 있는지 전반적인 리소스 관리를 수행한다.

운영체제에서는 여러 애플리케이션이 동시에 동작할 수 있기 때문에 특정 애플리케이션이 리소스를 독점하지 못하도록 관리하는 것이 매우 중요하다. **ulimit 명령어는 애플리케이션이 실행될때 얼마만큼 리로스를 할당받을 수 있는지 전반적인 리소스 관리를 수행한다.** 

ulimit에는 애플리케이션이 생성될때 할당받은 리소스의 최대값이 관리되고 있고, 이는 모든 애플리케이션에 공통으로 적용된다. 즉, 애플리케이션이 실행될때 ulimit에 설정된 양만큼만 할당 받을 수 있다. 

리눅스에서는 ulimit -a 명령어를 통해 한 프로세스가 가질 수 있는 리소스의 정보를 알 수 있다. 운영체제 입장에서는 엘라스틱서치도 하나의 프로세스이기 때문에 ulimit에서 설정된 값 이상의 리소스를 사용할 수는 없다. 

sysctl 명령어
: 리눅스 내부에 존재하는 커널 파라미터를 조절할 수 있도록 설정 값을 제공한다. 이를 이용해 커널 레벨의 다양한 파라미터를 확인할 수 있다. 

sysctl은 ulimit 명령어와 더불어 애플리케이션이 실행될때 생성되는 리소스의 정보를 커널 레벨까지 비교적 자세히 제공한다.

자 이제 위 두가지 명령어로 유저 레벨의 튜닝을 해보도록 하자. 

## ulimit 명령어를 이용한 유저레벨 튜닝

ulimit 값을 변경하는 작업은 매우 신중하게 진행해야 한다. 시스템의 특성이나 처리하는 데이터의 종류에 따라 리소스 제한을 변경해야 좀 더 좋은 성능을 발휘하는 경우도 있겠지만 일반적인 경우는 기본 설정 값을 사용하는것만으로도 충분하다. 

ulimit 값은 운영체제에서 실행되는 모든 애플리케이션에 적용되기 때문에 각 항목이 무엇을 의미하는지 자세히 알 필요가 있다. 

```
$ulimit -a // 운영체제에 설정된 전체 리로스 제한값 조회하기
$ulimit -l unlimited //스와핑 최소화를 위해 memory_lock 크기를 unlimited로 변경하기; -l [memlock]
```

### ulimit 소프트 설정과 하드 설정

소프트 설정은 **프로세스가 실행될때 최초로 할당 받는 값**이고 **하드 설정은 운영중에 리소스 한계에 도달할 경우 추가로 할당받을 수 있는 값**이다. 

이렇게 이중으로 리소스 제한값을 관리하는 이유로 모든 프로세스가 최대값으로 리소스를 할당받으면 자칫 리소스 낭비가 심해질수 있기 때문이다. 

대부분의 애플리케이션은 ulimit에 설정된 값보다 작은 리소스를 사용하는 것만으로도 충분하기 때문에 소프트, 하드 설정으로 나누어 설정하고 소프트 설정보다 큰 리소스가 필요할 경우에만 하드 설정까지 리소스를 바도록 하자. 

엘라스틱서치의 경우 많은 리소스를 사용한다 그래서 처음부터 소프트 설정 값과 하드 설정 값을 동일하게 설정하는 것이 좋다. 소프트 설정 값에 따라 최초 할당을 받은 후 추가 리소스를 할당할때도 비교적 많은 비용이 들기 때문에 처음부터 많이 가져오도록 하자.

### ulimit 영구설정

앞서 설정한 내용들은 시스템이 리부팅되면 모두 초기화된다. 시스템을 운영 하다보면 시스템이 비정상 종료되는 경우가 종종 발생하기도 하기 때문에 리부팅에 의해 설정된 내역이 초기화되면 바로 장애로 이어질 수 있다. 

각종 설정 정보가 튜닝되고 일단 시스템이 안정화된고 나면 설정 정보를 변경하는 일은 매우 위험할 수 있다. 그러므로 설정 내역이 유지되도록 영구적으로 저장하는 것이 반드시 필요하다. 

리눅스에서 설정 내역을 영구적으로 저장하기 위한 용도로 /etc/security/limits.conf 파일을 제공한다. 해당 파일을 수정하면 영구적으로 적용시킬 수 있다. 엘라스틱서치의 경우 애초에 소프트 설정과 하드 설정 모두 같은 값을 갖도록 하는게 좋다.

## sysctl 명령어를 이용한 커널 레벨 튜닝

sysctl로 제공되는 항목들은 모두 커널 레벨의 민감한 정보이기 때문에 수정할때 항상 주의해야 한다.

sysctl 명령어를 이용하면 커널 파라미터를 수정할 수 있다. 
```
sysctl -a //sysctl로 커널 파라미터 조회
sysctl -w 파라미터 = 파라미터 값 // 커널 파라미터 수정
etc/sys
```



> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ1MjM3MjIzLDE4ODg4NTQxOTEsMTM2Mz
I3ODI3MCwxOTgwODQ3MTYzLDE2MDY3NjUyOTcsNTc2Mzg0MjE4
LC0xMzQ5ODAyMzEyLDEyOTQ2NTYyMDMsLTE0ODk4NTkwNTUsLT
E5NDc1NjAwNjksLTUxNTYwMTc4NiwtOTcyNzg1NjE0LDE0NTI3
NDcxNjUsMTkyNzEwMDA5MSwtMTA0NDIwMDQ1MywtMTAwODIwNj
MzMywtMTAxMTMxNzIwMiwtMTgzNTYxMDg5NywtNjc4ODQxMDE2
LDEzMDc4Nzc3ODNdfQ==
-->