# Overview

클러스터는 최대 몇개의 인덱스를 생성할 수 있을까? 하나의 샤드 크기는 얼마가 적당할까? 이런 질문은 엘라스틱서치를 운영하는 사람들에게는 한번쯤 고민해봤을 질문이다. 

시간이 흐를수록 관리되는 데이터는 점점 커진다. 현재 서비스가 원할히 된다고 해서 앞으로 서비스가 잘 될거란 보장은 없다. 분산 시스템의 경우 이론상 데이터가 무한히 증가할 수 있기 때문에 최적화의 필요성이 더욱 더 커진다. 

분산 검색 시스템인 엘라스틱서치를 운영한다는 것은 항상 최적화를 고민해야 한다는 것과 같다. 이번 장에서는 운영하면서 한번쯤 고민해야할 사항을 알아보겠다.

# 운영 중 샤드 개수 수정이 불가한 이유

클러스터에서 운영 중인 인덱스는 원칙적으로 수정이 불가능하다. 인덱스를 생성할때 한번 설정된 샤드의 개수는 절대 변경이 불가능하기 때문에 데이터의 크기가 최대 얼마까지 증가할 것인지 잘 계산해서 최초 인덱스 생성시 샤드의 개수를 신중하게 결정해야 한다. 

샤드에는 두 종류가 존재한다. 첫번째는 실제 서비스가 일어나느 프라이머리 샤드이다. 실질적인 CRUD를 제공하는 샤드로 엘라스틱서치에서 핵심요소이다. 이 프라이머리 샤드를 일반적으로 샤드로 부른다. 두번째는 레플리카 샤드다. 기본적으로 장애 복구를 위해 존재한다. 하지만 프라이머리 샤드와 동일한 데이터를 가지고 있기 때문에 평상시에는 읽기 분산에도 활용된다. 

엘라스틱서치에서는 최초 인덱스를 생성할때 settings 속성을 이용해 샤드와 레플리카 개수를 각각 정의할 수 있다. 

number_of_shards
: 샤드의 개수(number_of_shards)는 전체 데이터를 몇개의 샤드로 나누어 보관할지를 의미한다. 이 값으로 프라이머리 샤드의 개수가 결정된다.

number_of_replicas

: 레플리카 개수(number_of_replicas)는 몇개의 복사본 세트를 만들것인지를 의미한다. 이 속성 값으로 레플리카 샤드 세트의 개수가 결정된다.

한 임의의 인덱스를 생성할때 만약 5개의 샤드와 1개의 레플리카 세트를 만든다고 했고 시간이 흘러 인덱스에 총 1억건의 데이터가 색인 됐다고 가정해보자. 5개의 프라이머리 샤드를 설정했기 때문에 각 샤드는 2천건의 데이터를 가져가게 될것이다. 

이런 상황에서 검색 성능을 향상 시키기 위해 물리적인 장비를 추가 투입해서 샤드 5개를 추가하기로 했다고 하자. 이런 경우 기존 프라이머리 샤드의 개수를 5개에서 10개로 변경하면 모든 문제가 간단히 해결될 것이다. 하지만 엘라스틱서치에서는 한번 생성된 프라이머리 샤드 개수의 변경을 허용치 않는다. 도대체 그 이유는 무엇일까?

엘라스틱 서치 샤드는 루씬 인덱스의 확장이라고 했다. 각 샤드는 내부에 독립적인 루씬 라이브러리를 가지고 있으며, 루씬은 단일 서버에서만 동작하는(Stand Alone) 검색 엔진이다. 

이러한 특성 때문에 샤드 내부의 루씬 입장에서는 함께 인덱스를 구성하는 다른 샤드의 존재를 전혀 눈치채지 못한다. 루씬 입장에서는 엘라스틱 서치를 구성하는 전체 데이터가 별도로 존재하고 자신은 그 중 일부만 가지고 있다는 사실을 알 수가 없는것이다. 오직 자신이 가지고 있는 일부 데이터만을 가지고 색인과 검색이 최대한 효율적으로 이뤄지도록 노력할 뿐이다. 한마디로 정리하자면 **단순히 2천건의 데이터를 가지고 있는 완전히 독립적인 루씬 5개가 존재한다는 것이다.** 

프라이머리 샤드의 개수를 변경한다는 것은 각각 독립적인 루씬이 가지고 있는 데이터들을 모두 재조정한다는 의미와 같다. 만약 루씬의 개수가 5개에서 10개로 늘어난다면 각 루씬이 가지고 있는 세그먼트가 잘게 쪼개져 일부 세그먼트들을 새롭게 추가된 루씬쪽으로 전송되어야 할 것이다. 그리고 새로 추가된 루씬은 여기저기서 전송된 세그먼트 조각들을 모아서 다시 합치는 작업을 수행해야 할 것이다. 세그먼트의 불변성을 위해서 원칙적으로 세그먼트의 변경을 불가능하다. 이러한 이유로 현재는 프라이머리 샤드의 개수를 변경할 방법이 없는것이다. 

엘라스틱 서치에서는 프라이머리 샤드의 개수를 변경하기 위해서는 새로운 인덱스를 생성하고 재색인 하도록 안내한다. 그리고 이를 지원하기 위해 ReIndex API를 제공한다. 

## Reindex API

**현재로서는 샤드 개수를 변경하기 위해 인덱스를 재생성하고 전체 데이터를 처음 부터 재색인하는 방법밖에 없다.** 이를 위해 엘라스틱서치에서는 이미 존재하는 인덱스를 새로운 인덱스로 다시 색인하는 Reindex API를 제공한다. 이를 이용하면 비교적 손쉽게 새로운 인덱스로 재색인할 수 있다. 

```
POST _reindex {
	"source" : { "index": "movie" },
	"dest" : {"index": "new_movie"}
}
```

# 레플리카 샤드 복제본 수는 얼마가 적당할까?

프라이머리 샤드는 처음 설정한 샤드 개수 변경이 불가능하지만 레플리카 샤드의 복제본 수는 운영 중에도 언제든 변경이 가능하다. 레플리카 샤드를 추가하고 싶은 경우 기존 프라이머리 샤드를 복사하기만 하면 되기 때문이다. 

일반적으로 장애가 발생했을때 빠른 복구를 위해 1개 이상의 복사본 세트를 사용하는게 좋다. 레플리카 샤드는 직접 생성할 샤드 개수를 지정할 수 없다. 단지 몇 개의 복사본 세트를 가질지 정할 뿐이다. 이는 레플리카 샤드가 프라이머리 샤드 전체를 복사해야만 그 의미가 있기 때문이다. 


number_of_replicas 값은 레플리카에 사용할 복사본 세트의 개수를 말한다. **실제 생성될 레플리카 샤드의 개수를 말하는게 아니다.** 실제 운영될 레플리카 샤드 개수는 그 인덱스에서 사용하는 프라이머리 샤드의 개수 * 레플리카 세트 개수(number_of_shards * number_of_replicas)이다.

복사본 세트의 수를 지정함으로써 모든 프라이머리 샤드가 복사되기 때문에 클러스터 내부에서 운영하고 있는 전체 샤드 개수를 고려해서 적절한 복사본 세트를 운영해야 한다. 장애 복구나 읽기 성능 향상을 위해 레플리카를 많이 사용하게 되지만 한가지 주의해야 할 점이 있다. 레플리카 세트의 수를 결정할 경우에는 사전에 충분한 테스트가 필요한데, **너무 많은 복사본이 존재하는 경우, 자칫 전체적인 색인 성능의 저하로 이루어질 수 있기 때문이다.** 

레플리카 샤드도 프라이머리 샤드와 마찬가지로 내부에 루씬을 가지고 있다. 엘라스틱 서치에 데이터가 추가되면 마스터 노드에 의해 적절히 라운팅 되어 특정 프라이머리 샤드로 데이터가 전송된다. 해당 샤드 내부에서는 루씬에 의해 새그먼트가 생성되는데, 레플리카도 이와 동일한 검색 결과를 보장해야 하기 때문에, 존재하는 모든 레플리카 샤드에도 데이터가 전송된다. 이처럼 프라이머리 샤드와 레플리카 샤드는 모두 동일한 세그먼트 생성 과정을 거치게 되는 것이다. 이러한 과정때문에 모든 복사본에 일관성이 부여된다. 이 일관성 때문에 레플리카를 이용해 읽기 분산이 가능해 진다. 

레플리카에도 프라이머리와 마찬가지 매커니즘으로 동작하기 때문에 리플리카가 많아질 수록 색인 성능이 비례해서 하락한다. 읽기 분산이 중요한 경우에는 색인 성능을 일부 포기하고 레플리카 세트 수를 늘리는 것이 좋을테고, 빠른 색인이 더 중요한 경우에는 읽기 분산을 일부 포기하고 레플리카 세트 수를 줄이는 것이 좋을 것이다. 

다행히 복사본의 수는 운영 중에 언제라도 변경이 가능하다. 그렇게 때문에 처음에 서비스를 운영할때는 복제본의 수를 최소화해서 운영을 시작하는 것이 좋다. 운영 중에 노드의 장애나 데이터 량에 따른 읽기 분산을 지속적으로 모니터링하고 이를 바탕으로 탄력적으로 북사본의 수를 조절해 나나가는 것을 권장한다. 

# 클러스터에서 운영 가능한 최대 샤드 수는?

**먼저 결론부터 이야기하자면 전체 샤드의 수에 대한 특별한 제한은 없다.** 이론 상 클러스터에는 인덱스가 무한대로 생성될 수 있기 때문이다. 하지만 개별 인덱스를 설정할대 설정 가능한 최대 샤드의 수는 현재 1024개로 제한되어 있다. 개별 인덱스를 생성할때 1024를 초과하는 값을 설정할 경우 오류가 발생한다. 엘라스틱서치 초기에는 이런 제약이 없었는데 새로 제약이 생긴 이유는 **극단적으로 많은 샤드를 생성할 경우 마스터 노드에 상당한 부하가 걸리기 때문에 마스터 노드의 보호차원에서 최소한의 제약을 만든 것이다.** 

현재 엘라스틱서치에서 강제하는 전체 샤드의 수에 대한 특별한 제한이 없기 때문에 리소스가 허용하는 한 샤드를 계속해서 생성할 수 있다. 하지만 샤드 수가 너무 적거나 너무 많을 경우 문제가 생길 수 있으며, 운영할 데이터의 크기에 따라 다양한 관점에서 고민해봐야 한다. 


# 클러스터에 많은 수의 샤드가 존재하는 경우

모든 샤드는 마스터 노드에서 관리된다. 그러므로 샤드가 많아질 수록 마스터 노드의 부하도 증가한다. 샤드가 많아질수록 관리해야하는 정보의 양도 많아지기 때문이다. 

마스터 노드에서 처리해야할 정보가 많아지면 검색이나 색인 작업도 덩달아 느려질 수 있다. 또한 마스터 노드의 메모리 사용량도 늘어난다. 마스터 노드는 빠른 처리를 위해 샤드 정보와 같은 관리 데이터를 모두 메모리에 올려서 제공하기 때문이다. 따라서 너무 많이 데이터로 인해 마스터 노드의 메모리가 부족해지지 않도록 주의해야 한다. **만약 마스터 노드에 장애가 발생한다면 클러스터 전체가 마비되는 대형사고로 번질 수 있다.**   

> 마스터 노드의 역할
> 모든 노드와 샤드를 관리하는 역할
> 평소 노드의 상태를 모니터링하면서, 색인 요청에 대한 라우팅을 처리하거나 검색 요청에 대한 부하를 분산하는 역할
> 장애 발생시 레플리카를 이용해 샤드를 복구하는 역할

## 인덱스가 다수의 샤드로 분산될 경우

단순히 검색 성능만 본다면 인덱스를 생성할때 프라이머리 샤드의 개수가 많을 수록 검색 성능이 좋아진다. 검색은 각 샤드가 독립적으로 검색을 수행하고 최종적으로 결과를 합쳐서 제공하기 때문에 다수의 샤드로 분산될수록 검색 속도가 빨라진다. 

샤드가 여러개로 나누어져 있을 경우, 다수의 리소스를 동시에 사용하고 그에 비례해서 시간이 단축되기 때문에 읽기 성능을 노이는 가장 좋은 수단이 된다. 

## 샤드의 물리적인 크기와 복구시간

마스터 노드의 입장에서는 샤드가 가지고 있는 데이터의 건수보다는 데이터의 물리적인 크기가 더 중요하다. **마스터 노드는 장애가 발생할 경우 샤드 단위로 복구를 수행하기 때문이다.** 

**일단 특정 노드에 장애가 발생하면 장애가 발생한 프라이머리 샤드와 동일한 데이터를 가지고 있는 레플리카 샤드가 순간적으로 프라이머리 샤드로 전환되어 서비스 된다.** 

그와 동시에 프라이머리 샤드로 전환된 샤드(레플리카 -> 프라이머리)와 동일한 샤드가 물리적으로 다른 장비에서 레플리카 샤드로 새롭게 성성된다. 그렇게 서비스는 안정적으로 한동안 유지된다. 시간이 지나 장애가 발생한 노드가 복구되면 복구된 노드로 일부 샤드들이 네트워크를 통해 이동한다. 이렇게 내부적으로 전체적인 클러스터의 균형을 맞춘다. 

이처럼 복구 시 샤드 단위로 데이터가 이동하기 때문에 샤드의 크기가 클 수록 복구작업에 부정적인 영향이 발생할 가능성이 있다. 그렇다면 적절한 샤드의 크기는 얼마일까?

**특별히 정해진 공식적인 크기는 없지만 다양한 사례를 통해 샤드 1개가 물리적으로 50GB를 넘지 않도록 권장하고 있다.** 

# 하나의 인덱스에서 생성 가능한 최대 문서수는?

이 의문을 해결하기 위해선 루씬을 다시 살펴볼 필요가 있다. 엘라스틱서치 샤드는 내부적으로 루씬 인덱스를 확장한다. 다수의 세그먼트를 가지고 이를 지속적으로 병합하는 과정으로 큰 세그먼트를 만들다. 세그먼트는 최대 몇건의 문서를 저장할 수  있을까?

루씬은 내부 세그먼트에 색인 가능한 최대 문서를 Java.lang.Integer 클래스의 Integer.MAX_VALUE에서 128을 뺀 만큼의 크기인 2,147,483,519를 가질 수 있다. (Intger.MAX_VALUE - 128)

하나의 샤드에서 색인할 수 있는 문서의 수가 대략 20억개 정도이고 한 인덱스는 최대 1024개의 샤드를 가질 수 있기 때문에 이론적으로 엘라스틱 서치에서 **개별 인덱스가 가질 수 있는 최대 문서수는 2조개라고 생각하면 된다.** (루씬에서 생성 가능한 문서수 * 설정 가능한 샤드수; 20억 * 1024)

루씬이 가지는 20억개라는 최대 문서 수 제약은 과거에는 큰 숫자였을지 몰라도 빅데이터가 끊임없이 쏟아지는 오늘날에는 충분히 현실적인 크기가 되었다. 

엘라스틱서치는 분산 검색엔진으로 설계됐기 때문에 사실상 색인 가능한 문서 수 제약을 제거했다. 이론상 샤드의 개수가 늘어날수록 수용 가능한 데이터의 수도 늘어난다. 하지만 안정적인 클러스터 운영을 위해서는 환경에 맞는 충분한 테스트가 필요하다. 최적의 인덱스 수나 최적의 샤드의 수를 구하는 공식 같은것은 현실에 존재하지 않는다. 

# Bootstrap

일반적으로 개발환경과 운영환경은 하드웨어적으로 큰 차이가 있다. 하드웨어 성능이나 운영체제 설정이 다를 경우 동일한 결과를 보장하지 않기 때문에 개발환경에서 클러스터를 구축하고 얻은 데이터를 운영환경에 그대로 사용해서는 안된다. 개발 완료 후 반드시 운영환경에서 재확인 하는 과정이 필요하다. 

엘라스틱서치는 운영환경이라고 판단되면 실행시 부트 스트랩(Bootstrap)이라는 과정으로 잘못된 설정이나 문제점을 친절히 알려준다. 이를 통해 일반적인 설정 오류는 방지할 수 있다. 하지만 시간이 흘러 데이터가 많아지면 잠재되어 있던 문제가 나타날것이다. 처음 시스템을 구축할때 최적화가 되었더라도 데이터의 종류와 양에 따라 계속 답이 달라질 수 있으므로 주요 포인트들을 주기적으로 확인해야 한다. 

# 노드 부트스트랩의 이해

엘라스틱서치에는 부트스트랩이라는 과정이 있다. 그리고 엘라스틱서치 노드는 최초 실행시 항상 부트스트랩 체크라는 과정을 거친다. 이 과정이 왜 필요하고 그리고 그 과정에서 어떤 일이 일어나는지 알아보자.

## 부트스트랩 과정이 필요한 이유

과거게은 엘라스틱서치 설정 중 일부가 잘못됬거나 필수적인 리소스 할당이 되지 않으면 단순히 경고(Warning) 메세지를 로그에 출력하는 방식을 택했다. 그러다 보니 경험 없는 대다수의 운영자는 잠재되어 있는 문제의 심각성을 인지못하고 그냥 넘어가기가 부지기수 였다. 

그런 이유로 최신 버전에서는 운영시 발생가능한 문제점을 미연에 방지하기 위해 좀 더 강력한 검사를 수행하게 됬고 이러한 과정이 부트스트랩 체크(Bootstrap Checks)라 한다. 

부트스트랩 과정에서 필수 설정이 잘못된 경우, 잘못된 사실을 사용자에게 알리고 정상적으로 수정될때 까지 엘라스틱서치를 강제 종료하여 필수 설정이 완료되어야만 엘라스틱서치가 실행되도록 강제한다. 

사용자는 간단한 테스트를 하고 싶을 뿐인데 가끔 강제적인 부트스트랩 때문에 환경설정에 더 많은 시간을 할애하게 될 경우가 있다. 그래서 엘라스틱서치는 내부적으로 개발모드와 운영모드 두 가지의 개념이 존재한다. 

개발모드(Development Mode)
: 실행시 IP 주소가 루프백으로 설정된 경우, 엘라스틱서치는 개발모드로 실행된다고 판단한다. 개발모드로 실행될 경우, 부트스트랩 체크과정이 무시된다. 개발 모드로 동작하면 로컬에서 동작하기 때문에 다른 노드와 클러스터를 구성할 수 없다. 말 그대로 개발환경에서 테스트 목적으로만 사용할 수 있다.

운영모드(Product Mode)
: 운영모드는 반드시 부트스트랩 체크과정을 필수로 거친다. 그리고 실행시 IP 주소를 할당받기 때문에 당연히 다른 엘라스틱서치 노드와도 클러스터 구성이 가능하다. 클러스터에 존재하는 모든 노드는 항상 부트스트랩 체크 과정을 거치는 것이 좋다.


## 부트스크랩 체크과정 따라가기

1. 힙 크기 체크(Heap size check)
JVM옵션 중 기본 힙 크기와 최대 힙 크기 옵션이 별도로 존재한다. 최소 메모리를 쓰다가 큰 메모리를 쓰게되는데 엘라스틱서치는 스와핑을 최소화하기 위해 전체 힙 메모리에 대해 Memory Lock을 수행한다. 소프트 설정과 하드 설정의 값이 다르면 소프트 과정(기본 힙) 크기만큼만 Memory 대상으로 잡는다. 그러한 이유로 Memory Lock 설정을 하더라도 나증에 늘어난 힙 크기만큼 메모리는 스와핑 대상이 될수도 있다. 이를 위해 이 단계에서는 **JVM의 기본 힙 크기와 최대힙 크기가 같은지 검사한다.**

2. 파일 디스크립터 체크(File descriptor check)
리눅스에선 모든 것이 파일로 처리된다.  일반적인 파일은 물론이고 소켓, 시스템 콜 등 모든 것이 파일로 처리된다. 그렇기 때문에 파일 디스크립터의 수가 매우 중요한 요소다. 특히 엘라스틱서치는 경우, 내부의 루씬이 역색인을 구성하는 정보를 모두 파일로 처리하기 떄문에 더 많은 디스크립터가 필요하다. 이를 위해 **파일 디스크립터가 충분히 존재하는지 검사한다.**

3. 메모리 락 체크(Memory lock check)
JVM 메모리 관리를 위해 주기적으로 GC를 수행한다. 이 과정에서 메모리 확보하기 위해 불필요한 객체를 찾게 되고 이러한 객체를 힙 영역에서 제거함으로써 메모리를 확보한다. 가비지 컬렉션이 수행될 경우 모든 힙 메모리르 검사해야 한다. 하지만 이때 힙을 구성하는 메모리 페이지 조각 중 단 하나라도 디스크에 스왑 아웃 되어 있다면 이를 메모리에 다시 올리는 스왑인 작업을 반드시 해야 한다. 이러한 스왑 in, 스왑 out 작업은 JVM 애플리케이션에 큰 부담을 준다. 엘라스틱서치는 스와핑을 최대한 피하도록 안내하며, 힙에 할당된 메모리는 스와핑 대상이 되지 않도록 Memory Lock을 이용해 잠그도록 안내한다. 이를 위해 이 단계에서는 **엘라스틱서치에 할당된 힙 메모리의 Memory Lock 여부를 검사한다.** 

4. 최대 스레드 수 체크(Maximum number of threads check)
엘라스틱 서치는 대량의 요청을 빠르게 처리하기 위해 내부를 기능별로 나누어 여러 단계의 모듈로 구성되어 있다. 각 모듈은 큐와 쓰레드풀을 가지고 있기 때문에 요청에 대한 처리량(throughput)을 조절하면서 탄력적으로 처리하는 것이 가능하다. 모든 Thread Pool Executor가 여유롭게 스레드를 생성하도록 엘라스틱서치가 최소 4096개 이상의 스레드를 생성하도록 하는게 좋다. 이를 위해 이 단계에선 **애플리케이션이 생성할 수 있는 최대 쓰레드 수를 검사한다.** 

5. 최대 메모리 크기 체크(Maximum size virtual memory check)
내부에 존재하는 루씬은 인덱스 생성 및 관리를 효율적으로 하기 위해 mmap을 이용해 메모리 매핑을 수행한다. mmap을 이용하면 JVM을 통하지 않고도 리눅스 커널로 직접 시스템 콜을 실행할 수 있어 고성능 자바 애플리케이션에서 많이 사용한다. mmap은 커널 레벨의 메모리를 직접 할당받아 애플리케이션의 가상 메모리 주소의 매핑해서 동작하기 때문에 가상 메모리 크기에 제한이 없는것이 유리한다. 엘라스틱서치에서는 mmap를 효율적으로 사용하기 위해 애플리케이션의 가상 메모리 크기를 무제한으로 설정하도록 안내한다. 이를 위해 이 단계에선 리눅스가 생성하는 애플리케이션의 **가상 메모리 크기가 무제한으로 설정되어 있는지 검사한다.** 

6. 최대 파일 크기 체크(Max file size check)
리눅스에서 생성 가능한 파일 크기는 얼마일까? 리눅스에서는 하나의 파일이 가질 수 있는 최대 파일 크기를 관리하고 있다. 파일이 너무 커질 경우 시스템 전체에 장애가 발생할 수 있기 때문에 최디 파일 크기를 제한하고 있다. 엘라스틱서치가 생성하는 대표적인 파일인 세그먼트 파일과 트랜스로그(Translog) 파일은 상황에 따라 수십  GB이상으로 커질 수도 있다. 리눅스가 제한하는 최대 파일 크기 이상으로 파일 크기가 커질 경우 데이터가 유실되거나 엘라스틱서치가 이상 동작을 할 수도 있다. 그래서 엘라스틱서치를 안정적으로 운영하기 위해서는 최대 파일 크기를 무제한(unlimit)로 설정하는 것이 좋다. 이를 위해 이 단계에서는 **리눅스가 생성한 애플리케이션이 만들 수 있는 최대 파일 크기가 무제한으로 설정되어 있는지 검사한다.** 

7. mmap 카운트 체크(Maximum map count check)
엘라스틱서치는 앞 단계에서 가상 메모리 크기가 무제한으로 설정된것을 확인하고 나면 mmap을 사용할 사전 준비가 완료되었다고 판단하고 mmap를 효율적으로 동작할 수 있도록 메모리 매핑 영역을 생성한다. 이때 생성되는 메모리 매핑 영역은 커널이 제공하는 특정 크기의 메모리맵 조각으로 생성되는데 리눅스에서는 생성 가능한 메모리 맵의 수도 관리하고 있다. 엘라스틱서치에서는 최대 262,114개의 메모리 맵 영역을 가질 수 있다. 이를 위해 **리눅스가 생성한 애플리케이션 가질 수 있는 최대 mmap 카운트를 검사한다.** 

8. 클라이언트 JVM 체크(Client JVM check)
자바 애플리케이션을 실행하는 JVM은 서버에서 동작하는 프로그램을 위한 Server JVM과 클라이언트에서 동작하는 프로그램을 위한 Client JVM으로 나뉘어져 있다. Server JVM은 고성능 처리를 위해 최적화 되어 있는 반면 Client JVM은 빠른 실행과 적은 메모리 사용에 좀 더 최적화되어 있다. 두 종류의 JVM은 서로 추구하는 바가 다르므로 성능 차이가 상당할 수 있다. 엘라스틱서치는 고성능으로 동작해야 하기 때문에 반드시 Server JVM으로 실행해야 한다. 이를 위해 **이 단계에서는 엘라스틱서치가 Server JVM으로 실행되는지 검사한다.** 

9. Serial Collector 사용 여부 체크(Use serial collector check)
자바로 만든 모든 애플리케이션은 기본적으로 JVM위에 동작한다. 그리고 JVM은 메모리 관리를 위해 GC과정을 주기적으로 수행한다. GC를 수행하는 Collector는 여러 종류가 있는데, 현재 JVM에서는 CMS나 G1 컬렉터를 많이 사용하고 있다. 예전에는 Serial Collector를 쓰고는 했는데 이는 하드웨어가 열악한 시절의 GC 방식이나 특수한 목적을 제외하곤 절대 사용해선 안된다. 엘라스틱서치에서느 대용량의 힙 메모리를 사용하기 때문에 Serial CG를 사용하면 안된다. 이를 위해 이 단계에선 **엘라스틱서치를 실행할때 Serial CG를 사용하지 않도록 사용 여부를 검사한다.** 

10. 시스템 콜 필터 체크(System call filter check)
현대 운영체제는 보안을 위해 유저 모드와 커널 모드로 메모리 공간을 분리해서 관리한다. 각 메모리 간의 데이터 공유를 위해 시스템 콜을 이용해 안전하게 통신을 수행한다. 리눅스의 경우 유저 모드에서 커널 모드로 접근하는 시스템 콜을 제한하는 방식으로 샌드박스(Sandbox)를 구현하고 있으며 임의 코드 실행 공격을 막기 위한 목적으로 시스템 콜 필터를 제공한다. (리눅스의 경우, seccomp, seccomp-bpf 함수를 이용해 샌드박스를 구현한다.) 엘라스틱서치는 보안을 위해 기본적으로 시스템 콜 필터가 설치되 있는지 여부를 검사한다. 운영체제 수준에서 시스템 콜 필터를 지원하지 않는 경우가 있는데 이러한 경우에는 부트스트랩 과정에서 한상 실패하기 때문에 elasticsearch.yml 설정에서 bootstrap.system_call_filter 옵션을 false로 설정해 필터가 설치되어 있는지 확인 하는 과정을 무시하도록 설정도 가능하다. 사실 보안 측면에서 시스템 콜 필터를 항상 설치하는 것이 좋다. 운영하려는 시스템에 시스템 콜 필터가 지원되지 않는다면 커널을 업데이트 하도록 하자. 이 단계에서는 **엘라스틱서치를 실행했을 때 시스템 콜 필터가 사용되고 있는지 검사한다.** 

11. OnError, OnOutOfMemoryError 체크(OnError and OnOutOfMemoryError checks)
JVM을 실행할때 옵션중 OnError와 OnOutOfMemoryError 옵션이 있다. 각각 애플리케이션에서 Error가 발생하거나 OutOfMemoryError가 발생할 경우 지정한 명령어나 스크립트가 실행되도록 지원하는 옵션이다. 이를 이용하면 로그를 생성하거나 특정 명령어를 실행해 추후 문제를 해결할 수 있다. 엘라스틱서치는 기본적으로 시스템 콜 필터를 사용하는데, 이 옵션의 경우 시스템 콜 필터를 사용하는 애플리케이션에서는 사용이 불가능하기 때문에 설정해서는 안된다. 따라서 이 단계에선 엘라스틱 서치를 실행할때 **시스템 콜 필터를 사용할 경우 OnError 옵션이나 OnOutOfMemory 옵션이 설정되어 있는지 여부를 검사한다.(설정되어 있으면 사용하면 안된다는 것을 알려준다.)** 

12. Early-access 체크(Early-access check)
JDK는 공식 릴리스가 나오기 전에 테스트 목적으로 다음 버전의 릴리스 스냅숏을 미리 제공한다. 미리 다음 버전의 기능을 확인해 볼 수 있다는 장점이 있지만 안정화된 버전이 아니기에 실제 서버에 적용하는 것을 옳지 않다. 이를 위해 이 단계에서는 엘라스틱서치가 테스트 버전의 JVM을 사용하고 있는지 여부를 검사한다. 

13. G1GC 체크(G1GC check)
초기버전의 자바 8 릴리스에서는 G1GC 수집기가 활성화될 경우 가끔 엘라스틱서치가 생성한 인덱스의 일부가 손상될 수 있는 심각한 문제점이 있었다. 이런 문제는 JDK 8u40이후 패치가 되었기 때문에 G1GC 방식을 사용하려면 반드시 8u40 이상의 버전을 사용해야 한다. 

14. All Permission 체크(All Permission check)
자바는 SecurityManager라는 강력한 보안 모델을 제공한다. 자바로 작성된 애플리케이션은 운영체제의 리소스를 사용하기 위해 SecurityManager를 통해 필요한 접근 권한을 선택적으로 받는다. 자바는 이 보안 모델을 기반으로 애플리케이션 보안을 안전하게 구현하고 있다. 
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTQ2MDYzNzQ2XX0=
-->