
# 루씬 분석기(luceneAnalyzer)

**엘락스틱서치는 루씬을 기반으로 구축된 텍스트 기반 검색엔진**이다. 루씬은 내부적으로 다양한 분석기를 제공하는데, 기본적으로 엘라스틱서치는 루씬이 제공하는 분석기를 그대로 활용한다. 따라서 테스트 분석 과정을 이해하려면 루씬이 제공하는 분석기가 동작하는 매커니즘을 이해하는게 가장 중요하다. 이번 장에서는 루씬의 분석기를 알아보자.

## 역색인 구조(Inverted Index)

![enter image description here](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https://t1.daumcdn.net/cfile/tistory/1168CE4A4F60B00B0C)

루씬의 색인은 역색인이라는 특수한 방식을 사용한다. 역색인 구조의 특징을 정리하자면 아래와 같다. 

* 모든 문서가 가지는 단어(Term)의 목록
* 특정 단어가 어느 문서에 속해 있는지에 대한 정보
* 전체 문서에서 단어가 몇 개 들어있는지에 대한 정보
* 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도

예를 들어 아주 간단한 2개의 문서가 있다고 하자. 
```
Doc1
elasticsearch is cool
Doc2
Elasticsearch is great
```

문서의 역색인을 만들기 위해선 각 문서를 토큰화해야 한다. 토큰화된 단어에 대해 문서 상의 위치와 출현 빈도 등의 정보를 만든다. 그 결과물은 대략 다음과 같다. 

|토큰| 문서번호|위치(Position)| 출현빈도(Frequency)|
|--|--|--|--|
|elasticsearch  | doc1 | 1| 1| 
|Elasticsearch  | doc2 | 1| 1| 
|is  | doc1, doc2 | 2,2| 2| 
|cool  | doc1 | 3| 1| 
|great  | doc2 | 3| 1| 

위 내용을 보면 특정 토큰이 어떤 문서에서 어느 위치에 나왔고, 몇번 나왔는지에 대한 정보를 얻을 수 있다. 이를 바탕으로 **입력한 검색어가 존재하는 문서를 찾기 위해 검색어와 동일한 토큰을 찾아 해당 토큰이 존재하는 문서를 찾는다.**

cool 토큰의 경우, 문서 1의 내용이 나온다. 하지만 "elasticsaerch"로 검색을 하면 어떻게 될까? 예상으로는 문서1과 문서2에 해당하는 내용이 모두 나와야 할 것이다. 하지만 토큰의 정보가 정확하게 일치하는 데이터만 출력하기 때문에 문서1은 출력되지만 문서2는 출력되지 않는다. 이 문제를 해결하는 가장 간단한 방법은 텍스트 전체를 소문자로 변환한 다음 역색인 구조를 만드는데 사용하는 것이다.  

**일반적으로 색인한다는 것은 역색인 파일을 만든다는 것이다. 그렇다고 원문 자체를 바꾸는 것은 아니며, 색인 파일에 들어갈 토큰만 변경되고 실제 문서의 내용은 변함없이 저장된다.** 색인할때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정을 분석(Analyze)라고 하며, 이 처리는 다양한 분석기모듈들의 조합으로 처리된다.

## 분석기의 구조 

분석기가 동작하는 기본 프로세스는 아래와 같다. 
```
문장 -> CHRACTER FILTER(전처리 필터) -> 전처리된 문장 ->
TOKENIZER FILTER(토큰화)-> Tokens(토큰) 
-> TOKEN FILTER(동의어사전) -> Terms(단어) -> Index
```

CHARACTER FILTER(전처리 필터)
: 문장을 분석하기 전에 입력 테스트에 대해 특정한 단어를 변경하거나 HTML 같은 태그를 제거하는 역할을 하는 필터다. 텍스트를 토큰화하기 전 전처리 과정으로, ReplaceAll 함수처럼 패턴으로 텍스트를 변경하거나 사용자가 정의한 필터를 적용할 수 있다. 

TOKENIZER FILTER
: 토크나이저 필터는 분석기를 구성할때 단 하나만 사용할 수 있으며 텍스트를 어떻게 나눌 것인지를 정의한다. 한글을 분해할 때는 한글 형태소 분석기 토크나이저를 사용하고, 영문을 분석할때는 영문 형태소 분석기를 사용하면 된다.

TOKEN FILTER
: 토큰필터는 토큰을 하나씩 필터링하여 사용자가 원하는 토큰으로 변환한다. 예를 들어, 불필요한 단어를 제거하거나 동의어 사전을 만들어 단어를 추가하거나 영문 단어를 소문자로 변환하는 등의 작업을 수행할 수 있다. 


이제 간단한 분석기를 정의해 보자. 아래 분석기는 들어온 텍스트에서 html 태그를 제거하고 특수문자 혹은 공백을 기준으로 토큰나이징을 한다. 마지막으로 각 토큰을 소문자로 변환하여 인덱스로 저장한다.

```
PUT /movie_analyzer -- 영화 분석기 설정  
{
	"settings": {
		"index": {
			"number_of_shards":5,
			"number_of_replicas":1
		}
	},
	"analysis": {
		"analyzer": {
			"custom_movie_analyzer": { -- 커스텀 영화 분석기 정의
				"type" : "custom",
				"chat_filter" : ["html_strip"],-- html 태그 제거
				"tokenizer" : "standard", -- 표준 토크나이저 
				"filter" : ["lowercase"] -- 소문자 토큰필터
			}
		}
	}
}
```

위 설정은 커스마이징 된 ㅁ

Standard Analyzer
: 인덱스를 생성할때 settings에 analyzer를 정의할 수 있는데, 별다른 정의를 하지 않고 필드의 데이터 타입을 Text 데이터 타입으한다면 이 분석기를 사용한다. 

Whitespace Analyzer
: 공백 문자열을 기준으로 토큰을 분리하는 매우 간단한 분석기다. 공백으로 토큰을 분리하는 Whitespace Tokenizer를 사용하고 Token Filter에는 아무것도 없다. 


###  CHARACTER FILTER(전처리 필터)

전처리 필터는 사실 토크나이저 내부에서도 동일한 처리가 가능하다. 그래서 사실 전처리 필터는 상대적으로 활용도가 많이 떨어진다. 엘라스틱서치에서 공식적으로 제공하는 전처리 필터도 그리 많지 않다. 앞서 보았던 HTML 을 제거하는 전처리 필터인 Html strip char 필터가 대표적이다.

### TOKENIZER FILTER(토크나이저 필터)

토크나이저 필터는 분석기를 구성하는 **가장 핵심 구성요소**다. 전처리 필터를 거쳐 토크나이저 필터로 문장이 오면 해당 텍스트는 토크나이저 필터의 특성에 맞게 적절히 분해된다. 분석기에서 어떠한 토크나이저를 사용하느냐에 따라 분석기의 전체적인 성격이 결정된다. 

#### Standard 토크나이저

가장 일반적으로 사용하는 토크나이저로 대부분의 기호(공백, 특수문자)를 만나면 그 기호를 기준으로 토큰을 나눈다.  

#### Whitespace 토크나이저

공백을 만나면 텍스트를 토큰화한다. 

#### Ngram 토크나이저

Ngram에서 N은 토큰화할 글자 수를 말한다. 별다른 옵션을 주지 않는다면 기본적으로 한 글자씩 토큰화를 한다. Ngram에 특정 문자를 지정할 수도 있으며, 이 경우 문자의 목록 중 하나를 만날때마다 단어를 자른다. 그 밖에도 다양한 옵션을 조합해서 **자동완성 기능**을 만들때 유용하게 활용할 수 있다. 

만약 문장으로 "Harry Potter"를 주고, N이 3이라면 아래와 같은 결과로 토크나이징을 하게 된다. 
```
Har, arr, rry, Pot, ott, tte, ter
```

#### Edge Ngram 토크나이저

시작 부분을 고정시켜 Ngram 단어를 자르는 방식으로 사용하는 토크나이저다. 해당 토크나이저도 자동 완성을 구현할때 유용하게 활용할 수 있다. 

만약 Edge Ngram 토크나이저에게 "Harry Potter"를 주면 아래와 같은 결과로 토크나이징을 하게 된다. 
```
H, Ha, Har, Harr, Harry, P, Po, Pot, Pott, Potte, Potter
```

### TOKEN FILTER(토큰 필터)

토큰 필터는 토크나이저에서 분리된 토큰들을 추가, 수정, 삭제할 때 사용하는 필터다. 토크나이저에 의해서 토큰이 분리가 되면 분리된 토큰은 배열 형태로 토큰 필터에게 전달된다. 토크 나이저가 앞단에서 처리를 해줘야 하기 때문에 토큰 필터는 독립적으로 사용할 수는 없다.

#### Ascii Folding 토큰 필터

아스키 코드에 해당하는 127개의 알파벳, 숫자, 기호에 해당하지 않는 경우 문자를 ASCII 요소로 변환한다. 예를들어 아스키 코드가 아닌 "javaca'fe"라는 단어가 들어오면 이는 "javacafe"로 Ascii 값에 맞게 변경된다.

#### Lowercase, Uppercase 토큰 필터

토큰 전체를 소문자나 대문자로 변환한다.

#### Stop 토큰 필터

사용하지 않을 금지 토큰을 적용시킬때 사용하는 필터이다. 인덱스로 만들고 싶지 않거나 검색되지 않게 하고 싶은 단어를 불용 사전에 등록해서 해당 토큰을 취급하지 않도록 한다. 

#### Stemmer 토큰 필터

Stemming 알고리즘을 이용해 토큰을 변형하는 필터다. 영어 토큰을 영단어 원형으로 변환한다.  

#### Synonym 토큰 필터

동의어 처리를 할 수 있는 필터다. 예를 들어 "Harry"라는 토큰이 들어오면 "해리"라는 동의어로 변환할 수 있다. 

#### Trim 필터

토큰의 앞뒤로 공백을 제거하는 토큰 필터이다. 

## 동의어 사전(synonym dictionary)

엘라스틱서치에서 제공하는 토큰 필터중 Synonym 필터를 사용하면 동의어 처리가 능해진다. 동의어는 검색 기능을 풍부하게 할 수 있게 도와주는 도군 중 하나다. **원문에 특정한 단어가 존재하지 않더라도 색인 데이터를 토큰화해서 저장할 때 동의어나 유의어에 해당하는 단어를 함께 저장해서 검색이 가능해지게 하는 기술이다.**  

예를 들어, "ElasticSearch"라는 단어가 포함된 원문이 필터를 통해 인덱스에 저장된다면 "엘라스틱서치"라고 검색했을때 검색되지 않을 것이다. 하지만 동의어 기능을 색인할때 "엘라스틱서치"도 함께 저장한다면 "ElasticSearch"도 검색이 가능하고 "엘라스틱서치"도 검색이 가능해진다.

동의어를 추가하는 방식에는 크게 두 가지가 있다. 첫 번째는 동의어를 매핑 설정 정보에 미리 파라미터로 등록하는 방식이고 두 번째는 특정 파일별도로 생성해서 관리하는 방식이다.  사실 첫번째 방법은 실에서 잘 사용되지 않는다. 매핑 정보에서 동의어를 관리ㄹ 경우 운영 중에는 동의어를 변경하기가 어렵기 때문이다. 

엘라스틱 서치에서 가장 까다로운 부분 중 하나가 바로 동의어를 관리하는 것이다. 검색엔진에서 다루는 분야가 많아지면 많아질 수록 동의어 수도 늘어난다. 분야별로 파일도 늘어날 것이고  그 안에 변환  규칙도 많아질것이다.

실무에선 이런 동의어를 모아둔 파일을 칭할 때 동의어 사전이라 부른다.

### 동의어 사전 사용법

동의어 사전 파일은 엘라스틱 서치가 설치된 서버 아래의 config 디렉터리에 생성해야 한다.

```
<엘라스틱서치 설치 디렉터리>/config/synonym.txt
```

여기선 synonym.txt라는 동의어 사전을 생성했다. 이제 데이터를 추가하거나 치환하는 방법을 알아보자. 

#### 동의어 추가 

동의를 추가할 때 단어를 표(,)로 분리해 등록하는 방법이다. 예를 들어, "Elasticsearch"와 "엘라스틱서치"를 동의어로 지정하고 싶다면 동의어 사전 파일에 "Elasticsearch, 엘라스틱 서치"라고 등록하면 된다. 

```
Elasticsearch, 엘라스틱서치
```

여기서 주의해야할 점은 동의어 처리 기준은 앞서 동작한 토큰 필터의 종류가 무엇이고, 어떤 작업을 했느냐에 따라 달라질 수 있다는 점이다. 예를 들어 "Elasticsearch"라는 토큰이 분리된 후 lowercase 필터를 적용하면  "elasticsearch"라는 토큰이 될 것이다. 이 경우 동의어로 등록한 "Elasticsearch"와 일치하지 않기 때문에 다른 토큰으로 인식해서 동의어가 적용되지 않을 것이다. 

#### 동의어 치환

특정 단어를 어떤 단어로 변경하고 싶다면 동의어 치환 기능을 사용하면 된다. 동의어를 치환하면 토큰이 제거되고 변경될 새로운 토큰이 추가된다 동의어 치환은 동의처 추가와 구분하기 위해 화살표(=>)를 사용한다. 
```
Elasticsearch, 엘라스틱서치 -- 동의어 추가
Harry => 해리 -- 동의어 치환
```

동의어 사전은 실시간으로 적용되지 않는다. 수정된 동의어를 적용하고 싶다면 해당 동의어 사전을 사용하고 있는 인덱스를 Reload해야 한다. 

한 가지 주의해야할 점이 있다. 동의어 사전은 색인 시점에도 사용될 수 있고 검색 시점에도 사용할 수 있다는 점이다. 검색 시점에는 사전의 내용이 변경되면 그 내용이 반영된다. 하지만 색인 시점에 동의어 사전이 사용됬다면 사전의 내용이 변경되더라도 색인이 변경되지는 않는다. 이 경우 기존 색인을 모두 삭제하고 색인을 새로 생성해야만 변경된 사전 내용이 적용된다. 이러한 문제점 때문에 동의어 사전이 빈번하게 수정되는 인덱스의 경우 색인 시점에는 적용하지 않고 검색 시점에만 적용하는 방식으로 이러한 문제점을 해결하기도 한다. 

자 이제 아래 문장이 엘라스틱서치에 들어왔다고 하자. 
```
Elasticsearch Harry
```

결과는 아래와 같다.
```
elasticsearch  -- 기존 Elasticsearch의 소문자 필터 토큰 생성
엘라스틱서치 -- 새로운 동의어 토큰이 추가
해리 -- Harry를 치환하는 대체 동의어 토큰
```

언급했듯이 동의어 사전이 변경될 경우 이를 인식시키기 위해서는 인덱스를 Reload해야 한다. 동의어 사전 데이터는 모두 메모리에 올라가 있는데 이를 갱신해 줘야 한다. 


> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEzNzQyOTA5ODAsOTAxNDIxNjYsLTE4Nz
AzMDA4NzQsNDAzOTE4MjIxLC0xMzg4NDM2OTQ3LDkzODI3MzMw
LDEyNDA2NzUxNzEsLTgyOTk4NTM3OSwtMTg5ODYwMzA5MCwtMT
AyMTAxNjUyMyw5Njg1MDE2ODMsLTQ2ODY4NzI2MCw5NjEyMDc1
NDNdfQ==
-->