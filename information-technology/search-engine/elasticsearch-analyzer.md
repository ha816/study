
# 엘라스틱 서치 분석기

엘락스틱서치는 루씬을 기반으로 구축된 텍스트 기반 검색엔진이다. 루씬은 내부적으로 다양한 분석기를 제공하는데, 엘라스틱서치는 루씬이 제공하는 분석기를 그대로 활용한다. 테스트 분석을 이해하려면 루씬이 제공하는 분석기가 어떻게 동작하는지를 이해하는게 가장 중요하다. 

## 역색인 구조

루씬의 색인은 역색인이라는 특수한 방식을 사용한다. 역색인 구조를 간단히 정리하자면 아래와 같다. 

* 모든 문서가 가지는 단어의 고유 단어 목록
* 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
* 전체 문서에 각 단어가 몇 개 들어있는지에 대한 정보
* 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도

예를 들어 2개의 문서가 있다고 하자. 
```
문서1
elasticsearch is cool

문서2
Elasticsearch is great
```

문서의 역색인을 만들기 위해선 각 문서를 토큰화 해야 한다. 토큰화된 단어에 대해 문서 상의 위치와 출현 빈도 등의 정보를 체크한다. 따라서 결과물은 대략 다음과 같다. 

|토큰| 문서번호|텀의 위치(Position)| 텀의빈도(Term Frequency)|
|--|--|--|--|
|elasticsearch  | doc1 | 1| 1| 
|Elasticsearch  | doc2 | 1| 1| 
|is  | doc1, doc2 | 2,2| 2| 
|cool  | doc1 | 3| 1| 
|great  | doc2 | 3| 1| 

위 내용을 보면 특정 토큰이 어떤 문서에서 어느 위치에 나왔고, 볓번 나왔는지에 대한 정보를 얻을 수 있다. 검색어가 존재하는 문서를 찾기 위해 검색어와 동일한 토큰을 찾아 해당 토큰이 존재하는 문서를 찾아 간다.

cool의 경우, 문서 1의 내용이 나온다. 하지만 "elasticsaerch"로 검색을 하면 어떻게 될까? 예상으로는 문서1과 문서2에 해당하는 내용이 다 나와야 할 것이다. 하지만 토큰의 정보가 정확하게 일치하는 데이터만 출력하기 때문에 문서1은 출력되지만 문서2는 출력되지 않는다. 

이 문제를 해결하는 가장 간단한 방법은 텍스트 전체를 소문자로 변환한 다음 색인하는 것이다. 

색인 한다는 것은 역색인 파일을 만든다는 것이다. 그렇다고 원문 자체를 바꾸는 의미는 아니다. 따라서 색인 파일에 들어갈 토큰만 변경되고 실제 문서의 내용은 변함없이 저장된다. 

색인할때 특정한 규칙과 흐름에 의해 텍스트를 변경하는 과정을 분석(Analyze)라고 하며 이 처리는 분석기 모듈의 조합으로 처리된다.

## 분석기의 구조 

분석기가 동작하는 기본 프로세스는 아래와 같다. 

문장 -> CHRACTER FILTER-> 전처리 처리된 문장 -> TOKENIZER FILTER -> Tokens -> TOKEN FILTER(동의어 사전) -> Terms -> index

CHARACTER FILTER(전처리 필터)
: 문장을 분석하기 전에 입력 테스트에 대해 특정한 단어를 변경하거나 HTML 같은 태그를 제거하는 역할을 하는 필터다. 해당 내용은 텍스트를 개별 토큰화하기 전의 전처리 과정이며, ReplaceAll 함수처럼 패턴으로 텍스트를 변경하거나 사용자가 정의한 필터를 적용할 수 있다. 

TOKENIZER FILTER
: 토크나이저 필터는 분석기를 구성할때 하나만 사용할 수 있으며 텍스트를 어떻게 나눌것인지 정의한다. 한글을 분해할 때는 한글 형태소 분석기의 토크나이저를 사용하고, 영문을 분석할때는 영문 형태소 분석기를 사용하면 된다.

TOKEN FILTER
: 토큰필터는 토큰화된 단어를 하나씩 필터링하여 사용자가 원하는 토큰으로 변환한다. 예를들어, 불필요한 단어를 제거하거나 동의어 사전을 만들어 단어를 추가하거나 영문 단어를 소문자로 변환하는 등의 작업을 수행할 수 있다. 


이제 간단한 분석기를 정의해 보자. 아래 분석기는 들어온 텍스트에서 html 태그를 제거하고 특수문자 혹은 공백을 기준으로 토큰나이징을 한다. 마지막으로 각 토큰을 소문자로 변환하여 인덱스로 저장한다.

```
PUT /movie_analyzer 
{
	"settings": {
		"index": {
			"number_of_shards":5,
			"number_of_replicas":1
		}
	},
	"analysis": {
		"analyzer": {
			"custom_movie_analyzer": {
				"type" : "custom",
				"chat_filter" : ["html_strip"],-- html 태그 제거
				"tokenizer" : "standard", --표준 토크나이저 
				"filter" : ["lowercase"] -- 소문자 토큰필터
			}
		}
	}
}
```

Standard Analyzer
: 인덱스를 생성할때 settings에 analyzer를 정의할 수 있는데, 별다른 정의를 하지 않고 필드의 데이터 타입을 Text 데이터 타입으한다면 이 분석기를 사용한다. 

Whitespace Analyzer
: 공백 문자열을 기준으로 토큰을 분리하는 매우 간단한 분석기다. 공백으로 토큰을 분리하는 Whitespace Tokenizer를 사용하고 Token Filter에는 아무것도 없다. 


###  CHARACTER FILTER(전처리 필터)

전처리 필터는 사실 토크나이저 내부에서도 동일한 처리가 가능하다. 그래서 사실 전처리 필터는 상대적으로 활용도가 많이 떨어진다. 엘라스틱서치에서 공식적으로 제공하는 전처리 필터도 그리 많지 않다. 앞서 보았던 HTML 을 제거하는 전처리 필터인 Html strip char 필터가 대표적이다.

### TOKENIZER FILTER(토크나이저 필터)

토크나이저 필터는 분석기를 구성하는 **가장 핵심 구성요소**다. 전처리 필터를 거쳐 토크나이저 필터로 문장이 오면 해당 텍스트는 토크나이저 필터의 특성에 맞게 적절히 분해된다. 분석기에서 어떠한 토크나이저를 사용하느냐에 따라 분석기의 전체적인 성격이 결정된다. 

#### Standard 토크나이저

가장 일반적으로 사용하는 토크나이저로 대부분의 기호(공백, 특수문자)를 만나면 그 기호를 기준으로 토큰을 나눈다.  

#### Whitespace 토크나이저

공백을 만나면 텍스트를 토큰화한다. 

#### Ngram 토크나이저

Ngram에서 N은 토큰화할 글자 수를 말한다. 별다른 옵션을 주지 않는다면 기본적으로 한 글자씩 토큰화를 한다. Ngram에 특정 문자를 지정할 수도 있으며, 이 경우 문자의 목록 중 하나를 만날때마다 단어를 자른다. 그 밖에도 다양한 옵션을 조합해서 **자동완성 기능**을 만들때 유용하게 활용할 수 있다. 

만약 문장으로 "Harry Potter"를 주고, N이 3이라면 아래와 같은 결과로 토크나이징을 하게 된다. 
```
Har, arr, rry, Pot, ott, tte, ter
```

#### Edge Ngram 토크나이저

시작 부분을 고정시켜 Ngram 단어를 자르는 방식으로 사용하는 토크나이저다. 해당 토크나이저도 자동 완성을 구현할때 유용하게 활용할 수 있다. 

만약 Edge Ngram 토크나이저에게 "Harry Potter"를 주면 아래와 같은 결과로 토크나이징을 하게 된다. 
```
H, Ha, Har, Harr, Harry, P, Po, Pot, Pott, Potte, Potter
```

### TOKEN FILTER(토큰 필터)

토큰 필터는 토크나이저에서 분리된 토큰들을 추가, 수정, 삭제할 때 사용하는 필터다. 토크나이저에 의해서 토큰이 분리가 되면 분리된 토큰은 배열 형태로 토큰 필터에게 전달된다. 토크 나이저가 앞단에서 처리를 해줘야 하기 때문에 토큰 필터는 독립적으로 사용할 수는 없다.

#### Ascii Folding 토큰 필터







## 동의어 사전(synonym dictionary)








> Written with [StackEdit](https://stackedit.io/).
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE0NjU0MTcwODAsMTQ2ODM5NzEzNCwtOT
c4OTAyMDA4LDMyMDU4NDcxOSwtNDA5MjExOTU2LC0xMzc2MDYw
NTMzXX0=
-->